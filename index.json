[{"content":"A couple of months ago, the new Kubernetes Gateway API graduated to beta.\nWhy do you need another API to handle external traffic when you have the stable Kubernetes Ingress API and dozens of implementations? What problems of the Ingress API does the new Gateway API solve? Does this mean the end of the Ingress API?\nI will try to answer these questions in this article by getting hands-on with these APIs and looking at how they evolved.\nStandardizing External Access to Services: The Ingress API The Kubernetes Ingress API was created to standardize exposing services in Kubernetes to external traffic. The Ingress API overcame the limitations of the default service types, NodePort and LoadBalancer, by introducing features like routing and SSL termination.\n Kubernetes Ingress   There are over 20 implementations of Ingress controllers available. In this article, I will use Apache APISIX and its Ingress controller for examples.\n APISIX Ingress   You can create an Ingress resource to configure APISIX or any other Ingress implementations.\nThe example below shows how you can route traffic between two versions of an application with APISIX Ingress:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api-v1 port: number: 8080 path: /v1 pathType: Prefix - backend: service: name: bare-minimum-api-v2 port: number: 8080 path: /v2 pathType: Prefix  Tip: You can check out this hands-on tutorial to learn more about setting up Ingress on Kubernetes with Apache APISIX Ingress controller.\n Since the Ingress API is not tied to any particular controller implementation, you can swap APISIX with any other Ingress controller, and it will work similarly.\nThis is okay for simple routing. But the API is limited, and if you want to use the full features provided by your Ingress controller, you are stuck with annotations.\nFor example, the Kubernetes Ingress API does not provide a schema to configure rewrites. Rewrites are useful when your upstream/backend URL differs from the path configured in your Ingress rule.\nAPISIX supports this feature, and you have to use custom annotations to leverage it:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes annotations: k8s.apisix.apache.org/rewrite-target-regex: \u0026#34;/app/(.*)\u0026#34; k8s.apisix.apache.org/rewrite-target-regex-template: \u0026#34;/$1\u0026#34; spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api port: number: 8080 path: /app pathType: Prefix This creates an Ingress resource that configures APISIX to route any requests with the /app prefix to the backend with the prefix removed. For example, a request to /app/version will be forwarded to /version.\nAnnotations are specific to your choice of an Ingress controller. These \u0026ldquo;proprietary\u0026rdquo; extensions limited the scope of portability intended initially with the Ingress API.\nCustom CRDs \u0026gt; Ingress API Being stuck with annotations also sacrifice the usability of the Ingress controllers.\nControllers therefore solved the limitations of the Ingress API by creating their own custom resources. The example below shows configuring Ingress to route traffic between two versions of an application using APISIX\u0026rsquo;s custom resource:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: api-routes spec: http: - name: route-1 match: hosts: - local.navendu.me paths: - /v1 backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 - name: route-2 match: hosts: - local.navendu.me paths: - /v2 backends: - serviceName: bare-minimum-api-v2 servicePort: 8080 These CRDs made it much easier to configure Ingress, but you are tied to the specific Ingress control implementation. Without the Ingress API evolving, you had to choose between usability or portability.\nExtending Ingress and Evolution to Gateway API Ingress API was not broken; it was limited. The Gateway API was designed to overcome these limitations.\n (Gateway API) aim to evolve Kubernetes service networking through expressive, extensible, and role-oriented interfaces \u0026hellip;\n gateway-api.sigs.k8s.io  What is the Gateway API?    It takes inspiration from the custom CRDs of different Ingress controllers mentioned earlier.\nThe Gateway API adds many features \u0026ldquo;on top\u0026rdquo; of the Ingress API\u0026rsquo;s capabilities. This includes HTTP header-based matching, weighted traffic splitting, and other features that require custom proprietary annotations with the Ingress API.\nTraffic split with APISIX Ingress resource (see ApisixRoute/v2 reference):\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: traffic-split spec: http: - name: rule-1 match: hosts: - local.navendu.me paths: - /get* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 90 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 10 Traffic split with Gateway API (see Canary traffic rollout):\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: traffic-split spec: hostnames: - local.navendu.me rules: - backendRefs: - name: bare-minimum-api-v1 port: 8080 weight: 90 - name: bare-minimum-api-v2 port: 8080 weight: 10 Another improvement from the Ingress API is how the Gateway API separates concerns. With Ingress, the application developer and the cluster operator work on the same Ingress object, unaware of the other\u0026rsquo;s responsibilities and opening the door for misconfigurations.\nThe Gateway API separates the configurations into Route and Gateway objects providing autonomy for the application developer and the cluster operator. The diagram below explains this clearly:\n The Gateway APIAdapted from gateway-api.sigs.k8s.io\n  Is This the End of Ingress API? The Gateway API is relatively new, and its implementations are constantly breaking. On the contrary, the Ingress API is in stable release and has stood the test of time.\nIf your use case only involves simple routing and if you are okay with using custom annotations to get extra features, the Ingress API is still a solid choice.\nWith the Gateway API being a superset of the Ingress API, it might make sense to consolidate both. Thanks to the SIG Network community, Gateway API is still growing and will soon be production ready.\nMost Ingress controllers and service meshes have already implemented the Gateway API along with the Ingress API, and as the project evolves, more implementations will surface.\nPersonally, at least for now, I would stick with custom CRDs provided by the Ingress controllers like Apache APISIX instead of the Ingress or Gateway API.\n","permalink":"https://navendu.me/posts/gateway-vs-ingress-api/","summary":"Exploring the new Kubernetes Gateway API and comparing it with the existing Kubernetes Ingress API for handling external traffic.","title":"Comparing Kubernetes Gateway and Ingress APIs"},{"content":"I ask a lot of questions to my peers and strangers on public forums on the internet. This year, I have been trying to improve how I ask questions. Here is how I do it.\nBut first of all,\nWhat are Good Questions? Good questions are the ones that are easy to answer.\nOur goal for asking a question is to have the other person explain what they know in a way you can understand. A series of good questions is the key to a good answer.\nBad:\n J: What happens when we strip the binaries? (Too vague and broad)\nN: Stripped binaries don’t have debugging information. So its size is reduced \u0026hellip; (Answers with a lot of irrelevant information)\n Good:\n J: I see that we are stripping the binaries to reduce their size before publishing. I found that it shouldn’t affect the performance. Is that right? What other implications does this have? (Clear question, easy to answer)\n  N: Stripping only removes the debugging information. It wouldn’t affect the performance in any way. But it will be difficult to debug if we run into any issues as debug symbols are removed from the traceback.\n The Problem with Bad Questions Bad questions can derail a conversation quickly.\nFor me, asking bad questions has often resulted in:\n  the person explaining things irrelevant to my question.\n  the person explaining things I have no clue of.\n  the person explaining what I already know.\n  the person not answering the question at all (especially for under-researched questions).\n  All of this boils down to you or both of you walking away frustrated and without a clear answer.\nAt this point, it should be obvious why you should focus on asking questions properly. So, here is my process.\nWho are you Asking? Who you are asking a question should impact how you ask the question. Let me explain.\nIf you are asking your coworker who works on your project or is familiar with the particular niche, you can reasonably assume that the person has some context on what you are asking.\nThis means there would be fewer things to explain, and you can build your explanation from your shared knowledge. But it is a different game when you are asking questions to the people of the interwebs.\nWhen I began programming, I had my share of bashing from people in Stack Overflow. I get that having a high bar for quality assurance helps Stack Overflow be the go-to place to ask questions, but some of the moderators are so trigger-happy that they will shoot you (your question) down right away.\nBut anyway, the important thing to remember here is that the person reading your question has very little context about your situation. It is obvious when a person has put little to no effort into the question, and these questions are the first to get the bashing.\nWhen to Ask? If you have a lot of questions or if you think answering your question will take time, it is better to schedule a time when you are both available.\nIf your questions are quick, it is better to ask them right away if it saves you a lot of time.\nGoogle First, Ask Later One of my biggest pet peeves is people who ask technical questions that can be answered by the first result of a Google search. It shows little effort on their part, and now I just ask people to Google it and do not bother to answer until they do their homework.\n I maintain a project called Meshery, and one of the new contributors (who came in to get a GSoC internship) literally asked if I could explain what Meshery is.\nWe have a website, 100+ pages of documentation, recordings of conference talks, and technical documentation, all sent to the user as they join the community.\nYou know how that conversation went.\nIt would have been different if they had asked me something like, “I have been going through Meshery’s docs and been trying it out locally. I’m unclear how Meshery adds value if a person already uses a service mesh. Could you point me to any docs where this is explained better?”.\nThink for a moment about how you would have answered in these scenarios.\n Doing a bit of research can help you build some foundational knowledge to ask a set of better questions.\nThe “Google first, ask later” motto is only good as a rule of thumb. Nothing has stopped me from asking obvious, googleable (it is a real word) questions when in conversation with someone.\nTo sum it up, make some effort, do your homework, and then ask your questions. Don’t expect to be spoon-fed.\nIs that Right? Let’s go back to the “stripped binary” example.\n J: I see that we are stripping the binaries to reduce their size before publishing. I found that it shouldn’t affect the performance. Is that right? What other implications does this have?\n See how stating what you already know lets you build the rest of the conversation.\nTo ask this question, you must spend some time digging through what a stripped binary is and how it is different from a “normal” binary. The time taken to understand and formulate that question is time well spent.\nOn the receiving end, the person will see that you have spent time in this and are not just asking them to do your work. It will also be easier to answer your question by building on your foundational knowledge.\nVague Precise Questions  J: How do I use a Kind cluster to set up my development environment?\n If you ask me this, I would reply with a link to the Kind docs. But this wasn’t what they intended to ask. So they say,\n J: I tried this, but it is not working.\n Well, there are million different reasons for this not to work. I am not Doctor Strange to evaluate all the possibilities in a second! A little bit more context might help.\nI will cut to the chase and say how I would ask this question.\n N: I was trying to set up Kind for my local development environment. I am on macOS. I have Docker Desktop and Kind running. I also have set up Metallb LoadBalancer, and I see the external IP of the service, as shown on the logs below. Still, I am not able to reach it from my host machine. Is there something I’m missing?\n Then that senior engineer with years of experience can jump right in and say,\n S: On macOS, Docker does not expose the docker network to the host. You can try port-forwarding to reach the pods.\n See how easy it was to answer?\nThis goes for all questions. The more precise you are with your questions, the easier it is to answer.\nThis also prevents the person answering from going off on a tangent, explaining irrelevant details that you may either not care about or aren’t relevant to your actual question.\nAnother way to prevent shooting off on a tangent is to ask questions that can be answered by a simple yes/no.\n J: Why are we using this gRPC middleware instead of directly calling the required service?\n    J: Are we using this gRPC middleware to convert between two different configuration formats?\n  N: Yes.\n The person usually goes to explain why yes/no after this, but these questions are easy to answer, and I almost always get quick responses.\nThese questions are pretty helpful when you are in conversation with a person, and they are explaining something to you. This segues into my next point.\nWhen in Doubt, Ask More Questions Imposter syndrome is real.\nWhen I started working with others, I often stopped myself from saying, “I don’t understand”, thinking I would look stupid.\nI have then come to learn that if you ask a “stupid” question, you are stupid for the day, but if you don’t, you are stupid for life (because you will always stop yourself from asking questions, ending up not understanding things completely\u0026hellip; umm, you get it right?).\nThis means when you get an answer, and you are not completely satisfied,\n  say what you don’t understand.\n  ask more clarifying questions.\n  stop the speaker and ask more specific questions.\n  Confronting the imposter syndrome is hard, but it has been helpful to me in knowing that everyone else faces this too.\nWhen you start thinking, “maybe I’m just not smart enough to understand the answer”, remember that people want to help you. You just have to help them help you!\nLearning in Public Ask questions in a public channel instead of DMs.\nThis may not work in every situation, but I try to do this more often now.\nThis will document the discussions publicly and would also help any others looking in. You can always point people to this discussion if they ask the same question.\nTake Stack Overflow, for example. You almost always find answers to problems you face from questions someone else asks.\nThe imposter syndrome shifts to the next gear here. Face it head-on.\nAsking Good Questions is a Skill And like all skills, it is sharpened with practice.\nAsking the right questions will help you extract the answers you want. In most scenarios, it is not that the person answering is incapable, but you are not asking the right questions.\nI have improved over the year and am still working out the kinks in my process.\nThis might be a good post to come back to in a year to reflect on and improve.\nTo summarise this post in a sentence,\nMake it easy for people to help you.\n","permalink":"https://navendu.me/posts/how-i-ask-questions/","summary":"I ask a lot of questions to my peers and to strangers on public forums in the internet. This year, I have been trying to improve this process to ask better questions. Here is how I do it.","title":"How I Ask Questions as a Software Engineer"},{"content":"Continuous monitoring is critical in making microservice systems robust. Without proper monitoring, microservices can quickly become overwhelmed, leading to errors and loss in performance.\nThrough continuous monitoring, developers can detect issues with their services as soon as they arise and take measures to prevent significant damage. It also provides insights into how your services are performing, allowing you to make informed decisions.\nThis article will introduce how you can set up monitoring on your microservice application using two of the popular tools in this space, Prometheus, and Grafana.\nThe source code and Docker Compose file for this tutorial are available in monitoring-101.\nPrometheus Basics Prometheus is an open source, monitoring and alerting tool. It \u0026ldquo;pulls\u0026rdquo; metrics (measurements) from microservices by sending HTTP requests and stores the results in a time-series database.\nYou can instrument your services by using client libraries provided by Prometheus. This will enable you to create and collect custom metrics from your services.\nPrometheus also has exporters that let you pull metrics that are not in Prometheus format. An exporter acts as a middleman and transforms exported data into Prometheus readable format.\n Prometheus   Prometheus provides a powerful query language, PromQL, to work with this collected data. You can use PromQL to create complex queries to filter, aggregate, and transform the data to the desired format.\nIn addition to pulling metrics, Prometheus can also trigger alerts when set thresholds are breached. The alerting mechanism is highly configurable and can send notifications to places like Slack or email.\nPrometheus has a GUI that lets you visualize the collected metrics easily. It also integrates with other advanced visualization tools like Grafana.\n Prometheus DashboardShows the memory stats of a Go-based service\n  Metric Types Prometheus offers four core metric types:\n Counter: represents a single monotonically increasing counter. Its value can increase or reset to zero on restart. You can use it to represent metrics like the number of requests served. Gauge: represents a numerical value that can go up or down. You can use it to represent values like memory usage or the number of requests per second. Histogram: samples data into configurable buckets. Use it to represent values like request durations or response sizes. Summary: similar to histogram, it also calculates configurable values over a sliding time window.  You can learn more about these metric types and how to use them from the official documentation.\nSample Application Our sample application will consist of an API gateway, a Go app, and a Python app:\n Sample applicationThe application will return, \u0026ldquo;Hello name!\u0026rdquo; in the language you choose with the \u0026lt;name\u0026gt; you provide. Apache APISIX will be the API gateway that directs traffic to your services.\n  The diagram below shows how the system works:\n Sample application sequence    The user sends a GET request to APISIX, the entry point for the application. APISIX forwards the request to the Go service. The Go service sends a GET request to the Python service to obtain \u0026ldquo;Hello\u0026rdquo; in the specified language. The Python service responds with the required translation of \u0026ldquo;Hello.\u0026rdquo; The Go service creates the required response by using the name provided in the query and sends it to APISIX. APISIX forwards the response back to the user.  Configuring Prometheus to Collect Metrics We will instrument and export metrics from all the services in our application and collect them in Prometheus. We will start with our API gateway, Apache APISIX.\nExporting Metrics from APISIX Apache APISIX is an open source, cloud native API gateway.\nYou don\u0026rsquo;t need to know about APISIX to follow along, and you can use the Docker Compose file provided to set everything up. To learn more about APISIX, visit apisix.apache.org.\nAPISIX offers a prometheus Plugin that easily exports metrics in the Prometheus format. You can configure the Plugin in your APISIX configuration file:\napisix: enable_admin: false # run APISIX in standalone mode config_center: yaml # use a YAML file for configuration instead of storing it in etcd plugin_attr: prometheus: export_uri: /prometheus/metrics # enable the prometheus Plugin and export the metrics to this URI enable_export_server: false # export the metrics in the default data-plane port  We can now enable the Plugin on every Route by making it a Global rule:\nroutes: # route requests to /hello/* to the go-app - uri: /hello/* upstream: type: roundrobin nodes: \u0026#34;go-app:8080\u0026#34;: 1 plugins: # remove the prefix \u0026#34;/hello\u0026#34; before forwarding the request to the go-app  proxy-rewrite: regex_uri: - \u0026#34;/hello/(.*)\u0026#34; - \u0026#34;/$1\u0026#34; # export Prometheus metrics to the specified URI - uri: /prometheus/metrics plugins: public-api: # enable the Prometheus Plugin globally on all Routes global_rules: - id: 1 plugins: prometheus: prefer_name: true #END This will export the metrics to the /prometheus/metrics endpoint in Apache APISIX.\nYou can learn more about the available metrics from the documentation.\nInstrumenting and Exporting Metrics from the Go Service Prometheus has an official Go client library for instrumenting Go applications.\nBy default, Prometheus will expose the default Go metrics. You can also create your own application-specific metrics.\nIn our service, we will expose the default metrics and create our own counter metric to track the number of requests:\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; // Prometheus packages \t\u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promauto\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) // Response stores the Message obtained from the python-app type Response struct { Message string `json:\u0026#34;message\u0026#34;` } // default language and name var ( lang = \u0026#34;en\u0026#34; name = \u0026#34;John\u0026#34; ) // create a new custom Prometheus counter metric var pingCounter = promauto.NewCounter( prometheus.CounterOpts{ Name: \u0026#34;go_app_request_count\u0026#34;, Help: \u0026#34;No of requests handled by the go-app\u0026#34;, }, ) // HelloHandler handles requests to the go-app func HelloHandler(w http.ResponseWriter, r *http.Request) { lang = r.URL.String() name = r.URL.Query()[\u0026#34;name\u0026#34;][0] fmt.Println(\u0026#34;Request for\u0026#34;, lang, \u0026#34;with name\u0026#34;, name) pingCounter.Inc() pUrl := os.Getenv(\u0026#34;PYTHON_APP_URL\u0026#34;) if len(pUrl) == 0 { pUrl = \u0026#34;localhost\u0026#34; } // call the python-app to obtain the translation \tresp, err := http.Get(\u0026#34;http://\u0026#34; + pUrl + \u0026#34;:8000\u0026#34; + lang) if err != nil { log.Fatalln(err) } body, err := ioutil.ReadAll(resp.Body) if err != nil { log.Fatalln(err) } resp.Body.Close() var m Response json.Unmarshal(body, \u0026amp;m) // send back response with \u0026#34;Hello name!\u0026#34; in the specified language \tfmt.Fprintf(w, \u0026#34;%s %s!\u0026#34;, m.Message, name) } func main() { // expose Prometheus metrics \thttp.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) http.HandleFunc(\u0026#34;/\u0026#34;, HelloHandler) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } This will expose the metrics to the endpoint /metrics. You can learn more about the Go client library from its GitHub repo.\nInstrumenting and Exporting Metrics from the Python Service Prometheus also has an official Python client library. There are also third-party libraries that are tailored to fit specific use cases.\nOur service uses FastAPI, and we will use the prometheus_fastapi_instrumentator library to instrument it:\nfrom fastapi import FastAPI from fastapi.middleware.cors import CORSMiddleware from prometheus_fastapi_instrumentator import Instrumentator app = FastAPI() hello = {\u0026#34;en\u0026#34;: \u0026#34;Hello\u0026#34;, \u0026#34;fr\u0026#34;: \u0026#34;Bonjour\u0026#34;, \u0026#34;es\u0026#34;: \u0026#34;Hola\u0026#34;, \u0026#34;ml\u0026#34;: \u0026#34;ഹലോ\u0026#34;} # expose the default Python metrics to the /metrics endpoint Instrumentator().instrument(app).expose(app) @app.get(\u0026#34;/{lang}\u0026#34;) async def get_hello(lang): return {\u0026#34;message\u0026#34;: hello[lang]} You can learn more about creating custom metrics from the documentation.\nConfiguring Prometheus We can now scrape and collect these metrics in Prometheus.\nYou can configure Prometheus to collect metrics from each of the services. By default, Prometheus checks for metrics in the /metrics path:\nglobal: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: \u0026#34;prometheus\u0026#34; static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] - job_name: \u0026#34;go-app\u0026#34; static_configs: - targets: [\u0026#34;go-app:8080\u0026#34;] - job_name: \u0026#34;python-app\u0026#34; static_configs: - targets: [\u0026#34;python-app:8000\u0026#34;] - job_name: \u0026#34;apisix\u0026#34; static_configs: - targets: [\u0026#34;apisix:9080\u0026#34;] metrics_path: \u0026#34;/prometheus/metrics\u0026#34; That\u0026rsquo;s it! Now, if you open up the Prometheus dashboard (default on port 9090) and click on \u0026ldquo;Status\u0026rdquo; from the navbar and \u0026ldquo;Targets,\u0026rdquo; you will be able to see the status of metrics being scraped from your services.\n Prometheus targetsMetrics are successfully being scraped from all sources\n  Querying and Visualizing Metrics in Prometheus Now, you can use the Prometheus dashboard to run queries and complex expressions.\n Querying PrometheusYou can get a lot more complex with your queries than this\n  You can learn more about querying Prometheus in the official documentation.\nUsing Grafana to Query Prometheus Grafana is an open source data visualization platform that works with Prometheus to provide a comprehensive tool for collecting, querying, and visualizing metrics.\nPrometheus is good at collecting metrics and querying but lacks in providing tooling for creating meaningful visualizations. Grafana overcomes this limitation by transforming the collected metrics into visualizations.\nGrafana is also compatible with many other data sources than Prometheus.\nOnce you have deployed Grafana, you can open the web UI (default on port 3000).\nFirst, you have to add Prometheus as a data source. To do this, go to `/datasources or \u0026ldquo;Configuration\u0026rdquo; and \u0026ldquo;Data sources.\u0026rdquo; Click on \u0026ldquo;Add data source\u0026rdquo; and select Prometheus. Specify where Prometheus is deployed, save and test the connection.\n Add Prometheus as a data sourceAdd the details of your Prometheus service, save, and test it\n  Using Pre-built Grafana Dashboards Grafana hosts a public dashboard repository that contains pre-built Grafana dashboards. You can use them in your Grafana instance to quickly visualize relevant metrics.\nWe will use the Go Processes dashboard which will process and visualize the process status published by the Prometheus Go client library.\nTo import this template, first, copy its ID (6671) from the dashboard repository. In your Grafana UI, go to \u0026ldquo;Dashboards\u0026rdquo; and select \u0026ldquo;Import.\u0026rdquo; Paste the ID you copied and click \u0026ldquo;Load.\u0026rdquo;\n Grafana dashboardVisualizes the process status of the Go service\n  You can also explore other pre-built dashboards or create your own. Refer to the documentation to learn more about this.\nWhat\u0026rsquo;s Next? That\u0026rsquo;s it for this tutorial!\nThis article was only an introduction to how you can set up monitoring on your services, and I encourage you to learn more about Prometheus and Grafana from the resources mentioned below:\n Prometheus Best Practices Alerting based on metrics Grafana Dashboards  The complete code and the Docker Compose file for this tutorial are available in monitoring-101.\n","permalink":"https://navendu.me/posts/introduction-to-monitoring-microservices/","summary":"This tutorial walks you through setting up monitoring on a microservice application using Prometheus and Grafana.","title":"An Introduction to Monitoring Microservices with Prometheus and Grafana"},{"content":"As your APIs scale, the need for making them reliable and robust increases.\nThis article discusses the best practices for building reliable APIs by introducing a special kind of reverse proxies called API gateways.\nWe will look into:\n Problems with traditional API designs What API gateways are How API gateways improve APIs and Patterns and examples using API gateways  But first, what are \u0026ldquo;reliable\u0026rdquo; APIs?\nWhat Makes an API Reliable? As a service provider, you might have service-level agreements (SLAs) with your customers, usually quoted in uptime—the amount of time the service is guaranteed to be online and operational.\nUptime is a myopic view of reliability. To understand what it means to be reliable, you have to look at the factors that affect uptime. Once you understand these factors, you will be in a better position to build reliable services.\nLet\u0026rsquo;s look at these factors and the questions they pose:\n Latency: How fast does your API respond to requests? Security: Who can access your API? Is it secure? Downtime Frequency: How frequently is your API down? Consistency: Are your API endpoints constant? Do consumers need to change their code often? Monitoring and Reporting: Can you observe issues and failures in your API? Are you reporting them to your consumers?   What does it mean to be reliable?   As organizations move to cloud native architectures, it becomes difficult for the development teams to account for these factors on each of their services. And as these systems scale, it would be much easier to delegate these responsibilities to a single, separate system. Say hello to API gateways!\nAPI Gateway, the Unified Entrypoint An API gateway acts as a middleman between your clients and your APIs. It will accept all traffic (API calls) like reverse proxies, forwards the request to the required services in your backend, and returns the needed results.\n API gateway   An API gateway can be the central point that handles all the authentication, security, traffic control, and monitoring concerns, leaving the API developers to focus on business needs and making it easier to improve reliability.\n What does an API gateway do?   There are a lot of open source and managed API gateway offerings available. In this article, I will be using Apache APISIX.\nThe following section will describe some of the best practices to make your APIs reliable using API gateways.\nReliability Best Practices with API Gateways We will focus more on the pattern underneath than the actual implementation, as it can vary based on your API gateway choice.\nI will divide these patterns into three categories:\n Authentication and security Monitoring and observability Version control and zero downtime  We will look into each category in detail below.\nAuthentication and Security User Authentication Authenticated requests with API gateways secure client-API interactions. After a client authenticates, your API gateway can use the obtained client details for fine-grained control.\n Authentication   APISIX handles authentication directly through plugins like key-auth and jwt-auth. APISIX also supports OAuth authentication and role-based access control systems like wolf through plugins like openid-connect and wolf-rbac, respectively.\nRate Limiting Intentional (DoS attacks) and unintentional (clients making too many requests) traffic spikes to your APIs can bring them down like a house of cards. Setting up rate limiting will improve the reliability of your systems in handling such scenarios.\nYou can set up rate limiting on your API gateway, and if the number of requests increases above a threshold, the API gateway could either delay or reject the exceeding requests.\n Rate limiting   With APISIX, you can use any of the three plugins to configure rate limits based on number of requests, number of concurrent requests per client, and count (limit-req, limit-conn, limit-count).\nMonitoring and Observability Your API\u0026rsquo;s reliability and your monitoring setup go hand in hand. You can monitor your reliability metrics by setting up monitoring on your API gateway.\n Monitoring   API logs and traces provide detailed information about an API call. This information will help you know when your API has failed or has an error as soon as possible. Silent fails lead to unfixed errors which can cause problems in the future.\nWith some configuration, you will also be able to predict and anticipate traffic for the future, helping you scale reliably.\nAPISIX has plugins that integrate with logging (Apache SkyWalking, RocketMQ), metrics (Prometheus, Datadog), and tracing (OpenTelemetry, Zipkin) platforms/specifications. You can read more on API Observability with APISIX Plugins.\nVersion Control and Zero Downtime Canary Release When switching to new versions of your APIs, you must ensure that you don\u0026rsquo;t drop your traffic. Clients should still be able to make requests to your API and get back the correct response.\nWith an API gateway, you can setup canary releases. This will ensure that your API remains functional during the transition, and you can also roll back to the older version if there are any issues.\nInitially, the API gateway will route all traffic to your API\u0026rsquo;s old version.\n Route all traffic to the old API version   When you have a new version, you can configure the API gateway to route some of your traffic to this new version. You can keep increasing the percentage of traffic to your new service and check if everything is working as expected.\n Route some traffic to the new API version   Finally, you can route all traffic to your new API.\n Route all traffic to the new API version   APISIX uses the traffic-split plugin that lets you control the traffic to your services. You can use it to set up canary releases or your custom release configuration.\nCircuit Breaking When one of your upstream services is unavailable or is experiencing high latency, it needs to be cut off from your system. Otherwise, the client will keep retrying the request, leading to resource exhaustion. This failure can creep into other services in your system and bring them down.\nLike how electrical circuit breakers isolate faulty components from a circuit, API gateways have a circuit breaker feature that disconnects faulty services, keeping the system healthy. Traffic to these services are rerouted or delayed until the service becomes healthy.\n Circuit breaking   APISIX comes with an api-breaker plugin that implements this pattern.\nRedirects As you update your APIs, their endpoints might undergo some change. Traditionally, this would mean that the client application should send requests to the /new-api-endpoint instead of the /old-api-endpoint, meaning your consumers must manually change each call to this API endpoint.\nIf unanticipated, this can break client applications.\nWith an API gateway, you can provide an abstraction layer and redirect requests to the /new-api-endpoint without having the clients change their requests. With proper redirect status codes and messages, you can gradually depreciate the /old-api-endpoint without your consumers experiencing any downtime.\n Redirecting   With APISIX, you can use the redirect plugin to configure redirects.\nConclusion When reliability becomes a primary concern, it is evident that API gateways are necessary as more organizations split their monoliths into microservices and move to cloud native architectures.\nHowever, this does not mean API gateways are for everyone. Depending on your API\u0026rsquo;s size and usage, an API gateway might be overkill, and you can get away with using a reverse proxy with basic routing and load balancing capabilities.\nThe use cases mentioned here only scratch the surface of an API gateway\u0026rsquo;s capabilities. You can learn more about API gateways and Apache APISIX at apisix.apache.org.\n","permalink":"https://navendu.me/posts/best-practices-for-building-reliable-apis/","summary":"As the size of your APIs increase, the need for making them reliable and robust also increases. This article discusses the best practices for designing reliable APIs by introducing you to a special kind of reverse proxies called API gateways.","title":"Best Practices for Building Reliable APIs"},{"content":"When talking to one of our users from the fintech industry during the Apache APISIX Community Meetup in Malaysia, we came across a peculiar feature request: mask confidential data in responses.\nFor example, a response from the upstream might contain sensitive data like credit card numbers, and APISIX should be able to replace it with ******* based on some predefined rules.\nCreating such a plugin in Lua might be trivial or daunting, depending on your level of expertise in APISIX+OpenResty+Nginx. So in this article, we will look at how you can create and run this plugin from the ground up while learning some basics of APISIX plugin development in Lua.\nSetting Things Up You can start with the template plugin from apisix-plugin-template. This contains boilerplate code for creating custom Lua plugins for APISIX.\nTo use the template, go to the repository and click \u0026ldquo;Use this template.\u0026rdquo; You can then clone it to your local machine for modification.\nUnder the apisix/plugins directory, you will find a file named demo.lua. You can rename this to data-mask.lua. This will be the starting point for our custom plugin.\nInitially, the main parts of the file will look like this containing some boilerplate code which includes some imports and variable definitions:\n-- local common libs local require = require local core = require(\u0026#34;apisix.core\u0026#34;) -- module define local plugin_name = \u0026#34;data-mask\u0026#34; -- plugin schema local plugin_schema = { type = \u0026#34;object\u0026#34;, properties = {}, required = {}, } local _M = { version = 0.1, -- plugin version priority = 0, -- the priority of this plugin will be 0 name = plugin_name, -- plugin name schema = plugin_schema, -- plugin schema } -- module interface for schema check -- @param `conf` user defined conf data -- @param `schema_type` defined in `apisix/core/schema.lua` -- @return \u0026lt;boolean\u0026gt; function _M.check_schema(conf, schema_type) return core.schema.check(plugin_schema, conf) end -- module interface for header_filter phase function _M.header_filter(conf, ctx) end -- module interface for body_filter phase function _M.body_filter(conf, ctx) end return _M There are three functions (interfaces for the plugin) declared on the structure _M:\n check_schema: used for validating the plugin configuration and is called when this plugin is enabled on a route. header_filter and body_filter: for modifying the response header and body, respectively, before sending it to the client.  In the end, this returns _M, and the APISIX can use the data from this to get the metadata and functions from the plugin.\nDesigning the Plugin Like every sound engineer, let\u0026rsquo;s first design the plugin before we start writing code.\nThe goal of this plugin is simple:\n The user should be able to define what sensitive data would look like in the plugin configuration (maybe RegEx?). They should be able to define what sensitive data should be replaced with (like *******). APISIX should then modify requests and responses based on these configurations.  So each rule can contain a regular expression and a replacement string. This rule will be applied to the response, and the masked data will be returned to the client:\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;regex\u0026#34;: \u0026#34;.*\u0026#34;, \u0026#34;replace\u0026#34;: \u0026#34;******\u0026#34; }, { \u0026#34;regex\u0026#34;: \u0026#34;.*\u0026#34;, \u0026#34;replace\u0026#34;: \u0026#34;**\u0026#34; } ] } We can now define the JSON schema to validate the plugin configuration. This can help avoid issues with improper plugin configurations during runtime:\nlocal plugin_schema = { type = \u0026#34;object\u0026#34;, properties = { rules = { type = \u0026#34;array\u0026#34;, items = { type = \u0026#34;object\u0026#34;, properties = { regex = { type = \u0026#34;string\u0026#34;, minLength = 1, }, replace = { type = \u0026#34;string\u0026#34;, }, }, required = { \u0026#34;regex\u0026#34;, \u0026#34;replace\u0026#34;, }, additionalProperties = false, }, minItems = 1, }, }, required = { \u0026#34;rules\u0026#34;, }, } The rules here is an array of objects meaning you can have multiple rules for defining what sensitive data should look like and what it should be replaced with. Each object in the array contains two required string fields, regex and replace, just like we designed.\nLet\u0026rsquo;s Write Some Code! We have now decided what the plugin\u0026rsquo;s functionality would look like and added some JSON schema to validate the plugin\u0026rsquo;s configuration.\nWe will first modify the _M.header_filter function, which is called before the response header is sent to the client. But why are we changing this? Isn\u0026rsquo;t our plugin supposed to modify the response body?\nWell, yes. But when we modify the data in the response body (from 2378-4531-5789-1369 to 2378-\\***\\*-\\*\\***-1369), the Content-Length header will no longer be accurate. This can cause the client to interpret that the data returned by the server is abnormal and fail to complete the request.\nSince we haven\u0026rsquo;t modified the request body yet, we cannot calculate the new, accurate value for the Content-Length header. So we need to delete this header value, modify the response body, recalculate the new header value, and set it to the response. To do this in a single sweep, APISIX provides the core.response.clear_header_as_body_modified function:\nfunction _M.header_filter(conf, ctx) core.response.clear_header_as_body_modified() end We can now work on modifying the response body to mask the data. To do this, we must modify the _M.body_filter function.\nSometimes, the upstream response will be sent in chunks (Content-Encoding: chunked), and the body_filter function will be called multiple times. Since each of these chunks are incomplete in itself, we need to cache the data passed each time and call the body_filter function only when all blocks are received and spliced together. And like before, APISIX provides a function, core.response.hold_body_chunk to handle this scenario:\nlocal body = core.response.hold_body_chunk(ctx) if not body then return end Now to mask the response data, we can use the ngx.re.gsub function, which takes in a regular expression and a replacement string and replaces matching strings with the replacement string.\nThe RegEx conforms to the PCRE specification. For example, when the expression is (.*)-(.*)-(.*)-(.*), it will extract the four variables separated by -, and you can use $1, $2, $3, and $4 in the replacement string to refer to the four variables:\nfor _, rule in ipairs(conf.rules) do body = ngx.re.gsub(body, rule.regex, rule.replace, \u0026#34;jo\u0026#34;) end Finally, to set this as the new response body, we will modify the value of ngx.arg[1] as mentioned in the OpenResty docs. Once we set the value of ngx.arg[2] to true, APISIX will send the new response body to the client:\nngx.arg[1] = body ngx.arg[2] = true Combining all these, the body_filter function will look like this:\nfunction _M.body_filter(conf, ctx) local body = core.response.hold_body_chunk(ctx) if not body then return end for _, rule in ipairs(conf.rules) do body = ngx.re.gsub(body, rule.regex, rule.replace, \u0026#34;jo\u0026#34;) end ngx.arg[1] = body ngx.arg[2] = true end Now the only thing left to do is glue everything together. The entire plugin code will look like this:\n-- local common libs local require = require local ipairs = ipairs local ngx_re_gsub = ngx.re.gsub local core = require(\u0026#34;apisix.core\u0026#34;) -- module define local plugin_name = \u0026#34;data-mask\u0026#34; -- plugin schema local plugin_schema = { type = \u0026#34;object\u0026#34;, properties = { rules = { type = \u0026#34;array\u0026#34;, items = { type = \u0026#34;object\u0026#34;, properties = { regex = { type = \u0026#34;string\u0026#34;, minLength = 1, }, replace = { type = \u0026#34;string\u0026#34;, }, }, required = { \u0026#34;regex\u0026#34;, \u0026#34;replace\u0026#34;, }, additionalProperties = false, }, minItems = 1, }, }, required = { \u0026#34;rules\u0026#34;, }, } local _M = { version = 0.1, -- plugin version priority = 0, -- the priority of this plugin will be 0 name = plugin_name, -- plugin name schema = plugin_schema, -- plugin schema } -- module interface for schema check -- @param `conf` user defined conf data -- @param `schema_type` defined in `apisix/core/schema.lua` -- @return \u0026lt;boolean\u0026gt; function _M.check_schema(conf, schema_type) return core.schema.check(plugin_schema, conf) end -- module interface for header_filter phase function _M.header_filter(conf, ctx) core.response.clear_header_as_body_modified() end -- module interface for body_filter phase function _M.body_filter(conf, ctx) local body = core.response.hold_body_chunk(ctx) if not body then return end for _, rule in ipairs(conf.rules) do body = ngx_re_gsub(body, rule.regex, rule.replace, \u0026#34;jo\u0026#34;) end ngx.arg[1] = body ngx.arg[2] = true end return _M Testing the Plugin Let\u0026rsquo;s assume our upstream will return a response like this:\n{ \u0026#34;username\u0026#34;: \u0026#34;jack\u0026#34;, \u0026#34;credit_number\u0026#34;: \u0026#34;2378-4531-5789-1369\u0026#34; } It contains the username and the credit card number, which is far from ideal. We can use the data-mask plugin to rewrite it to 2378-****-****-1369. The configuration would look like this:\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;regex\u0026#34;: \u0026#34;\\\u0026#34;credit_number\\\u0026#34;:\\\u0026#34;(.*)-(.*)-(.*)-(.*)\\\u0026#34;\u0026#34;, \u0026#34;replace\u0026#34;: \u0026#34;\\\u0026#34;credit_number\\\u0026#34;:\\\u0026#34;$1-****-****-$4\\\u0026#34;\u0026#34; } ] } A common way the APISIX plugin developers debug plugins by mocking upstream response is through the serverless-pre-function plugin. Through this plugin, you can run custom Lua code, in our case, to send back a response with un-masked credit card numbers:\nreturn function() ngx.header[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json\u0026#34;; require(\u0026#34;apisix.core\u0026#34;).response.exit(200, { credit_number = \u0026#34;1234-5678-8765-4321\u0026#34;, username = \u0026#34;jack\u0026#34; }) end We can configure both our custom plugin and the serverless-pre-function plugin on the same route as shown below:\ncurl -X PUT \u0026#39;http://localhost:9180/apisix/admin/routes/data-mask\u0026#39; \\ -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;uri\u0026#34;: \u0026#34;/data-mask\u0026#34;, \u0026#34;plugins\u0026#34;: { \u0026#34;serverless-pre-function\u0026#34;: { \u0026#34;phase\u0026#34;: \u0026#34;access\u0026#34;, \u0026#34;functions\u0026#34;: [ \u0026#34;return function() ngx.header[\\\u0026#34;Content-Type\\\u0026#34;] = \\\u0026#34;application/json\\\u0026#34;; require(\\\u0026#34;apisix.core\\\u0026#34;).response.exit(200, {credit_number = \\\u0026#34;1234-5678-8765-4321\\\u0026#34;, username = \\\u0026#34;jack\\\u0026#34;}) end\u0026#34; ] }, \u0026#34;data-mask\u0026#34;: { \u0026#34;rules\u0026#34;: [ { \u0026#34;regex\u0026#34;:\u0026#34;\\\u0026#34;credit_number\\\u0026#34;:\\\u0026#34;(.*)-(.*)-(.*)-(.*)\\\u0026#34;\u0026#34;, \u0026#34;replace\u0026#34;: \u0026#34;\\\u0026#34;credit_number\\\u0026#34;:\\\u0026#34;$1-****-****-$4\\\u0026#34;\u0026#34; } ] } } }\u0026#39; Now if you send a request to the route, you will get back the masked credit card numbers in the response:\ncurl -X GET \u0026#39;http://localhost:9080/data-mask\u0026#39; { \u0026#34;username\u0026#34;: \u0026#34;jack\u0026#34;, \u0026#34;credit_number\u0026#34;: \u0026#34;1234-****-****-4321\u0026#34; } Using in Production You can directly build your own APISIX image with this plugin included for using it in production:\nFROMapache/apisix:3.3.0-debianCOPY ./data-mask.lua /usr/local/apisix/apisix/plugins/data-mask.luaThen run docker build:\ndocker build -t your-own-registry.com/apisix:3.3.0-data-mask . Next, in the configuration file (config.yaml) you can add your plugin to the list:\nplugins: # any other plugins from config-default.yaml - xxxx - data-mask Note: The values in plugins in the config.yaml file override those in the config-default.yaml file. If you want to use other plugins, copy it to the config.yaml file.\nOnce you add it to the configuration file, you should be able to use the plugin on your routes.\nLearn More All the sample code from this article can be found here. You can also use the plugin template to create your own plugins easily.\nAPISIX comes with many in-built plugins; you can refer to its source code for more details on how you can create your own plugins. You can also refer to the lua-nginx-module and this series of OpenResty tutorials to learn more.\n","permalink":"https://navendu.me/posts/data-mask-plugin/","summary":"A tutorial on creating a custom Apache APISIX plugin in Lua through a real use case.","title":"Creating a Custom Data Mask Plugin"},{"content":"A few days ago, one of our workflows to collect and display metrics from APISIX\u0026rsquo;s public channels failed. The workflow could not push the collected metrics to the database, and our charts had week-long missing data.\nFortunately, we had logs that contained the missing metrics, but we needed to convert these to SQL queries to add the data manually. Writing SQL queries from more than a week of logs is tedious. And like any other engineer, I used this as an excuse to build automation.\nThe logs looked like this:\n2023-06-16T04:06:41.4852848Z ##[group]Run python main.py 2023-06-16T04:06:41.4853225Z \u001b[36;1mpython main.py\u001b[0m 2023-06-16T04:06:41.4907169Z shell: /usr/bin/bash -e {0} 2023-06-16T04:06:41.4907511Z env: 2023-06-16T04:06:41.4907865Z pythonLocation: /opt/hostedtoolcache/Python/3.9.17/x64 2023-06-16T04:06:41.4908302Z PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.9.17/x64/lib/pkgconfig 2023-06-16T04:06:41.4908764Z Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.9.17/x64 2023-06-16T04:06:41.4909138Z Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.9.17/x64 2023-06-16T04:06:41.4909544Z Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.9.17/x64 2023-06-16T04:06:41.4909932Z LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.9.17/x64/lib 2023-06-16T04:06:41.4910442Z CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE: /home/runner/work/xxx-metrics-xxx/xxx-metrics-xxx/gha-creds-66629ec6ee542974.json 2023-06-16T04:06:41.4911073Z GOOGLE_APPLICATION_CREDENTIALS: /home/runner/work/xxx-metrics-xxx/xxx-metrics-xxx/gha-creds-66629ec6ee542974.json 2023-06-16T04:06:41.4911775Z GOOGLE_GHA_CREDS_PATH: /home/runner/work/xxx-metrics-xxx/xxx-metrics-xxx/gha-creds-669789ec6ee542974.json 2023-06-16T04:06:41.4912226Z CLOUDSDK_CORE_PROJECT: xxx-metrics-xxx-412278 2023-06-16T04:06:41.4912643Z CLOUDSDK_PROJECT: xxx-metrics-xxx-412278 2023-06-16T04:06:41.4913012Z GCLOUD_PROJECT: xxx-metrics-xxx-412278 2023-06-16T04:06:41.4913364Z GCP_PROJECT: xxx-metrics-xxx-412278 2023-06-16T04:06:41.4913726Z GOOGLE_CLOUD_PROJECT: xxx-metrics-xxx-412278 2023-06-16T04:06:41.4914674Z TWITTER_TOKEN: *** 2023-06-16T04:06:41.4915100Z GITHUB_TOKEN: *** 2023-06-16T04:06:41.4915390Z ##[endgroup] 2023-06-16T04:06:52.7931243Z Starting script for xxx-metrics-xxx 2023-06-16T04:06:52.7931915Z Creating BigQuery SQL statement 2023-06-16T04:06:52.7932395Z Fetching github metrics for APISIX 2023-06-16T04:06:52.7934186Z Metrics fetched: {\u0026#39;star\u0026#39;: 12029, \u0026#39;fork\u0026#39;: 2239, \u0026#39;watcher\u0026#39;: 306, \u0026#39;issue\u0026#39;: 698} 2023-06-16T04:06:52.7940164Z Fetching dockerhub metrics for APISIX 2023-06-16T04:06:52.7940582Z Metrics fetched: {\u0026#39;star\u0026#39;: 68, \u0026#39;pull\u0026#39;: 15336546} 2023-06-16T04:06:52.7947174Z Fetching medium metrics for APISIX 2023-06-16T04:06:52.7947554Z Metrics fetched: {\u0026#39;follower\u0026#39;: 219} 2023-06-16T04:06:52.7949347Z Fetching stackoverflow metrics for APISIX 2023-06-16T04:06:52.7949744Z Metrics fetched: {\u0026#39;count\u0026#39;: 43} 2023-06-16T04:06:52.7955023Z Big Query statement created 2023-06-16T04:06:52.7955343Z Creating BigQuery client 2023-06-16T04:06:52.7955651Z BigQuery client created The highlighted lines show the important stuff. We had to get this data into a table that looks like this:\n   SOURCE ACCOUNT TYPE VALUE CREATED     github APISIX star 12029 2023-06-16 04:06:52.06614 UTC   github APISIX fork 2239 2023-06-16 04:06:52.06614 UTC   github APISIX watcher 306 2023-06-16 04:06:52.06614 UTC   github APISIX issue 698 2023-06-16 04:06:52.06614 UTC   dockerhub APISIX star 68 2023-06-16 04:06:52.06614 UTC   dockerhub APISIX pull 15336546 2023-06-16 04:06:52.06614 UTC   medium APISIX follower 219 2023-06-16 04:06:52.06614 UTC   stackoverflow APISIX count 43 2023-06-16 04:06:52.06614 UTC    To add the missing data to the table, we need to write SQL queries like:\nINSERT INTO DATASET.METRICS(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;github\u0026#39;, \u0026#39;APISIX\u0026#39;, \u0026#39;star\u0026#39;, 12029, \u0026#39;2023-06-16 04:06:52.793239 UTC\u0026#39;); INSERT INTO DATASET.METRICS(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;github\u0026#39;, \u0026#39;APISIX\u0026#39;, \u0026#39;fork\u0026#39;, 2239, \u0026#39;2023-06-16 04:06:52.793239 UTC\u0026#39;); INSERT INTO DATASET.METRICS(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;github\u0026#39;, \u0026#39;APISIX\u0026#39;, \u0026#39;watcher\u0026#39;, 306, \u0026#39;2023-06-16 04:06:52.793239 UTC\u0026#39;); INSERT INTO DATASET.METRICS(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;github\u0026#39;, \u0026#39;APISIX\u0026#39;, \u0026#39;issue\u0026#39;, 698, \u0026#39;2023-06-16 04:06:52.793239 UTC\u0026#39;); INSERT INTO DATASET.METRICS(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;dockerhub\u0026#39;, \u0026#39;APISIX\u0026#39;, \u0026#39;star\u0026#39;, 68, \u0026#39;2023-06-16 04:06:52.794016 UTC\u0026#39;); INSERT INTO DATASET.METRICS(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;dockerhub\u0026#39;, \u0026#39;APISIX\u0026#39;, \u0026#39;pull\u0026#39;, 15336546, \u0026#39;2023-06-16 04:06:52.794016 UTC\u0026#39;); To automate this, we first needed to find the SOURCE and ACCOUNT from the log. For example, from the log:\n2023-06-16T04:06:52.7932395Z Fetching github metrics for APISIX we should extract the SOURCE as github and ACCOUNT as APISIX. An easy way to do this is to use regular expressions (RegEx). And that\u0026rsquo;s precisely what we did.\nAn important caveat here is that we know the structure of the logs beforehand, and we are sure it will be the same throughout. It is easy enough to use RegEx to capture the pattern of the logs and extract the relevant data. With this in mind, we used Python to write some RegEx:\nsource_match = re.search(r\u0026#34;Fetching (\\w+) metrics for (\\w+)\u0026#34;, log) if source_match: source = source_match.group(1) account = source_match.group(2) Once we have the SOURCE and ACCOUNT, we can get the metric TYPEs and its VALUEs from the logs. Since the metrics are in a dictionary string, we can directly store them for easy manipulation:\nmetrics_match = re.search(r\u0026#34;Metrics fetched: (.+)\u0026#34;, next_log) if metrics_match: metrics_data = eval(metrics_match.group(1)) We also needed to convert the timestamps to a different format before storing them in the database. First, we extract the timestamps from the logs using RegEx, parse them, and then convert them to the desired format:\ntimestamp_match = re.search(r\u0026#34;^(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}.\\d{7}Z)\u0026#34;, log) if timestamp_match: timestamp = timestamp_match.group(1) dt = parser.isoparse(timestamp) desired_format = \u0026#34;%Y-%m-%d%H:%M:%S.%f%Z\u0026#34; timestamp = dt.strftime(desired_format) Finally, we will put all of this together to write an SQL query:\nfor metric_type, value in metrics_data.items(): query = f\u0026#34;INSERT INTO {table_name}(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;{source}\u0026#39;, \u0026#39;{account}\u0026#39;, \u0026#39;{metric_type}\u0026#39;, {value}, \u0026#39;{timestamp}\u0026#39;);\u0026#34; queries.append(query) The entire code is shown below:\nimport re import os from datetime import datetime from dateutil import parser log_file = \u0026#34;/content/2023_06_25_Read metrics and store them.txt\u0026#34; query_file = \u0026#34;/content/25_queries.txt\u0026#34; table_name = \u0026#34;DATASET.METRICS\u0026#34; queries = [] with open(log_file, \u0026#34;r\u0026#34;) as file: logs = file.readlines() for log in logs: if \u0026#34;Fetching\u0026#34; in log and \u0026#34;metrics for\u0026#34; in log: source_match = re.search(r\u0026#34;Fetching (\\w+) metrics for (\\w+)\u0026#34;, log) if source_match: source = source_match.group(1) account = source_match.group(2) next_log = logs[logs.index(log) + 1] metrics_match = re.search(r\u0026#34;Metrics fetched: (.+)\u0026#34;, next_log) if metrics_match: metrics_data = eval(metrics_match.group(1)) timestamp_match = re.search(r\u0026#34;^(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}.\\d{7}Z)\u0026#34;, log) if timestamp_match: timestamp = timestamp_match.group(1) dt = parser.isoparse(timestamp) desired_format = \u0026#34;%Y-%m-%d%H:%M:%S.%f%Z\u0026#34; timestamp = dt.strftime(desired_format) for metric_type, value in metrics_data.items(): query = f\u0026#34;INSERT INTO {table_name}(SOURCE, ACCOUNT, TYPE, VALUE, CREATED) VALUES (\u0026#39;{source}\u0026#39;, \u0026#39;{account}\u0026#39;, \u0026#39;{metric_type}\u0026#39;, {value}, \u0026#39;{timestamp}\u0026#39;);\u0026#34; queries.append(query) with open(query_file, \u0026#34;w\u0026#34;) as file: for query in queries: print(query) file.write(query + \u0026#34;\\n\u0026#34;) We were able to run the generated queries to patch the missing data in the database. The entire process was trivial and took about 30-40 minutes.\nTo sum it up, RegEx is powerful, and Python is trivial. Setting up similar automation to solve problems that would otherwise require grunt work is relatively straightforward. But it is also easy to go down the automation rabbit hole ending up doing more than some little manual work. I will leave you with a line:\n Little Alice fell\nd\no\nw\nn\nthe hOle,\nbumped her head\nand bruised her soul\n Lewis Carroll  Alice in Wonderland    ","permalink":"https://navendu.me/posts/regex-grunt-work/","summary":"My recent experience in using regular expressions to automate a menial task.","title":"Grunt Work with RegEx"},{"content":"A key feature of Apache APISIX is its pluggable architecture. In addition to providing 80\u0026#43; Lua plugins out of the box, APISIX also supports external plugins written in other languages through plugin runners and WebAssembly (Wasm).\nIn this article, we will write a \u0026ldquo;tiny\u0026rdquo; Go plugin for APISIX, compile it to a Wasm binary, run it in APISIX, and learn how it all works. We will also compare the benefits and costs of using Wasm plugins, external plugins (plugin runners), and native Lua plugins.\nAPISIX and Wasm APISIX supports Wasm through the WebAssembly for Proxies (proxy-wasm) specification. APISIX is a host environment that implements the specification, and developers can use the SDKs available in multiple languages to create plugins.\nUsing Wasm plugins in APISIX has multiple advantages:\n Many programming languages compile to Wasm. This allows you to leverage the capabilities of your tech stack in APISIX plugins. The plugins run inside APISIX and not on external plugin runners. This means you compromise less on performance while writing external plugins. Wasm plugins run inside APISIX but in a separate VM. So even if the plugin crashes, APISIX can continue to run. APISIX can only maintain its Wasm support without having to maintain plugin runners for multiple languages*.  * These advantages come with a set of caveats which we will look at later.\nAPISIX\u0026rsquo;s plugin architecture below shows native Lua plugins, external plugins through plugin runners, and Wasm plugins:\n Lua plugins, plugin runners, and Wasm plugins   A \u0026ldquo;Tiny\u0026rdquo; Go Plugin Let\u0026rsquo;s get coding! To write a Go plugin, we will use the proxy-wasm-go-sdk.\nReading through the documentation, you will understand why this plugin is called \u0026ldquo;tiny,\u0026rdquo; i.e., the SDK uses the TinyGo compiler instead of the official Go compiler. You can read more about why this is the case on the SDK\u0026#39;s overview page, but the TLDR version is that the Go compiler can only produce Wasm binaries that run in the browser.\nFor our example, we will create a plugin that adds a response header. The code below is pretty self-explanatory, and you can refer to other plugins for more implementation details:\n// references: // https://github.com/tetratelabs/proxy-wasm-go-sdk/tree/main/examples // https://github.com/apache/apisix/blob/master/t/wasm/ package main import ( \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\u0026#34; \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\u0026#34; \u0026#34;github.com/valyala/fastjson\u0026#34; ) func main() { proxywasm.SetVMContext(\u0026amp;vmContext{}) } // each plugin has its own VMContext. // it is responsible for creating multiple PluginContexts for each route. type vmContext struct { types.DefaultVMContext } // each route has its own PluginContext. // it corresponds to one instance of the plugin. func (*vmContext) NewPluginContext(contextID uint32) types.PluginContext { return \u0026amp;pluginContext{} } type header struct { Name string Value string } type pluginContext struct { types.DefaultPluginContext Headers []header } func (ctx *pluginContext) OnPluginStart(pluginConfigurationSize int) types.OnPluginStartStatus { data, err := proxywasm.GetPluginConfiguration() if err != nil { proxywasm.LogErrorf(\u0026#34;error reading plugin configuration: %v\u0026#34;, err) return types.OnPluginStartStatusFailed } var p fastjson.Parser v, err := p.ParseBytes(data) if err != nil { proxywasm.LogErrorf(\u0026#34;error decoding plugin configuration: %v\u0026#34;, err) return types.OnPluginStartStatusFailed } headers := v.GetArray(\u0026#34;headers\u0026#34;) ctx.Headers = make([]header, len(headers)) for i, hdr := range headers { ctx.Headers[i] = header{ Name: string(hdr.GetStringBytes(\u0026#34;name\u0026#34;)), Value: string(hdr.GetStringBytes(\u0026#34;value\u0026#34;)), } } return types.OnPluginStartStatusOK } // each HTTP request to a route has its own HTTPContext func (ctx *pluginContext) NewHttpContext(contextID uint32) types.HttpContext { return \u0026amp;httpContext{parent: ctx} } type httpContext struct { types.DefaultHttpContext parent *pluginContext } func (ctx *httpContext) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.Action { plugin := ctx.parent for _, hdr := range plugin.Headers { proxywasm.ReplaceHttpResponseHeader(hdr.Name, hdr.Value) } return types.ActionContinue } To compile our plugin to a Wasm binary, we can run:\ntinygo build -o custom_response_header.go.wasm -scheduler=none -target=wasi ./main.go Configuring APISIX to Run the Plugin To use the Wasm plugin, we first have to update our APISIX configuration file to add this:\nwasm: plugins: - name: custom-response-header priority: 7000 file: /opt/apisix/wasm/custom_response_header.go.wasm Now we can create a route and enable this plugin:\nroutes: - uri: /* upstream: type: roundrobin nodes: \u0026#34;127.0.0.1:80\u0026#34;: 1 plugins: custom-response-header: conf: |{ \u0026#34;headers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;X-Go-Wasm\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;APISIX\u0026#34; }] } #END Testing the Plugin We can send a request to the created route to test the plugin:\ncurl http://127.0.0.1:9080 -s --head The response header would be as shown below:\n... X-Go-Wasm: APISIX Wasm for the Win? From this article, it seems like using Wasm plugins benefits both users and APISIX maintainers.\nA test using our example custom-response-header function implemented through a Lua plugin, an external Go plugin runner, and a Wasm plugin show how the performance varies:\n Wasm plugins aren\u0026#39;t that bad!Running 30s tests with 5 threads and 50 connections using wrk\n  Looking solely at this example, it might be tempting to ask why anyone would want to use plugin runners or write Lua plugins. Well, all the advantages of using Wasm comes with the following caveats:\n Limited plugins: The Wasm implementation of programming languages often lacks complete support. For Go, we were limited to using the TinyGo compiler, similar to other languages. Immature stack: Wasm and its usage outside the browser is still a relatively new concept. The proxy-wasm spec also has its limitations due to its relative novelty. Lack of concurrency: Wasm does not have built-in concurrency support. This could be a deal breaker for typical APISIX uses cases where high performance is critical. Better alternatives: Since APISIX can be extended using Lua plugins or plugin runners, there are always alternatives to using Wasm.  Despite these caveats, the future of Wasm in APISIX and other proxies seems promising. You can choose to hop on the Wasm bandwagon if its benefits tip the scale against these costs. But currently, APISIX plans to continue supporting all three ways of creating custom plugins for the foreseeable future.\n","permalink":"https://navendu.me/posts/tiny-apisix-plugin/","summary":"A \u0026ldquo;tiny\u0026rdquo; example to demonstrate how Apache APISIX supports Wasm plugins.","title":"A \"Tiny\" APISIX Plugin"},{"content":"People typically use the MQTT protocol when transferring data from IoT devices because of its low overhead and ease of implementation.\nMQTT was created for sending small chunks of data over unreliable networks and uses a binary format rather than the typical text-based format used in protocols like HTTP or SMTP. With client libraries in multiple programming languages, you are unlikely to have to implement the protocol on your own but use an existing library.\nAs your IoT devices move to the cloud, you face a different challenge of managing multiple protocols seamlessly. Questions arise like \u0026ldquo;How can I use a single entry point for all my traffic?\u0026rdquo;, \u0026ldquo;How do I decouple my IoT and cloud infrastructure with little overhead?\u0026rdquo;, and \u0026ldquo;How can I do all this securely?\u0026rdquo;\nThis article attempts to answer these questions using Apache APISIX, which supports HTTP and MQTT protocols, to proxy requests between your devices, message brokers, and the cloud.\nWhy an MQTT Proxy? APISIX is primarily used as an API gateway for routing HTTP traffic.\nAs an IoT developer recently pointed out, APISIX\u0026rsquo;s support for stream routes and, in extension, the MQTT protocol is often overlooked. Let\u0026rsquo;s change this by looking at an end-to-end example of how APISIX can act as an MQTT proxy.\nIn this example, you own two warehouses and an office. Data from your warehouses and the office are sent to the storage, monitoring, and analytics services deployed in your cloud through APISIX over HTTP. We will refer to this as \u0026ldquo;the system.\u0026rdquo;\n The systemApache APISIX will route all requests from your warehouses and office to appropriate services in your cloud backend. Learn more about APISIX at apisix.apache.org\n  You now decide to add two IoT devices to improve the efficiency of the system:\n A flow meter to measure the flow rate of water in your warehouses. A light sensor to measure the illuminance in your office and warehouses.  These devices are small, energy-efficient, and support the MQTT protocol. The measurements from the flow meter are used by an automatic valve to control the water flow rate, and the light sensor is used to maintain optimal lighting.\n System upgradeHere, the devices will send messages directly to your MQTT broker. Multiple clients including phones, PCs, valves, and your cloud services are subscribed to the broker\n  As shown above, you can deploy this system independently from your cloud infrastructure. But the toll of maintaining separate infrastructures for your IoT devices and cloud can also be pretty high both in terms of cost and effort.\nHowever, combining the two systems is not a trivial task. You must work with multiple protocols but use a single entry point to reduce infrastructure costs and maintenance overhead. This is where being pragmatic and using APISIX pays you off.\nA Unified Entry Point Apache APISIX supports MQTT and HTTP protocols and can work as a proxy between your IoT devices and the cloud. APISIX supports this through the mqtt-proxy plugin, which allows it to load balance and route MQTT messages between brokers.\nWith the APISIX MQTT proxy, your system can look something like this:\n APISIX MQTT proxyNow in addition to your HTTP traffic, APISIX also manages your MQTT traffic and can route it between your message brokers\n  Now, APISIX will do all the heavy lifting and process both HTTP and MQTT requests, removing any additional overhead from your cloud or IoT devices.\nRouting MQTT messages is useful when you want to use multiple brokers. The mqtt-proxy plugin routes the messages based on its clientId using a consistent hashing algorithm. This allows you to send messages from different clients to different brokers dynamically. For example, you can send messages from the flow meters and light sensors to different brokers.\nIn practice, you can configure this in the mqtt-proxy plugin. The example below shows the configuration in standalone mode:\napisix: enable_admin: false stream_proxy: only: false # allow HTTP as well tcp: - addr: 9000 tls: false deployment: role: data_plane # deploy APISIX in standalone mode as a data plane role_data_plane: config_provider: yaml stream_plugins: - mqtt-proxy # enable the mqtt-proxy plugin #END stream_routes: - id: 1 upstream_id: 1 plugins: mqtt-proxy: protocol_name: \u0026#34;MQTT\u0026#34; protocol_level: 5 # use MQTT 5.0 upstreams: # configure multiple brokers - nodes: \u0026#34;host.docker.internal:1883\u0026#34;: 1 \u0026#34;host.docker.internal:1884\u0026#34;: 1 type: chash key: mqtt_client_id id: 1 #END You can also use the Admin API to configure this on the fly:\ncurl http://127.0.0.1:9180/apisix/admin/stream_routes/1 -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;plugins\u0026#34;: { \u0026#34;mqtt-proxy\u0026#34;: { \u0026#34;protocol_name\u0026#34;: \u0026#34;MQTT\u0026#34;, \u0026#34;protocol_level\u0026#34;: 5 } }, \u0026#34;upstream\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;chash\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;mqtt_client_id\u0026#34;, \u0026#34;nodes\u0026#34;: [ { \u0026#34;host\u0026#34;: \u0026#34;host.docker.internal\u0026#34;, \u0026#34;port\u0026#34;: 1883, \u0026#34;weight\u0026#34;: 1 }, { \u0026#34;host\u0026#34;: \u0026#34;host.docker.internal\u0026#34;, \u0026#34;port\u0026#34;: 1884, \u0026#34;weight\u0026#34;: 1 } ] } }\u0026#39; Decoupling from the Cloud Using multiple message brokers is a straightforward way to separate the cloud from your IoT devices while keeping them connected.\nIn our example, you can deploy a broker locally with the IoT devices and use a different broker for the cloud. APISIX will be able to route traffic between the two as shown below:\n DecouplingThe broker in the local network will communicate with the valve and rest of the clients in the local network. APISIX will route the required messages to the broker deployed in your cloud which different clients in the cloud can subscribe to\n  Decoupling has its benefits. Now APISIX can handle the communication with the cloud, and your devices can communicate with APISIX instead of directly with the cloud.\nTLS All the Way! Another critical requirement for any communication system is security.\nWith APISIX as the MQTT proxy, security would mean securing the IoT device-to-APISIX and APISIX-to-cloud channels. The diagram below illustrates how APISIX achieves this through TLS authentication:\n Securing communicationsAPISIX can secure the client-to-APISIX and APISIX-to-broker communication\n  In practice, we can update our configuration files to enable mutual TLS for the route (IoT-to-APISIX) and the upstream (APISIX-to-cloud):\napisix: enable_admin: false stream_proxy: only: false # allow HTTP as well tcp: - addr: 9000 tls: true # enable TLS authentication deployment: role: data_plane # deploy APISIX in standalone mode as a data plane role_data_plane: config_provider: yaml stream_plugins: - mqtt-proxy # enable the mqtt-proxy plugin #END stream_routes: - id: 1 upstream_id: 1 sni: mqtt.navendu.me plugins: mqtt-proxy: protocol_name: \u0026#34;MQTT\u0026#34; protocol_level: 5 # use MQTT 5.0 upstreams: # configure multiple brokers - nodes: \u0026#34;host.docker.internal:1883\u0026#34;: 1 \u0026#34;host.docker.internal:1884\u0026#34;: 1 scheme: tls # enable TLS on upstream type: chash key: mqtt_client_id id: 1 #END For a complete guide on configuring TLS, refer to the documentation.\nImproving the MQTT Proxy One of the key features of Apache APISIX is that it is entirely open source and extensible. You can add more features like MQTT transcoding to the MQTT plugin or create new plugins for your specific MQTT proxy.\nWe are seeing more users using APISIX for its MQTT support, which is bound to increase further as MQTT becomes the default protocol for communication between IoT devices.\nA special thank you to Alfonso González for his input and review. Alfonso and his team use APISIX\u0026rsquo;s MQTT proxy features in production.\n","permalink":"https://navendu.me/posts/iot-to-cloud/","summary":"A guide to using Apache APISIX as an MQTT proxy to connect IoT devices to the cloud.","title":"Connecting IoT Devices to the Cloud"},{"content":"India is undoubtedly one of the biggest creators and adopters of artificial intelligence.\nThe AI Index Report by Stanford University estimates $3.24 billion in investments to AI startups in India in 2022. Although this is low compared to the $47.36 billion and $13.41 billion investments in the US and China, respectively, Indian investors have spent almost half the money they have spent in the last decade on AI startups in the previous year.\nAs governments and tech leaders across the world try to get ahead of the technology, preaching inevitable doom and pushing for urgent legislation, India and its government have been surprisingly pragmatic with their approach to AI.\n Less but moreAlthough US and China still leads the AI market, India has seen a lot more investment in the last year than it had ever before. Data source: The AI Index Report\n  On April 2023, the Ministry of Electronics and Information Technology of the Government of India clarified its position to not introduce new legislation to curb AI growth. I would not be surprised to see more investment in AI and related tech in 2023 because of this policy stance.\nBut \u0026ldquo;regulate\u0026rdquo; and \u0026ldquo;don\u0026rsquo;t regulate\u0026rdquo; aren\u0026rsquo;t the only choices for the government, although they are pronounced. So what else can the government do?\nIn this article, I will try to answer this question with eight possible approaches with increasing levels of state intervention. Let\u0026rsquo;s start with the least effortless and intrusive approach, which is to\nDo Nothing Doing nothing has its merits! The AI landscape is still a bit like the wild west, constantly changing and evolving. By leaving it alone, the government can allow the system to develop organically.\nBut inaction also has consequences. A lack of accountability could lead to potential misuse of AI and put us on a path to the preached doom.\nIt will be up to the government and policymakers to decide if the benefits of inaction outweigh the costs. However, India left this station long ago and is more proactive with its approach.\nEngage in Rhetoric This is a slightly more intervening activity than doing nothing.\nThe Indian government has been actively engaged in AI for some time by setting up bodies like The National AI Portal of India (INDIAai). The body does not actively do anything to intervene in AI growth. Still, it hints that AI is an important technology, and the country needs to prepare to tackle future challenges it poses.\nDiscussions in the parliament on AI regulation and using AI for the public good can also be seen as engagement by the government. With AI potentially altering the course of human history, being in the loop with such discussions might be a bare minimum action in any government. Nudge Change NITI Aayog, the government\u0026rsquo;s public policy think tank, has published multiple strategy guides on how AI can be used in the country. These are just guidelines and not concrete legislation for using AI.\nSuch initiatives can nudge creators and users of AI in the country to act in a certain way without the government intervening too much. For example, in a set of guidelines titled \u0026ldquo;Responsible AI,\u0026rdquo; the policymakers suggest how AI should be used with the country\u0026rsquo;s existing public infrastructure with responsibility. AI creators can choose to adhere to these guidelines.\nUmpire Just like an umpire ensures a fair game, the government will ensure a fair market environment by setting regulations and coercing all players to follow these regulations.\nIn a recent US Senate hearing, Sam Altman, the CEO of OpenAI, Christina Montgomery, IBM\u0026rsquo;s chief privacy and trust officer, and Gary Marcus, a well-known professor, and an AI critic, unanimously agreed with the lawmakers that AI should be regulated. Although this seems like a fair idea, this could lead to drastic outcomes like regulatory capture, where large companies use the regulations in their favor to build a market monopoly.\nIndia\u0026rsquo;s decision to not introduce new regulations is in stark contrast to what the US and EU are leaning on currently and has the potential to build the country into a leading AI economy.\nMarginally Incentivize The government can also choose to incentivize private players to work on AI through tax reductions or special economic zones. These are small changes to alter the behavior of private players to encourage them to work in AI.\nThe government can also start adopting AI projects like the new AI traffic camera system in Kerala, which could result in more companies building AI products. For the government, this would mean they can delegate the responsibility to acquire and apply knowledge about AI to these private players instead of trying to build this capacity on their own.\nDrastically Change Incentives  ChatGPT was banned in Italy earlier this year, stating privacy concerns. The Indian government can follow the Italian way and similarly ban AI tools (we know the Indian government likes to ban software).\nBut banning something is always a wrong solution, especially when it comes to software. There will still be a lot of ways (like using VPNs) for Italians to access ChatGPT, and in reality, the government will not have any reliable way to enforce this ban.\nAn AI ban in India could result in companies moving out of India to set up shop in countries with better policies, hurting India in the long run.\nThe government can also choose to go the other way and provide significant incentives like funding and low tax rates to bring in more companies to work on AI. The tricky part is to design incentives that effectively change behavior.\nAn interesting thing here was that the ban in Italy forced OpenAI to improve the privacy measures in ChatGPT. So the question of what works and doesn\u0026rsquo;t isn\u0026rsquo;t completely obvious.\nChange/Reassign Ownership The government can take over and nationalize AI companies in the country as it did with banks.\nThis might be the worst thing the government can do and can take us back to a Soviet dystopia. Although the central and state governments in the country like public sector undertaking, I highly doubt it would do something similar with AI.\nA possibility of this happening is in countries like China or North Korea, where the government might take full or partial ownership of AI companies.\nDIY Finally, the government can choose to build AIs of its own. As AI becomes a tool for dominance in international relations, developing AIs in areas like defense might be a good strategy for India.\nMany technologies we use today have origins within government institutions, particularly the military. Since AI is already public and changing rapidly, the government might have to assess whether it can justify spending resources to build DIY AIs that could easily be inferior to new AIs in the market.\nBut I can see the government trying DIY on a smaller scale. An interactive AI that can answer people\u0026rsquo;s questions about the government could be a relatively small effort and might be the type of AIs that the government would build itself in the future.\nWhat Should the Government Do? Deciding what to do fundamentally boils down to two things: the capacity of the state and the extent to which the state wants to intervene.\nIndia currently has a low capacity in this area, and its direction on policy actions is towards the less intrusive side of the spectrum. This is the right approach for the country and would result in the best possible outcomes.\nBut the AI landscape is constantly changing, and India\u0026rsquo;s policy stances should change with it. Having more technologists engage and work in policy matters and active fostering from the government could help the country emerge as a leader in this AI revolution.\n","permalink":"https://navendu.me/posts/government-and-ai/","summary":"What can the Indian Government do about artificial intelligence?","title":"Eight Things the Indian Government Can Do about AI"},{"content":"Apache APISIX is primarily used to handle north-south traffic and often sits at the boundary between client applications and backend services.\nWith the APISIX Ingress controller, APISIX can also control the ingress-egress traffic in Kubernetes clusters with native configuration.\nBut as organizations embrace microservices, there is a new challenge to handle the east-west traffic between these microservices.\nService meshes like Istio solve this by removing the networking responsibility from the microservice developer, providing an additional L4/L7 networking layer.\nWith the new Amesh library and Istio, Apache APISIX can also be used as a service mesh, specifically as a data plane for Istio, bringing all its traffic management capabilities to service-to-service communication.\nIn this article, we will examine what Amesh is, how it was developed, and how it is used to bring APISIX into a service mesh.\nIstio and the xDS Protocol Istio is one of the most widely used service meshes.\nUnder the hood, Istio uses Envoy as the reverse proxy in its sidecar containers.\n Istio service meshIstio uses Envoy proxy as the data plane\n  Istio manages traffic by dynamically configuring the sidecars using Envoy\u0026#39;s xDS APIs.\nThe xDS APIs are a way to configure Envoy with incremental changes instead of simple configurations with static files.\nAlthough these APIs were initially intended for configuring Envoy, the APIs have evolved to become a universal data plane API. Any data plane proxy can implement these APIs, and any control plane can use this API to work with these data plane proxies.\nIn Istio, this means that you can replace the default Envoy data plane with any data plane that implements the xDS APIs. So you can replace Envoy with APISIX to get its traffic management capabilities in a service mesh.\nBut APISIX does not support the xDS APIs out-of-the-box. And that\u0026rsquo;s where Amesh comes in.\nAmesh Amesh is a library that translates the data from Istio\u0026rsquo;s control plane to APISIX configuration.\n APISIX \u0026#43; Amesh \u0026#43; IstioAPISIX replaces Envoy as the data plane for Istio\n  Istio communicates to the data plane through the xDS APIs. Amesh supports these APIs and then converts them to APISIX configuration.\nThis is similar to how APISIX and the APISIX Ingress controller work. The Ingress controller converts the configurations defined using the Ingress or the Gateway API to APISIX format.\n APISIX \u0026#43; APISIX Ingress controllerThe Ingress controller translates configuration defined using the Kubernetes Ingress or Gateway APIs to APISIX configuration\n  Since the xDS APIs are supported by more service meshes like Linkerd and Open Service Mesh, APISIX can also work with them using the Amesh library. Amesh is still in the early stages of development and currently works with Istio v1.13.1.\nWith Amesh + APISIX, you can use Istio as you would typically. Once you configure traffic rules with Istio\u0026rsquo;s virtual service, APISIX can implement these rules.\nAPISIX\u0026rsquo;s extended capabilities come through its 80\u0026#43; plugins. To use APISIX plugins with Istio, we deploy an Amesh control plane component called the Amesh controller.\nThe Amesh controller takes a plugin configuration defined with the AmeshPluginConfig CRD and converts it to an APISIX plugin configuration.\nAll of this will enable us to leverage the full capabilities of APISIX within sidecar containers.\nAPISIX + Istio Mesh Let\u0026rsquo;s put everything we learned about in action.\nWe will build the Amesh images, configure Istio to use APISIX sidecars, deploy Istio, and test everything by running a sample application.\nBuilding the Images We will build three images:\n amesh-iptables: used for creating an init container to set up some iptables rules. These rules are to direct all traffic through APISIX. amesh-sidecar: used for creating the sidecar container. amesh-controller: used for creating the Amesh controller control plane component. This controller is used to configure APISIX plugins.  First, clone the Amesh repo:\ngit clone https://github.com/api7/Amesh.git cd Amesh You can build and push these images to your own registry.\nAdd the address of your registry in an environment variable before you run the build, as shown below:\nexport REGISTRY=\u0026#34;docker.io/navendu\u0026#34; make prepare-images If you don\u0026rsquo;t want to build your own images, you can use these images:\ndocker pull navendup/amesh-iptables:dev docker pull navendup/amesh-sidecar:dev docker pull navendup/amesh-controller:latest Deploy Amesh Controller and Install CRDs We will use Helm to deploy everything to the Kubernetes cluster. I use minikube in these examples.\nWe will start by creating a new namespace istio-system:\nkubectl create namespace istio-system To deploy the Amesh controller, run:\nhelm install amesh-controller -n istio-system \\ ./controller/charts/amesh-controller You also need to install CRDs to work with the Amesh controller:\nkubectl apply -k controller/config/crd/ Configure and Deploy Istio + APISIX Before deploying the service mesh, we will set some environment variables:\nexport ISTIO_RELEASE=1.13.1 export REGISTRY=\u0026#34;docker.io/navendup\u0026#34; Then we will use Helm to deploy the service mesh:\nhelm install amesh \\ --namespace istio-system \\ --set pilot.image=istio/pilot:\u0026#34;$ISTIO_RELEASE\u0026#34; \\ --set global.proxy.privileged=true \\ --set global.proxy_init.image=\u0026#34;$REGISTRY\u0026#34;/amesh-iptables:dev \\ --set global.proxy.image=\u0026#34;$REGISTRY\u0026#34;/amesh-sidecar:dev \\ --set global.imagePullPolicy=IfNotPresent \\ --set global.hub=\u0026#34;docker.io/istio\u0026#34; \\ --set global.tag=\u0026#34;$ISTIO_RELEASE\u0026#34; \\ ./charts/amesh Now we have the service mesh and Amesh controller deployed. Next, let\u0026rsquo;s deploy a sample application to test our service mesh.\nDeploying Bookinfo We will use Istio\u0026rsquo;s Bookinfo sample app.\nFirst, we will add a label to the default namespace to automatically inject sidecars to any pod in the namespace:\nkubectl label ns default istio-injection=enabled Then we can deploy Bookinfo by running:\nkubectl apply -f e2e/bookinfo/bookinfo.yaml This will spin up the Bookinfo application, and each of the pods will have APISIX sidecars:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-cbn87 2/2 Running 0 55s productpage-v1-6b746f74dc-tntc8 2/2 Running 0 55s ratings-v1-b6994bb9-r5j45 2/2 Running 0 55s reviews-v1-545db77b95-n657s 2/2 Running 0 55s reviews-v2-7bf8c9648f-zn97s 2/2 Running 0 55s reviews-v3-84779c7bbc-wn8k2 2/2 Running 0 55s Testing the Mesh To access the Bookinfo application, we would need an ingress gateway.\nYou can use APISIX for this ingress gateway, but that\u0026rsquo;s for another time. For now, we can just use port-forward to access the product-page service:\nkubectl port-forward productpage-v1-6b746f74dc-tntc8 9080:9080 Now if we open up localhost:9080, we will be able to see our sample application.\n Bookinfo homepageThis uses the reviews-v3 service. If you refresh the page, it will cycle through the reviews service\n  Each time you refresh the page, the reviews are pulled from a different version of the reviews service (we deployed three versions).\nNow let\u0026rsquo;s apply a rule using virtual services that routes all traffic to v1 versions of the services.\nThe rule is self-explanatory, and it would look like this:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: productpage spec: hosts: - productpage http: - route: - destination: host: productpage subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ratings spec: hosts: - ratings http: - route: - destination: host: ratings subset: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: details spec: hosts: - details http: - route: - destination: host: details subset: v1 --- You can apply it to your cluster by running:\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/networking/virtual-service-all-v1.yaml Now if we go back to our application and hit refresh, it will stop cycling through the multiple versions of the reviews service and will only route to the v1 version.\n Routing to only v1 versions of servicesNotice how the look of the reviews section changed here. It will remain the same even if you refresh the page\n  To summarize, we configure a rule in Istio, and Istio implements it using its sidecar containers with Apache APISIX. Neat!\nAmesh Beyond Amesh is an experimental project, and it is still in its infancy.\nFuture versions of the project aim to support more features through virtual services.\nYou can contribute to improving the project or keep track of new versions on GitHub.\n","permalink":"https://navendu.me/posts/amesh/","summary":"Apache APISIX is generally used to manage north-south traffic in Kubernetes and often sits at the edge of a cluster. With Amesh, APISIX is now a service mesh.","title":"APISIX Service Mesh"},{"content":"\u0026ldquo;High speed,\u0026rdquo; \u0026ldquo;minimum latency,\u0026rdquo; and \u0026ldquo;ultimate performance\u0026rdquo; are often used to characterize Apache APISIX. Even when someone asks me about APISIX, my answer always includes \u0026ldquo;high-performance cloud native API gateway.\u0026rdquo;\nPerformance benchmarks (vs. Kong, Envoy) confirm these characteristics are indeed accurate (test yourself).\n \u0026#34;High speed,\u0026#34; \u0026#34;minimum latency,\u0026#34; and \u0026#34;ultimate performance\u0026#34;Tests run for 10 rounds with 5000 unique routes on Standard D8s v3 (8 vCPUs, 32 GiB memory)\n  But how does APISIX achieve this?\nTo answer that question, we must look at three things: etcd, hash tables, and radix trees.\nIn this article, we will look under the hood of APISIX and see what these are and how all of these work together to keep APISIX maintaining peak performance while handling significant traffic.\netcd as the Configuration Center APISIX uses etcd to store and synchronize configurations.\netcd is designed to work as a key-value store for configurations of large-scale distributed systems. APISIX is intended to be distributed and highly scalable from the ground up, and using etcd over traditional databases facilitates that.\n APISIX architectureUsing an etcd cluster to store and manage APISIX configuration\n  Another key indispensable feature for API gateways is to be highly available, avoiding downtime and data loss. You can efficiently achieve this by deploying multiple instances of etcd to ensure a fault-tolerant, cloud native architecture.\nAPISIX can read/write configurations from/to etcd with minimum latency. Changes to the configuration files are notified instantly, allowing APISIX to monitor only the etcd updates instead of polling a database frequently, which can add performance overhead.\nThis chart summarizes how etcd compares with other databases.\nHash Tables for IP Addresses IP address-based allowlists/denylists are a common use case for API gateways.\nTo achieve high performance, APISIX stores the list of IP addresses in a hash table and uses it for matching (O(1)) than iterating through the list (O(N)).\nAs the number of IP addresses in the list increases, the performance impact of using hash tables for storage and matching becomes apparent.\nUnder the hood, APISIX uses the lua-resty-ipmatcher library to implement this functionality. The example below shows how the library is used:\nlocal ipmatcher = require(\u0026#34;resty.ipmatcher\u0026#34;) local ip = ipmatcher.new({ \u0026#34;162.168.46.72\u0026#34;, \u0026#34;17.172.224.47\u0026#34;, \u0026#34;216.58.32.170\u0026#34;, }) ngx.say(ip:match(\u0026#34;17.172.224.47\u0026#34;)) -- true ngx.say(ip:match(\u0026#34;176.24.76.126\u0026#34;)) -- false The library uses Lua tables which are hash tables. The IP addresses are hashed and stored as indices in a table, and to search for a given IP address, you just have to index the table and test whether it is nil or not.\n Storing IP addresses in a hash tableTo search for an IP address, it first computes the hash (index) and checks its value. If it is non-empty, we have a match. This is done in constant time O(1).\n  Radix Trees for Routing Please forgive me for tricking you into a data structures lesson! But hear me out; this is where it gets interesting.\nA key area where APISIX optimizes performance is route matching.\nAPISIX matches a route with a request from its URI, HTTP methods, host, and other information (see router). And this needs to be efficient.\nIf you have read the previous section, an obvious answer would be to use a hash algorithm. But route matching is tricky because multiple requests can match the same route.\nFor example, if we have a route /api/*, then both /api/create and /api/destroy must match the route. But this is not possible with a hash algorithm.\nRegular expressions can be an alternate solution. Routes can be configured in a regex, and it can match multiple requests without the need to hardcode each request.\nIf we take our previous example, we can use the regex /api/[A-Za-z0-9]+ to match both /api/create and /api/destroy. More complex regexes could match more complex routes.\nBut regex is slow! And we know APISIX is fast. So instead, APISIX uses radix trees which are compressed prefix trees (trie) that work really well for fast lookups.\nLet\u0026rsquo;s look at a simple example. Suppose we have the following words:\n romane romanus romulus rubens ruber rubicon rubicundus  A prefix tree would store it like this:\n Prefix treeThe highlighted traversal shows the word \u0026ldquo;rubens\u0026rdquo;\n  A radix tree optimizes a prefix tree by merging child nodes if a node only has one child node. Our example trie would look like this as a radix tree:\n Radix treeThe highlighted traversal still shows the word \u0026ldquo;rubens\u0026rdquo;. But the tree looks much smaller!\n  When you create routes in APISIX, APISIX stores them in these trees.\nAPISIX can then work flawlessly because the time it takes to match a route only depends on the length of the URI in the request and is independent of the number of routes (O(K), K is the length of the key/URI).\nSo APISIX will be as quick as it is when matching 10 routes when you first start out and 5000 routes when you scale.\nThis crude example shows how APISIX can store and match routes using radix trees:\n Crude example of route matching in APISIXThe highlighted traversal shows the route /user/* where the * represents a prefix. So a URI like /user/navendu will match this route. The example code below should give more clarity to these ideas.\n  APISIX uses the lua-resty-radixtree library, which wraps around rax, a radix tree implementation in C. This improves the performance compared to implementing the library in pure Lua.\nThe example below shows how the library is used:\nlocal radix = require(\u0026#34;resty.radixtree\u0026#34;) local rx = radix.new({ { paths = { \u0026#34;/api/*action\u0026#34; }, metadata = { \u0026#34;metadata /api/action\u0026#34; } }, { paths = { \u0026#34;/user/:name\u0026#34; }, metadata = { \u0026#34;metadata /user/name\u0026#34; }, methods = { \u0026#34;GET\u0026#34; }, }, { paths = { \u0026#34;/admin/:name\u0026#34; }, metadata = { \u0026#34;metadata /admin/name\u0026#34; }, methods = { \u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34; }, filter_fun = function(vars, opts) return vars[\u0026#34;arg_access\u0026#34;] == \u0026#34;admin\u0026#34; end } }) local opts = { matched = {} } -- matches the first route ngx.say(rx:match(\u0026#34;/api/create\u0026#34;, opts)) -- metadata /api/action ngx.say(\u0026#34;action: \u0026#34;, opts.matched.action) -- action: create ngx.say(rx:match(\u0026#34;/api/destroy\u0026#34;, opts)) -- metadata /api/action ngx.say(\u0026#34;action: \u0026#34;, opts.matched.action) -- action: destroy local opts = { method = \u0026#34;GET\u0026#34;, matched = {} } -- matches the second route ngx.say(rx:match(\u0026#34;/user/bobur\u0026#34;, opts)) -- metadata /user/name ngx.say(\u0026#34;name: \u0026#34;, opts.matched.name) -- name: bobur local opts = { method = \u0026#34;POST\u0026#34;, var = ngx.var, matched = {} } -- matches the third route -- the value for `arg_access` is obtained from `ngx.var` ngx.say(rx:match(\u0026#34;/admin/nicolas\u0026#34;, opts)) -- metadata /admin/name ngx.say(\u0026#34;admin name: \u0026#34;, opts.matched.name) -- admin name: nicolas The ability to manage a large number of routes efficiently has made APISIX the API gateway of choice for many large-scale projects.\nLook under the Hood There is only so much I can explain about the inner workings of APISIX in one article.\nBut the best part is that the libraries mentioned here and Apache APISIX are entirely open source, meaning you can look under the hood and modify things yourself.\nAnd if you can improve APISIX to get that final bit of performance, you can contribute the changes back to the project and let everyone benefit from your work.\n","permalink":"https://navendu.me/posts/apisix-go-brr/","summary":"Taking a look under Apache APISIX\u0026rsquo;s hood to understand how it achieves ultimate performance.","title":"How is Apache APISIX Fast?"},{"content":" Disclaimer: I am still learning about public policy and base this blog post on my current understanding of the topic. I will continue improving this post as I learn more. Any feedback and corrections are welcome.\n I\u0026rsquo;m still trying to figure out what to make of the new Digital Personal Data Protection Bill (DPDPB).\nI thought it was a great idea when I was introduced to the bill last year. I was familiar with GDPR and considered the bill an Indian version of the GDPR (Desi-DPR™?). Users in the EU have generally responded to the GDPR positively, and I believed the bill would just be like that.\nIt was only in February when I actually read through it (had someone read it to me). It did not change what I thought the impact of the bill would be, but it made me realize two things:\n The bill is super easy to read, and it wasn\u0026rsquo;t a whole lot of legal jargon like I expected. I could learn more about policymaking and be analytical about policies impacting technology.  I ended up reading the bill and opinions about the bill from organizations like Internet Freedom Foundation and Software Freedom Law Center, media outlets, and public discourse. Everyone seems to agree that the bill got a lot of things right, but there are still a lot of unaddressed concerns.\nI also started learning more about policymaking and policy analysis and wanted to write about my opinions on the bill.\nOriginally I planned to write multiple articles with these ideas:\n Comparing DPDPB and GDPR. How DPDPB could affect tech companies. Explaining DPDPB through illustrations. Concerns in DPDPB.  But the more I look into it, the more I find it difficult to form a comprehensive opinion. So here are my fragmented notes on the bill, which you can (don\u0026rsquo;t) use to develop your own opinions.\nUnchecked Power The provisions in the bill do not apply to government instruments (and, by extension, private agents) in certain situations the Union Government sees fit. These situations are described vaguely in the bill opening up the potential for misuse by the Government and can lead to a violation of the fundamental right to privacy.\nThe Data Protection Board (DPB), which will be established through the bill, is also not an independent body. But the independent operation of the DPB is needed to ensure proper adherence to the bill.\nLaws Should Protect People (?) The goal of this bill is to protect people\u0026rsquo;s privacy, but the general consensus among people who have read the bill is that it actively fails to do so.\nThe concept of deemed consent, where the Data Principal (DP) is assumed to have given consent for the Data Fiduciary (DF) to collect and use their personal data, can be problematic, especially when the DF is the Government. Furthermore, the conditions for deemed consent are also vague, defeating the bill\u0026rsquo;s purpose.\nThe bill also imposes fines on the DF and the DP for failing to adhere to the bill. These fines imposed on the DF will be collected by the Government, and the bill does not mention any relief to DP who are affected by this failure. Also, fines on DPs? What?\nIncomplete Laws The bill misses things when we compare the DPDPB with the EU\u0026rsquo;s GDPR.\nFor example, the bill isn\u0026rsquo;t clear on transferring personal data outside India even though the bill applies to the personal data of Indian citizens processed outside India.\nThe GDPR has data localization rules and allows data to be transferred to other countries only if a certain security level is guaranteed. Compared to the GDPR, the bill does not lay down rules concretely and leaves much room for existing systems and regulations to work.\nNew Tech DFs must notify DPs in every language listed in the Eight Schedule to the Constitution of India. The bill also allows DPs to have a Consent Manager (CM) to give, manage, review, or withdraw their consent from the DFs.\nMeitY has already provided a specification for the CMs to adhere to, meaning existing platforms like Android, iOS, and Windows could use this specification to act as the CMs and mediate decisions regarding the use of personal data between the DPs and the DFs.\nThese CMs should be registered with the DPB and accountable to the DPs.\nEven if the DP has given consent before the bill\u0026rsquo;s implementation, DFs must provide notice to the DPs about how they have used their personal data. DFs also have to make withdrawing consent as easy as giving it. Once the need for a DP\u0026rsquo;s personal data is no longer needed, the DFs must remove the data.\nDFs are also required to set up organizational and technical measures to adhere to the bill. The Government can also declare DFs as Significant Data Fiduciaries (SDFs) based on certain criteria. SDFs are also required to appoint a Data Protection Officer (DPO) and an independent Data Auditor to evaluate compliance with this bill.\nThe bill also prevents DFs from monitoring children\u0026rsquo;s behavior or direct targeted advertisements at children (unless Government allows it?).\nAmateur Hour I\u0026rsquo;m still not experienced enough to understand many of the bill\u0026rsquo;s implications. I hope to converse with more people better equipped to speak about this in the future and update my opinions.\nThese notes are incomplete, and I meant to publish them in this unfinished state. I would like to see how my view changes as I learn more and as the bill (hopefully) undergoes changes.\n","permalink":"https://navendu.me/posts/dpdpb/","summary":"Semi-ordered thoughts on the new Digital Personal Data Protection Bill by the Indian Government.","title":"Notes on the Digital Personal Data Protection Bill"},{"content":" Disclaimer: I am still learning about public policy and base this blog post on my current understanding of the topic. I will continue improving this post as I learn more. Any feedback and corrections are welcome.\n Will we have safer roads in Kerala with the new AI traffic cameras?\nMaybe.\nBut is this the best way to allocate resources to ensure safer roads?\nNot really.\nKeeping the controversies aside, in this article, I will explain why this is the case and how this entire scenario is a textbook example of shortsighted policymaking.\nBut first, let\u0026rsquo;s look at the current state of roads, road accidents, and the \u0026ldquo;Safe Kerala\u0026rdquo; project, which resulted in these AI traffic cameras.\nKerala\u0026rsquo;s Roads Kerala has a dense road network. The state has 3,31,904 kilometers of road, or 5.6% of India\u0026rsquo;s total 1, while sharing only 1.18% of the country\u0026rsquo;s total area 2.\nBut Kerala has narrower highways than the rest of the states, while studies have repeatedly shown that wider roads can reduce accidents 3 4.\nIt was the only state to request it from the National Highways Authority of India besides Goa, stating difficulty in land acquisition 5. Although new projects like the ongoing widening of NH66 in north Kerala look to improve this, they will still lack things like wide medians found commonly in other parts of the country 6 due to this difficulty.\nA massive 24.2% of households in Kerala own a car compared to 7.5% nationally 7. Such a significant figure could maybe indicate a lack of reliable public transport facilities.\nRoad Accidents Over the last year, 4,317 people died in road accidents in the state.\nDriver error being the cause of 80% of the accidents 8 implies that improper infrastructure (or lack thereof) is only one of a myriad of reasons, including improper training during driving tests and negligent driving 9.\nPedestrians in Kerala also have a hard time as most cities and towns are not walkable. They account for 26% percent of the total death in road accidents, which is quite concerning 8 9.\nThe lack of proper infrastructure for pedestrians, like footpaths, crosswalks, and bridges, misuse of existing infrastructure by parking on pathways, stopping on crosswalks, and extending private spaces (like shops) to roads, and people\u0026rsquo;s/government\u0026rsquo;s general indifference to pedestrians, are significant contributors to this problem.\nAI Traffic Cameras On 20th April 2023, the Government of Kerala started operations of its new fleet of traffic cameras under the Safe Kerala project. The fleet includes 10:\n 675 AI traffic violation cameras 25 AI parking violation cameras and 18 AI red-light violation cameras  The traffic violation cameras can detect the following violations:\n Not wearing helmets on motorcycles Not wearing seatbelts on cars Using phones while driving More than one pillion rider on motorcycles  In the future, these cameras will also detect lane and one-way violations. Or at least, that is what they say.\nThe cameras are accompanied by control rooms in each of the districts in the state. Additional personnel, including 85 squads, 99 vehicle inspectors, and 225 assistant inspectors of the Motor Vehicle Department, will also be added to the service under the Safe Kerala project 11.\nKeltron delivered the entire project for 128 crores (just for the infrastructure) 12 and had the help of private companies on tenders. So the AI—or the computer software that processes the images from these cameras, detects traffic violations, and issues the challan—was built by a private company.\nKeltron expects it will send around 25,00,000 challans every year. The first few days of operation saw 4,50,552; 3,97,487; 2,68,378; 2,90,000; 2,37,000; and 2,39,000; violations per day. If this trend continues, it will be close to 7,30,00,000 violations annually.\nAlthough \u0026ldquo;if you create more laws, there will be more lawbreakers\u0026rdquo; is a solid argument on why these cameras were a bad idea, I would like to look at it from different perspectives and create a more concrete statement.\nIs Kerala Ready? Is Kerala ready to be implementing AI traffic cameras?\nNo.\nAs I pointed out in the above sections, Kerala\u0026rsquo;s problems are bad drivers and worse roads. Wider, high-quality, pedestrian-friendly roads with better safety measures are much more needed than the publicity points you get from using the \u0026ldquo;AI\u0026rdquo; buzzword.\nMore rules would mean more violations and more people searching for alternative modes of transport, like public buses and trains. Does Kerala have the required public transport infrastructure to support this new influx of travelers? No.\nThe pandemic catalyzed the decline of the public transport system in Kerala. The system was already falling under with more people opting for private vehicles over the past decade 13. It is challenging to bring this back up organically.\nThere is also a lack of personnel to aid the functioning of these cameras 14.\nHaving the basics walking well before trying to run with technologies like AI is a logical step the government seems to have missed.\nPrevention is Better The goal of the Safe Kerala project is to make roads safer in Kerala. A good indicator to track this goal would be the number of accidents.\nNudging people to wear helmets and seatbelts will reduce deaths caused by accidents but not the number of accidents. But if we could prevent accidents in the first place, it would automatically reduce deaths caused by accidents.\nThe project is trying to optimize for the wrong indicator without considering how it could better impact the broader goal of making the roads safer.\nArbitrary Implementation The 700 cameras do not even come close to covering Kerala\u0026rsquo;s 3,00,000+ km of roads.\nAny attempt to place these cameras will be arbitrary, meaning some people will be regularly monitored while some others won\u0026rsquo;t be monitored at all.\nSo, in practice, the traffic rules will only apply to a subset of the drivers in Kerala.\nBetter Alternatives If the goal is to make our roads safer, there are far better ways to allocate resources than AI traffic cameras.\nFor instance, the Motor Vehicle Department could use these resources to improve the driving tests in the state and ensure new drivers are fully equipped to drive safely. This should nip the problem in the bud and produce better drivers and, as a result, safer roads.\nYou could argue that the results could take a while to show with this solution. But, even then, the government could have spent these resources to improve existing roads.\nThe public transport system of the state also seems like a better recipient of these resources. Increasing the frequency of buses on existing routes and starting service on new routes would encourage more people to use public transport.\nOther states in India have also leveraged AI tools to improve road safety.\nThe iRASTE project in Telangana uses AI to build driver assistants that can predict accidents and alert the driver. The project was initially rolled out in 14 buses and will be extended to 200 buses. These AI systems can also continuously monitor the road infrastructure and provide feedback to improve and maintain roads to avoid future accidents 15.\nThe Karnataka State Road Transport Corporation also has a similar project that uses AI to detect collisions, driver drowsiness, and lane departures 16.\nBoth projects are far more direct in preventing accidents than the Safe Kerala project.\nIt is also interesting to see that NITI Ayog published a strategy guide on how to adopt AI in India, detailing how government can use AI to improve areas like healthcare, agriculture, education, infrastructure, and transportation 17 without ever mentioning using cameras this extensively to detect traffic violations.\nThere could be even more viable alternatives to spend money on than AI cameras. Did the government really think this was the best idea?\nUnsound Technology AI, although having existed since the last century, grew and became what we know now in the past decade.\nThe technology is so novel that we kind of know how it works, but not really.\nGovernments worldwide are trying to get ahead of the AI revolution and control its exponential growth. India\u0026rsquo;s IT minister Ashwini Vaishnaw recently announced that the government has no plans to regulate AI 18, which is a good decision for now.\nBut this also means that the AI used in these camera systems is built and operated at the discretion of the maker, a private company. We have no clue about the data used to train the AI or how accurate the AI is.\nIt does not stop at accuracy. AI models are also prone to inherent biases due to imperfect training data and training of the model. Meaning the system can be discriminatory to a particular group of people, and there is no way to prove otherwise reliably.\nThe government doesn\u0026rsquo;t provide or enforce any quality benchmarks to evaluate this black box of technology before using it directly on the public. NITI Ayog released a set of guidelines in 2021 19 for building responsible AI, but since these are just guidelines, we cannot know if the AI camera system follows them.\nThe proposed Digital Personal Data Protection Bill 20 could also impact the future of software, especially AIs, as they use a large amount of data. But the central government can exempt state governments from the provisions mentioned in the bill on specific grounds, which could be the case for the Safe Kerala project.\nUnintended Consequences Shortsighted policies often result in unintended consequences.\nDiscouraging more than two passengers on a motorcycle is a good idea considering the safety of the passengers and fellow drivers on the road.\nBut there are a lot of families consisting of two parents and children that rely on a single two-wheeler for transport. Strict enforcement would force such families to use the public transport system, which wouldn\u0026rsquo;t be a problem if the system is sound.\nYes, people shouldn\u0026rsquo;t travel with three other people on a 50cc scooter, but what else can they do?\nThese consequences will disproportionately affect people with lower incomes.\nPrivacy Violation The current laws 21 for using monitoring cameras in India allow a lot of room for abuse without accountability.\nThe government still needs to address such potential privacy concerns as these traffic cameras increase in the state.\nThe state and the central government should also invest resources in building legislation to regulate the usage of AI on the public. Jumping the gun and implementing these technologies without fully understanding or being able to control their implications can quickly turn catastrophic.\nTraffic Rules Aren\u0026rsquo;t Bad The purpose of this article was not to say that enforcing traffic rules for safer roads was a bad idea but to argue that AI traffic cameras were not the right solution to the problem for multiple reasons.\nThese reasons suggest shortsighted decision-making by the government for publicity points which they could have easily avoided if someone with clout had asked, \u0026ldquo;Do we need to do this?\u0026rdquo;\n  Ministry of Road Transport and Highways, Government of India. 2021. \u0026ldquo;Annual Report 2019-20.\u0026rdquo; https://morth.nic.in/sites/default/files/Ministry%20Annual%20Report_2019-20.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Government of Andhra Pradesh. 2013. \u0026ldquo;Comparative Statistics (States).\u0026rdquo; https://web.archive.org/web/20131126011218/http://www.ap.gov.in/Other%20Docs/COMPARATIVE%20STATISTICS%20(STATES).pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Zegeer CV et al. 1988. \u0026ldquo;Safety effects of cross-section design for two-lane roads.\u0026rdquo; Transportation research record. 1195: 20-32. https://www.safetylit.org/citations/index.php?fuseaction=citations.viewdetails\u0026amp;citationIds[]=citjournalarticle_604175_38.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Høye, Alena. \u0026ldquo;Wider roads can result in fewer accidents.\u0026rdquo; Nordic Roads, September 27, 2021. https://nordicroads.com/wider-roads-can-result-in-fewer-accidents/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Mishra, Mihir. \u0026ldquo;Highway authority projects hit road block in Kerala, Goa, Bengal.\u0026rdquo; Business Standard, January 21, 2013. https://www.business-standard.com/article/economy-policy/.highway-authority-projects-hit-road-block-in-kerala-goa-bengal-112031100019_1.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Chandran, Cynthia. \u0026ldquo;Kerala: Safety card wins six-lane highway.\u0026rdquo; Deccan Chronicle, March 20, 2017. https://www.deccanchronicle.com/nation/in-other-news/200317/kerala-safety-card-wins-six-lane-highway.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n International Institute for Population Sciences. 2021. \u0026ldquo;National Family Health Survey (NFHS-5) - India.\u0026rdquo; http://rchiips.org/nfhs/NFHS-5Reports/Kerala.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Kerala Police. \u0026ldquo;Road Accidents in Kerala.\u0026rdquo; May 15, 2023. https://keralapolice.gov.in/crime/road-accidents.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Raj, Ajith. \u0026ldquo;Over 23,500 people killed in road accidents in Kerala in past 6 years.\u0026rdquo; Mathrubhumi, February 12, 2023. https://english.mathrubhumi.com/news/kerala/over-23-500-people-killed-in-road-accidents-in-kerala-in-past-6-years-1.8303973.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n E. N. Jishnu. \u0026ldquo;AI or no AI? What do the new Kerala MVD traffic surveillance cameras offer?\u0026rdquo; Mathrubhumi, April 29, 2023. https://english.mathrubhumi.com/features/technology/ai-or-no-ai-what-do-the-new-kerala-mvd-traffic-surveillance-cameras-offer-1.8517915.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n \u0026ldquo;Kerala CM Launches Safe Kerala Project To Reduce Road Accidents.\u0026rdquo; Outlook, April 20, 2023. https://www.outlookindia.com/national/kerala-cm-launches-safe-kerala-project-to-reduce-road-accidents-news-280032.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Keltron. \u0026ldquo;Tender ID : 2020_KSEDC_368203_1.\u0026rdquo; November 19, 2020. https://www.keltron.org/index.php/safe-kerala.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n S. Unnikrishnan. \u0026ldquo;Public road transport system on verge of breakdown in Kerala.\u0026rdquo; New Indian Express, March 6, 2023. https://www.newindianexpress.com/states/kerala/2023/mar/06/public-road-transport-system-on-verge-of-breakdown-in-kerala-2553511.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n \u0026ldquo;MVD starts issuing notices for traffic violations, fines from May 20 onwards.\u0026rdquo; Mathrubhumi, May 9, 2023. https://english.mathrubhumi.com/news/kerala/ai-cameras-mvd-starts-issuing-notices-for-traffic-violations-fines-from-may-20-onwards-1.8543291.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n INAI. \u0026ldquo;Project iRASTE Telangana.\u0026rdquo; May 20, 2023. https://inai.iiit.ac.in/iraste-telangana.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n \u0026ldquo;KSRTC to deploy AI to improve road safety and driver efficiency.\u0026rdquo; INDIAai, June 10, 2021. https://indiaai.gov.in/news/ksrtc-to-deploy-ai-to-improve-road-safety-and-driver-efficiency.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n NITI Aayog. 2018. \u0026ldquo;National Strategy for Artificial Intelligence.\u0026rdquo; https://niti.gov.in/sites/default/files/2019-01/NationalStrategy-for-AI-Discussion-Paper.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Singh, Priya. \u0026ldquo;\u0026lsquo;No regulations for Artificial Intelligence in India\u0026rsquo;: IT Minister Ashwini Vaishnaw.\u0026rdquo; Business Today, April 6, 2023. https://www.businesstoday.in/technology/news/story/no-regulations-for-artificial-intelligence-in-india-it-minister-ashwini-vaishnaw-376298-2023-04-06.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n NITI Aayog. 2021. \u0026ldquo;Responsible AI.\u0026rdquo; https://www.niti.gov.in/sites/default/files/2021-02/Responsible-AI-22022021.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Ministry of Electronics \u0026amp; IT, Government of India. 2022. \u0026ldquo;The Digital Personal Data Protection Bill 2022.\u0026rdquo; https://pib.gov.in/PressReleasePage.aspx?PRID=1881402.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Ministry of Communications and Information Technology, Government of India. 2011. \u0026ldquo;The Gazette of India : Extraordinary [ Part II-Sec. 3(i)].\u0026rdquo; https://www.meity.gov.in/writereaddata/files/GSR313E_10511%281%29_0.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://navendu.me/posts/ai-traffic-cameras/","summary":"An amateur policy analyst attempts to explain why (disregarding controversies) installing AI traffic cameras in Kerala was a bad idea.","title":"AI Traffic Cameras Were Always a Bad Idea"},{"content":"AI has gone from a niche technology interest to a mainstream tool used by all sorts of people in the past few months.\nGenerative AI models like GPT for text and Midjourney and Stable Diffusion for images have generated hype, interest, and significant venture capital investment allowing the technology to advance even further.\nThese rapid advancements do not appear to slow down even after tech industry leaders like Elon Musk and Steve Wozniak called for a pause on giant AI experiments and top companies around the world continue to compete for a slice of the 100 billion dollar industry (expected to reach two trillion dollars by 2030).\nDuring these rapid advancements, tools like ChatGPT have disrupted or displaced many businesses. Chegg, the online education company, had its share price drop 40% after the CEO reported reduced users after ChatGPT\u0026rsquo;s launch. Stack Overflow had a 14% traffic drop in March, while traffic to ChatGPT has grown steadily.\nOn the other hand, Khan Academy, the non-profit education organization, launched Khanmigo, an AI assistant powered by GPT-4 that works as a virtual tutor for students and a classroom assistant for teachers.\nKhan Academy and other companies use APIs provided by ChatGPT to leverage its powerful AI model to build tailored applications. So instead of spending money and effort to develop an in-house AI, companies can use state-of-the-art AI in their own applications through simple HTTP requests.\nThese \u0026ldquo;APIs to multimillion-dollar AIs\u0026rdquo; have created a new market for companies building AI-powered products and services in a wide range of industries. In the future, more AI companies will offer their APIs, and more businesses will leverage these APIs to build new products.\nThis newly emerging \u0026ldquo;AI-API economy\u0026rdquo; is not unidirectional.\nAI models, while used extensively, are limited. By design, large language models (LLMs) can only learn from their training data and output text. This output does not translate into actions in the real world. To solve this, AI tools use \u0026ldquo;traditional\u0026rdquo; APIs to carry out these actions and provide a better user experience.\nChatGPT released plugins to do precisely this by leveraging third-party services through APIs. For example, the OpenTable plugin lets you use ChatGPT to get restaurant recommendations and book a table.\nUnder the hood, OpenTable exposes an API to ChatGPT through an OpenAPI specification and a manifest file containing the plugin metadata.\nThe OpenAPI spec is a standard way of defining HTTP APIs. In this context, the spec is a wrapper around the third-party API, which tells ChatGPT how to use the API.\nAPI developers can choose what endpoints to expose to ChatGPT through this specification. For example, in the OpenTable plugin, you might want to avoid giving ChatGPT complete control to make table reservations for you. Instead, you can only let ChatGPT make GET requests, and you can manually book a table.\nThere are already 70 plugins available through ChatGPT, and it is bound to increase progressively.\nLike ChatGPT plugins, Google recently announced adding \u0026ldquo;tools\u0026rdquo; to its LLM, Bard.\nAPIs are no longer just for human users but also AI models. As a result, more API developers will adopt standards like the OpenAPI spec to make their APIs discoverable by these AI models.\nAPI developers will also have to design APIs specifically for AIs or improve their existing ones to ensure that AIs can use them as intended.\nThe AI-API economy will also force companies that don\u0026rsquo;t have APIs to think about exposing their offerings through APIs in addition to current ways or risk falling behind to some competitor who does.\nThere will be a significant rise in the APIs available. Companies building AI models and tools will focus on delivering the core user experiences and delegate everything else to these APIs.\nMore tools supporting the API lifecycle will emerge to cater to this newfound interest, and existing tools will improve their support for standards and enhance the developer experience for building AI-first APIs.\nBoth AI and APIs have been around for a long time. Both of these technologies have gone through a massive overhaul recently, and their combination has created an AI-API economy, producing novel solutions to existing problems and trailblazing new frontiers.\nIt has been increasingly hard to predict the future of technology, but it is a safe bet to assume it will be AI/API-first.\n","permalink":"https://navendu.me/posts/ai-and-apis/","summary":"What do the recent advancements in generative AI mean for APIs?","title":"AI and APIs"},{"content":"Translations: Simplified Chinese | 简体中文.\nThere is still a lot of confusion about API gateways, Kubernetes gateways, and service meshes. A lot of this is because:\n People often mention these technologies with the same keywords, like canary deployments, rate limiting, and service discovery. All these technologies use reverse proxies. Some API gateways have their own Kubernetes gateways and service meshes and vice versa. There are a lot of articles/videos that compare the three technologies and conclude why one is better than the other.  In this article, I will try to explain these technologies and share how they fundamentally differ and cater to different use cases.\nAPI Gateways An API gateway sits between your client applications and your APIs. It accepts all client requests, forwards them to the required APIs, and returns the response to clients in a combined package.\nIt is basically a reverse proxy with a lot of capabilities.\n API gateway   On top of this, an API gateway can also have features like authentication, security, fine-grained traffic control, and monitoring, leaving the API developers to focus only on business needs.\nThere are many API gateway solutions available. Some of the popular free and open source solutions are:\n Apache APISIX: A high-performance, extensible, cloud native API gateway built on top of Nginx. Gloo Edge: An API gateway built on top of Envoy proxy. Kong: A pluggable API gateway also built on Nginx. Tyk: An API gateway written in Go supporting REST, GraphQL, TCP, and gRPC protocols.  Cloud platforms like GCP, AWS, and Azure also have their own proprietary API gateways.\nAPI gateways, Kubernetes gateways, and service meshes support canary deployments—gradually rolling out a new software version to a small subset of users before making it generally available.\nThe example below shows how to configure a canary deployment in Apache APISIX.\n Canary deployments with an API gateway   You can send a request to the APISIX Admin API with the following configuration:\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 \\ -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;uri\u0026#34;:\u0026#34;/*\u0026#34;, \u0026#34;plugins\u0026#34;:{ \u0026#34;traffic-split\u0026#34;:{ \u0026#34;rules\u0026#34;:[ { \u0026#34;weighted_upstreams\u0026#34;:[ { \u0026#34;upstream\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;api-v1\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;roundrobin\u0026#34;, \u0026#34;nodes\u0026#34;:{ \u0026#34;api-v1:8080\u0026#34;:1 } }, \u0026#34;weight\u0026#34;:95 }, { \u0026#34;weight\u0026#34;:5 } ] } ] } }, \u0026#34;upstream\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;roundrobin\u0026#34;, \u0026#34;nodes\u0026#34;:{ \u0026#34;api-v2:8080\u0026#34;:1 } } }\u0026#39; APISIX will now route 95% of the traffic to the api-v1 service and 5% to the api-v2 service.\nKubernetes Gateways Kubernetes gateways are just Kubernetes-native API gateways. i.e., you can manage these API gateways with the Kubernetes API, similar to a Kubernetes pod, service, or deployment.\nIn Kubernetes, your APIs are pods and services deployed in a cluster. You then use a Kubernetes gateway to direct external traffic to your cluster.\nKubernetes provides two APIs to achieve this, the Ingress API and the Gateway API.\n Kubernetes gateway   Kubernetes Ingress API The Ingress API was created to overcome the limitations of the default service types, NodePort and LoadBalancer, by introducing features like routing and SSL termination. It also standardized how you expose Kubernetes services to external traffic.\nIt has two components, the Ingress and the Ingress controller.\nThe Ingress Kubernetes native object defines a set of rules on how external traffic can access your services.\nThis example configuration shows routing traffic based on URI path with the Kubernetes Ingress object:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes spec: ingressClassName: apisix rules: - http: paths: - backend: service: name: api-v1 port: number: 8080 path: /v1 pathType: Exact - backend: service: name: api-v2 port: number: 8080 path: /v2 pathType: Exact An Ingress controller implements these rules and routes traffic to your cluster using a reverse proxy.\nThere are over 20 Ingress controller implementations. APISIX has an Ingress controller that wraps around APISIX API gateway to work as Kubernetes Ingress.\n APISIX Ingress   The APISIX Ingress controller converts the Kubernetes Ingress object to APISIX configuration.\n APISIX Ingress controller translates configuration   APISIX then implements this configuration.\n Canary deployments with Kubernetes Ingress API   You can swap APISIX with any other Ingress controller, as the Ingress API is not tied to any specific implementation.\nThis vendor neutrality works well for simple configurations. But if you want to do complex routing like a canary deployment, you must rely on vendor-specific annotations.\nThe example below shows how to configure a canary deployment using Nginx Ingress. The custom annotations used here are specific to Nginx:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-weight: \u0026#34;5\u0026#34; name: api-canary spec: rules: - http: paths: - backend: serviceName: api-v2 servicePort: 8080 path: / The above configuration will route 5% of the traffic to the api-v2 service.\nIn addition to annotations, Ingress controllers like APISIX have custom Kubernetes CRDs to overcome the limitations of the Ingress API.\nThe example below uses the APISIX CRD, ApisixRoute to configure a canary deployment:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: api-canary spec: http: - name: route match: paths: - /* backends: - serviceName: api-v1 servicePort: 8080 weight: 95 - serviceName: api-v2 servicePort: 8080 weight: 5 These custom CRDs made it much easier to configure Ingress and leverage the full capabilities of the API gateway underneath but at the expense of portability.\nKubernetes Gateway API The Gateway API is a new Kubernetes object that aims to \u0026ldquo;fix\u0026rdquo; the Ingress API.\nIt takes inspiration from the custom CRDs developed by Ingress controllers to add HTTP header-based matching, weighted traffic splitting, and other features that require custom proprietary annotations with the Ingress API.\nThe example below shows configuring a canary deployment with the Kubernetes Gateway API:\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: api-canary spec: rules: - backendRefs: - name: api-v1 port: 8080 weight: 95 - name: api-v2 port: 8080 weight: 5 Any Ingress controller (that implements the Gateway API) can now implement this configuration.\nThe Gateway API also makes many improvements over the Ingress API, but it is still in alpha, and the Gateway API implementations are constantly breaking.\nService Meshes API gateways and Kubernetes gateways work across application boundaries solving edge problems while abstracting your APIs.\nService Meshes solve a different challenge.\nA service mesh is more concerned about inter-service communication (east-west traffic) than service-client communication (north-south traffic).\nTypically, this is achieved by deploying sidecar proxies with APIs/services.\n Service mesh   Here, the sidecar proxies handle the service-to-service communication instead of the developer having to code the networking logic to the services.\nThere are a lot of service meshes available. Some of the popular ones are:\n Istio: By far the most popular service mesh. It is built on top of Envoy proxy, which many service meshes use. Linkerd: A lightweight service mesh that uses linkerd2-proxy, written in Rust specifically for Linkerd. Consul Connect: A service mesh emphasizing security and observability. It can work with either a built-in proxy or Envoy.  New service mesh offerings like Cilium offer alternatives to sidecar-based service meshes by using networking capabilities directly from the kernel through eBPF.\n Sidecar-less service meshA typical service mesh requires 8 proxies for 8 services whereas eBPF-based service meshes like Cilium don\u0026rsquo;t. Adapted from Cilium Service Mesh – Everything You Need to Know\n  Service meshes also have basic ingress/egress gateways to handle north-south traffic to and from the services. Ingress gateways are the entry points of external traffic to a service mesh, and egress gateways allow services inside a mesh to access external services.\n Ingress and egress gateways with a service mesh   Apache APISIX also has a service mesh implementation called Amesh. It works with Istio\u0026rsquo;s control plane using the xDS protocol replacing the default Envoy proxy in the sidecar.\nA service mesh lets you configure canary deployments. For example, you can split the requests from one service between two versions of another service.\n Canary deployments with a service mesh   The example below shows configuring a canary deployment with Istio service mesh:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: api-virtual-service spec: hosts: - api http: - route: - destination: host: api subset: v1 weight: 80 - destination: host: api subset: v2 weight: 20 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: api-destination-rule spec: host: api subsets: - name: v1 labels: version: \u0026#34;1.0\u0026#34; - name: v2 labels: version: \u0026#34;2.0\u0026#34; These configurations are specific to Istio. To switch to a different service mesh, you must create a different but similarly vendor-dependant configuration.\nThe Service Mesh Interface (SMI) specification was created to solve this portability issue.\nThe SMI spec is a set of Kubernetes CRDs that a service mesh user can use to define applications without binding to service mesh implementations.\nA standardization attempt will only work if all the projects are on board. But this did not happen with the SMI spec, and only a few projects participated actively.\nMore recently, the Kubernetes SIG Network has been evolving the Gateway API to support service meshes.\nThe GAMMA (Gateway API for Mesh Management and Administration) initiative is a dedicated group with the Gateway API project with goals to \u0026ldquo;investigate, design, and track Gateway API resources, semantics, and other artifacts related to service mesh technology and use-cases.\u0026rdquo;\nGateway API is a natural next step to the Ingress API, but we must wait to see how it will work for service meshes. Istio has announced its intention to use the Gateway API as its default API for all traffic management and continues to drive the project forward.\nThe example below shows configuring a canary deployment in Istio with the Gateway API. The underlying idea is using parentRefs to attach to other services instead of the gateway:\napiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: api-canary spec: parentRefs: - kind: Service name: api-a port: 8080 rules: - backendRefs: - name: api-b-v1 port: 8080 weight: 95 - name: api-b-v2 port: 8080 weight: 5 There are some concerns that the GAMMA project might become skewed to serve the needs of one particular project than the larger community, which will eventually lead to other projects using their own APIs, similar to the custom CRD scenario after the Kubernetes Ingress API.\nBut the Gateway API project has been the best attempt at standardizing traffic management in service meshes. The SMI project also joined the GAMMA initiative with a shared vision and will help advocate for consistent implementations of the Gateway API by service mesh projects.\nOther projects like Flagger and Argo Rollouts have also integrated with the Gateway API.\nWhat Should You Use? There is only one correct answer to this question; \u0026ldquo;it depends.\u0026rdquo;\nIf you are developing APIs and need authentication, security, routing, or metrics, you are better off using an API gateway than building this on your own in your APIs.\nIf you want to do something similar in a Kubernetes environment, you should use a Kubernetes gateway instead of trying to wrangle your API gateway to work on Kubernetes. Thankfully, a lot of API gateways also work with Kubernetes-native configurations.\nBut sometimes, the features offered by an API gateway + Ingress controller might be an overkill for a Kubernetes environment, and you might want to switch back to simple traffic management.\nService meshes, on the other hand, solve an entirely different set of problems. They also bring their own gateways to handle north-south traffic (usually enough) but also let you use your own gateways with more features.\nThe convergence of the API gateway and the service mesh through the Kubernetes Gateway API should make it easier for the application developer to focus on solving problems than worry about the underlying implementation.\nProjects like Apache APISIX use the same technology to build the API gateway and service mesh offerings and integrate well with these specifications, incentivizing vendor-neutral choices.\nIt is also likely that you will not need any of these. You may not even need microservices or a distributed architecture, but when you need them, gateways and meshes can make your lives a lot easier.\n","permalink":"https://navendu.me/posts/gateway-and-mesh/","summary":"A comprehensive comparison of API gateways, Kubernetes gateways, and service meshes with actionable insights.","title":"A Comprehensive Guide to API Gateways, Kubernetes Gateways, and Service Meshes"},{"content":"Uzhunnu vada/ഉഴുന്ന് വട is a popular South Indian snack (like fritters) made from black gram (Vigna mungo).\nAn uzhunnu vada (UV) profitable™ business makes just enough money for the founder to pay for an evening tea and vada. It is technically profitable but unsustainable.\nThis article is about a project I built and open sourced three years ago called NSFW Filter and our journey to make it UV profitable.\nWhat\u0026rsquo;s NSFW Filter? Before examining UV profitability, let me give you some context on how I built NSFW Filter.\nI was in-between jobs and had a lot of free time to hack things together back in 2020.\nNSFW Filter was a weekend project I built during this time to learn how TensorFlow.js worked.\nIt was a simple browser extension that used a deep learning model to filter out NSFW images from web pages.\n NSFW Filter in actionFrom github.com/nsfw-filter/nsfw-filter\n  The first release was hacky and had a lot of performance issues, but I published it to Chrome and Firefox extension stores. And surprisingly, people started using it.\nHaving people use something you built is always fun. I spent more effort improving and promoting the project, gaining a lot more users.\nBut more users meant more demand for new features and improvements to existing ones.\nI also had a contributor join in to help with the project. He is a maintainer of the project now.\nMaking Money Soon, I was spending all my time working on the project, and it was starting to become unsustainable as I did not have a job or the time to look for a job.\nI initially had the drive to work on the project, but it did not last, which meant creating a business (paid version with a better model and more features?) around the project was a bad idea.\nThe project being moderately successful (front page of Hacker News successful), also attracted a lot of offers to buy the project.\nBeing an open source absolutist, I rejected all these offers and set up sponsorships to cover the running costs ($5 for registration and $15/year for the domain) and for users to show interest in the project.\nEven after almost three years and little development, two people still sponsor the project monthly on Patreon. And the project is UV profitable.\nReaching UV Profitability Over the past three years, 12 people have pledged to sponsor NSFW Filter. Six of them have actually done it.\n Six people has sponsored $269.87 in totalData from Patreon\n  Some patrons pledge a small sum monthly, while others make a substantial one-time pledge.\n Monthly earnings since October 2020 after platform feesData from Patreon\n  The project has earned $269.87 to date after platform fees. It continues to earn $8.30 on average per month.\nA better way to visualize this is in terms of uzhunnu vadas.\nAssuming an uzhunnu vada costs ₹10 ($0.12), $269 could get you 2241 and a half uzhunnu vadas—or one uzhunnu vada per six users.\n 2250 uzhunnu vadasMade with NumPy and OpenCV\n  At $7 ($8.30 - $1.30 for running costs) a month, I can get 58 uzhunnu vadas or 14 and a half uzhunnu vadas per week (one uzhunnu vada per 157 weekly users).\n 14 and a half uzhunnu vadasMade with NumPy and OpenCV\n  If I start drinking tea again, it would cost me ₹20 ($0.24) for a vada and tea. So $7 monthly could get me at least seven vada + tea sets sorting me out all week.\n Seven vada \u0026#43; tea sets for each day of the weekMade with NumPy and OpenCV\n  i.e., We have achieved UV profitability.\nWhat Does This Mean? This doesn\u0026rsquo;t mean anything. I was bored on the weekend and saw a pull request to the NSFW Filter repo that made me think of the project and write about it in the silliest way possible.\nThis also means something.\nThousands of open source projects with small user bases like NSFW Filter might never become a profitable business or have enough sponsorship for the maintainers to work on it full-time.\nI\u0026rsquo;m fortunate to have a full-time job where I get paid to work on open source projects. I can live sustainably and don\u0026rsquo;t have to rely on sponsorships for my personal open source projects.\nBut most open source maintainers don\u0026rsquo;t make any money at all from their open source work.\nThis is where you and I can come in to support open source projects and maintainers as users of open source software.\nEven if you can\u0026rsquo;t support them financially, you can still make a difference by contributing to the project through code, docs, tests, feedback, bug reports, or countless other ways.\nMy co-maintainer and I currently fund NSFW Filter. We haven\u0026rsquo;t taken any money out of our Patreon account, and we plan to utilize this money to support open source projects/maintainers.\nI also want to clarify that I\u0026rsquo;m not the man from a hypothetical South Indian-specific mathematics textbook who goes around buying all the uzhunnu vadas money can buy.\n","permalink":"https://navendu.me/posts/vada-profitable/","summary":"This article is about a project I built and open sourced three years ago and our journey to make it uzhunnu vada (UV) profitable™.","title":"Vada (Fritters) Profitable"},{"content":" See the discussion on Hacker News.\n Recently, a person has been using AI tools to generate code and open pull requests to open source projects I contribute to.\nThe code is entirely wrong and doesn\u0026rsquo;t work, and it is evident that the person making these pull requests doesn\u0026rsquo;t understand the code.\nThe person also copied explanations (which was an obvious giveaway as it sounded like a typical \u0026lt;popular AI tool\u0026gt; response) into the pull request and attempted to explain the code and answer questions from the reviewers.\nWe were polite and when it didn\u0026rsquo;t work, reported the person to GitHub.\nI don\u0026rsquo;t want to shame the person publicly. But I want to make other open source maintainers aware that this is a thing and prevent them from wasting time and effort chasing such people down.\nYou can read more to learn what happened exactly.\nAI Tools Are Helpful, Generally I recently used an AI tool to generate some charts with Matplotlib.\nI could say things like \u0026ldquo;the x-axis labels are overlapping and unreadable,\u0026rdquo; or the \u0026ldquo;font is too small,\u0026rdquo; or \u0026ldquo;I want the lines to be red\u0026rdquo; (to be honest, I was less verbose and polite), and the AI would update the code to be just what I want.\nIt is clear that AI tools are helping a lot of programmers to write better code, faster.\nBut there are always people who take perfectly good things and turn them into a \u0026ldquo;let\u0026rsquo;s spam open source projects\u0026rdquo; fest.\nI have hidden the identity of the person who spammed our repository in the sections below. We have already reported this person to GitHub.\nHow It Started For the last couple of weeks, we have been receiving comments on open issues from a person. Let\u0026rsquo;s call this person X.\nThese were \u0026ldquo;solutions\u0026rdquo; X proposed for the issue and contained a code snippet and an explanation of how it would fix the issue:\n These look legit to the untrained eyeScreenshot taken on 29th March 2023\n  At a glance, these comments look legit. But if you have spent time in \u0026lt;popular AI tool\u0026gt;, you would immediately find it suspicious.\nLooking through X\u0026rsquo;s profile, I found more instances in other open source projects where X made similar comments and maintainers replied thinking it was legit:\n The solution \u0026#34;leaves things unanswered\u0026#34; because it was generated by an AIScreenshot taken on 29th March 2023\n  Spam Pull Requests X then opened pull requests with this AI-generated code and explanations and started consistently pinging the maintainers.\nThe code did not make sense, and it even used the database schema of a different project (that does similar things), which confirmed X used AI.\nWe politely asked X to explain the use of the schema, and X, to no one\u0026rsquo;s surprise, used AI to generate an answer.\nWithout answering follow-up questions, X proceeded to ping maintainers again for review and open more AI-generated pull requests. The audacity!\nFrom the number of closed pull requests in X\u0026rsquo;s profile, it is clear that X has been doing this for some time:\n No one would just merge code that doesn\u0026#39;t workScreenshot taken on 29th March 2023\n  How It Ended After confirming that X was using AI-generated code that did not work or hadn\u0026rsquo;t been tested in the pull requests, we closed them:\n Some things are ridiculousScreenshot taken on 29th March 2023\n  Some of the comments are more straightforward:\n Some other things doesn\u0026#39;t make any senseScreenshot taken on 29th March 2023\n  We finally reported X to GitHub to prevent other open source projects and maintainers from being spammed:\n Reporting X to GitHubScreenshot taken on 29th March 2023\n  No open source maintainer will readily merge your pull request without at least testing to see if it works.\nIt is fairly easy to understand when the code doesn\u0026rsquo;t work that an AI generated it. You wouldn\u0026rsquo;t be opening pull requests with code that doesn\u0026rsquo;t work.\nPlease don\u0026rsquo;t turn this into a “let’s spam open source projects” fest.\nSee the discussion on Hacker News.\n","permalink":"https://navendu.me/posts/ai-generated-spam-prs/","summary":"Someone tried to open pull requests to open source projects with AI-generated code.","title":"AI-Generated Spam Pull Requests"},{"content":"In a previous article, I explained how to extend APISIX Ingress with Plugins.\nAPISIX comes with 80\u0026#43; Plugins out of the box. Still, there may come a time when your use case does not fit any Plugins.\nIn such scenarios, APISIX lets you write custom Plugins in Lua.\nIn this article, we will create and use a small custom Plugin with APISIX deployed in Kubernetes.\nTo learn how to write custom Plugins for APISIX, refer to the documentation. APISIX also supports external Plugins written in languages like Java, Go, and Python. These are out of the scope of this article.\nBefore you move on, make sure you:\n Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install and configure kubectl to communicate with your cluster. Install Helm to deploy the APISIX Ingress controller.  The complete code used in this article is available here.\nDeploying a Sample Application We will deploy the bare-minimum-api as our sample application:\nkubectl run bare-minimum-api --image navendup/bare-minimum-api --port 8080 -- 8080 v1.0 kubectl expose pod bare-minimum-api --port 8080 Writing a Custom Plugin For this example, we will create a sample Plugin that rewrites the response body from the Upstream with a custom value:\n-- some required functionalities are provided by apisix.core local core = require(\u0026#34;apisix.core\u0026#34;) -- define the schema for the Plugin local schema = { type = \u0026#34;object\u0026#34;, properties = { body = { description = \u0026#34;custom response to replace the Upstream response with.\u0026#34;, type = \u0026#34;string\u0026#34; }, }, required = {\u0026#34;body\u0026#34;}, } local plugin_name = \u0026#34;custom-response\u0026#34; -- custom Plugins usually have priority between 1 and 99 -- higher number = higher priority local _M = { version = 0.1, priority = 23, name = plugin_name, schema = schema, } -- verify the specification function _M.check_schema(conf) return core.schema.check(schema, conf) end -- run the Plugin in the access phase of the OpenResty lifecycle function _M.access(conf, ctx) return 200, conf.body end return _M  Tip: A complete guide on writing custom Plugins is available on the APISIX documentation.\n We can now configure APISIX to use this Plugin and create Routes with this Plugin enabled.\nOne way to do this is to create a custom build of APISIX with this code included. But that is not straightforward.\nInstead, we can create a ConfigMap from the Lua code and mount it to the APISIX instance in Kubernetes.\nYou can create the ConfigMap by running:\nkubectl create ns ingress-apisix kubectl create configmap custom-response-config --from-file=./apisix/plugins/custom-response.lua -n ingress-apisix Now we can deploy APISIX and mount this ConfigMap.\nDeploying APISIX We will deploy APISIX via Helm as we did in the previous tutorials.\nBut we will make some changes to the default values.yaml file to mount the custom Plugin we created.\nYou can configure the Plugin under customPlugins as shown below:\ncustomPlugins: enabled: true plugins: - name: \u0026#34;custom-response\u0026#34; attrs: {} configMap: name: \u0026#34;custom-response-config\u0026#34; mounts: - key: \u0026#34;custom-response.lua\u0026#34; path: \u0026#34;/usr/local/apisix/apisix/plugins/custom-response.lua\u0026#34; You should also enable the Plugin by adding it to the plugins list:\nplugins: - api-breaker - authz-keycloak - basic-auth - batch-requests - consumer-restriction - cors ... ... - custom-response Finally you can enable the Ingress controller and configure the gateway to be exposed to external traffic. For this, set gateway.type=NodePort, ingress-controller.enabled=true, and ingress-controller.config.apisix.serviceNamespace=ingress-apisix in your values.yaml file.\nNow we can run helm install with this updated values.yaml file:\nhelm install apisix apisix/apisix -n ingress-apisix --values ./apisix/values.yaml APISIX and APISIX Ingress controller should be ready in some time with the custom Plugin mounted successfully.\nTesting without Enabling the Plugin First, let\u0026rsquo;s create a Route without our custom Plugin enabled.\nWe will create a Route using the ApisixRoute CRD like in the previous articles:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: api-route spec: http: - name: route match: hosts: - local.navendu.me paths: - /api backends: - serviceName: bare-minimum-api servicePort: 8080 We can now test the created Route:\ncurl http://127.0.0.1:52876/api -H \u0026#39;host:local.navendu.me\u0026#39; This will give back the response from our Upstream service as expected:\nHello from API v1.0! Testing the Custom Plugin Now let\u0026rsquo;s update the Route and enable our custom Plugin on the Route:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: api-route spec: http: - name: route match: hosts: - local.navendu.me paths: - /api backends: - serviceName: bare-minimum-api servicePort: 8080 plugins: - name: custom-response enable: true config: body: \u0026#34;Hello from your custom Plugin!\u0026#34; Now, our custom Plugin should rewrite the Upstream response with \u0026ldquo;Hello from your custom Plugin!\u0026rdquo;\nLet\u0026rsquo;s apply this CRD and test the Route and see what happens:\ncurl http://127.0.0.1:52876/api -H \u0026#39;host:local.navendu.me\u0026#39; And as expected, we get the rewritten response from our custom Plugin:\nHello from your custom Plugin! What\u0026rsquo;s Next? In this tutorial, you learned how to configure custom Plugins written in Lua to work with APISIX Ingress.\nSee the resources below to learn more about writing custom Plugins:\n Writing custom Plugins External Plugins  See the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress\u0026rdquo;.\n","permalink":"https://navendu.me/posts/custom-plugins-in-apisix-ingress/","summary":"A hands-on tutorial on using custom APISIX Plugins in Kubernetes environments with APISIX Ingress.","title":"Custom Plugins in APISIX Ingress"},{"content":"Writing stuff down has been a way for me to think clearly and articulate my thoughts.\nWhile maintaining open source projects with remote teammates, we were forced to practice async communication. We usually work on a new feature by following this process:\n Create a design document. Open it up for review with the stakeholders. Receive feedback. Iterate. Implement.  This is similar to how traditional organizations handle things. But traditional organizations have a more accessible communication channel because of their physical proximity in offices.\nThis is far from how we operated on open source projects where teammates span continents and time zones. How do you communicate effectively without being on calls at weird times?\nYou write stuff down.\nCreating a design document required a lot of writing. The feedback was in writing. And the improvements to the design meant more writing.\nIt worked well, and we improved the way we communicate.\nGoing into my fourth year of remote work, I have taken this lesson into a default practice and would urge my teammates to do the same.\nI also take the \u0026ldquo;write stuff down\u0026rdquo; practice outside work to my personal life. I use Notion (for now) as my note-taking app, and it has sections for every aspect of my life, from daily planning to travel planning and meeting notes to blog post ideas.\nI also forget my thoughts and ideas quickly, so writing them down as soon as I have them helps me go back to them in the future. Frequently writing has enabled me to do it more quickly than before, so I don\u0026rsquo;t lose my train of thought thinking about how to write them down.\nStrategies to Write Clearly Over the past years, I have experimented with how I can write clearly to fully articulate my thoughts and to make it easier for the reader to understand what I\u0026rsquo;m thinking.\nThese strategies worked well for me.\nBe Verbose Don\u0026rsquo;t shy away from expressing your thoughts verbosely. I don\u0026rsquo;t mean you should always be wordy when you could have explained more straightforwardly, but make sure your explanation is unambiguous.\nMake Reasonable Assumptions Technical documents usually require the reader to have some prior context to understand them.\nLeverage Emphasis, Lists, and Tables Use bold to highlight text, lists to order content, and tables to organize information.\nThese, along with proper headings, paragraphs, and other typographic tools, will make your writing more readable.\nIterate A lot of times, these documents are a collaborative effort of multiple people. This means you would have to iterate and improve the document before presenting it in its final form.\nIf you are working on the document alone, you can ask for feedback in between from peers and improve it.\nThis is especially useful when you want to create timeless documents. For example, if you are making a guide that new engineers in your team would follow, you should ensure that the guide is comprehensive. This could take multiple iterations before it reaches its final stage, which is better than tackling it in one go.\nIt took me some time to figure out what worked and what didn\u0026rsquo;t. But working on improving how I communicate with writing has helped me immensely while working remotely. It will take some deliberate practice, but I assure you that you will benefit from it.\n","permalink":"https://navendu.me/posts/write-stuff-down/","summary":"Writing as a way to think clearly and articulate your thoughts.","title":"Write Stuff Down"},{"content":"Previously, I wrote an article comparing the Kubernetes Gateway API and the Ingress API.\nThe Gateway API was designed to overcome the limitations of the Ingress API (like proprietary annotations and custom CRDs), thus providing a unified way to expose services outside the cluster.\nYou can read more about the Gateway API, but we will keep the focus of this tutorial on using the Gateway API in practice with Apache APISIX Ingress.\nBefore you move on, make sure you:\n Go through the previous tutorial to learn about canary releases. Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install and configure kubectl to communicate with your cluster. Install Helm to deploy the APISIX Ingress controller.  Installing the Gateway API CRDs Kubernetes does not support the Gateway API out of the box. So, you can manually install the CRDs by running:\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v0.5.0/standard-install.yaml Deploying APISIX Ingress and the Sample Application You can install APISIX and APISIX Ingress controller with Helm. To enable APISIX Ingress controller to work with the Gateway API, you can set the flag --set ingress-controller.config.kubernetes.enableGatewayAPI=true as shown below:\nhelm repo add apisix https://charts.apiseven.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update kubectl create ns ingress-apisix helm install apisix apisix/apisix --namespace ingress-apisix \\ --set gateway.type=NodePort \\ --set ingress-controller.enabled=true \\ --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix \\ --set ingress-controller.config.kubernetes.enableGatewayAPI=true As in our previous tutorials, we will deploy bare-minimum-api as our sample application:\nkubectl run bare-minimum-api-v1 --image navendup/bare-minimum-api --port 8080 -- 8080 v1.0 kubectl expose pod bare-minimum-api-v1 --port 8080 kubectl run bare-minimum-api-v2 --image navendup/bare-minimum-api --port 8080 -- 8080 v2.0 kubectl expose pod bare-minimum-api-v2 --port 8080 Configuring Canary Release With the Gateway API A canary release allows you to rollout new changes in your application gradually. You can configure this in APISIX through the Kubernetes Gateway API as shown below:\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: canary-release spec: hostnames: - local.navendu.me rules: - backendRefs: - name: bare-minimum-api-v1 port: 8080 weight: 50 - name: bare-minimum-api-v2 port: 8080 weight: 50 We use the HTTPRoute API to configure our Route in APISIX. The APISIX Ingress controller will convert the HTTPRoute resource to APISIX configuration.\nAs in the previous tutorial, you can adjust the weights of the traffic split to configure a canary release. With the configuration shown above, traffic will be split equally between the two versions of the bare-minimum-api:\n➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! What\u0026rsquo;s Next? This tutorial gave you a quick walkthrough on how you can use the Kubernetes Gateway API with Apache APISIX.\nYou can learn more about using APISIX as your Kubernetes Ingress/Gateway from apisix.apache.org.\nSee the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress\u0026rdquo;.\n","permalink":"https://navendu.me/posts/kubernetes-gateway-with-apisix/","summary":"A hands-on tutorial on using the new Kubernetes Gateway API with Apache APISIX Ingress.","title":"Kubernetes Gateway API With APISIX Ingress"},{"content":" I don\u0026rsquo;t know if I would classify this as an article. This is a note an open source maintainer made about open source communities. In the future, I might turn this into a complete article.\n The pandemic brought a cultural shift in our work environments, making it similar to the distributed collaboration practices in open source.\nOpen source software has a history of becoming self-governing, forming communities of passionate developers who manage to build and sustain the project effectively.\nHowever, as open source becomes a popular way to build software, we realize that these communities don\u0026rsquo;t form themselves.\nGorilla Web Toolkit, a popular Go-based library for building HTTP web applications, went into archive mode three weeks ago.\nThe project checked all the boxes that made an open source project successful. It had a large user base1 (two of their most popular projects, mux and websocket, has 18,000 stars each on GitHub), stood the test of time, and had 100+ contributors.\nI\u0026rsquo;m pretty sure that top tech companies are using the Gorilla libraries. But even after four years of announcing that he was stepping down from the maintainer, Gary Burd couldn\u0026rsquo;t find someone else to hand over the keys. No corporate companies using Gorilla stepped up to take over the project.\nThis is a story as old as open source itself. Big corporations reap the rewards of free and open source software and give back nothing in return.\nBut was this inevitable? No. Does this mean that open source projects aren\u0026rsquo;t sustainable no matter how successful they are? Certainly not2.\nArticles like \u0026ldquo;Open Source is Broken\u0026rdquo; talk about the importance of supporting open source maintainers financially to help sustain projects. But today, I want to talk about this from the perspective of an open source project/maintainer. Specifically, I want to talk about open source communities that revolve around open source projects.\nOpen source projects are a by-product of its community. And as I mentioned, these communities don\u0026rsquo;t form themselves as soon as you open source your project.\nProject maintainers need to take the initiatives to build a community. At first, the community will likely be a handful of people and would take a lot of effort to run. But as time passes and the community gets bigger, it becomes self-sustainable.\nA big, thriving community can mean a lot more users, contributors, and ideas for the project. More users mean more people to support the project financially, more contributors mean faster bug fixing and feature updates, and more ideas mean a better overall project.\nA community can also ensure balance in an open source project. If a project has a large user base but only has a single maintainer, it is wildly unsustainable3. In such scenarios, building a community can ensure that more people are working on the project to meet the enormous demand.\nBuilding communities can start with something simple, like creating a platform for the community members to interact. Instead of the maintainers communicating one way with code, a common platform for the community to interact can make a significant impact.\nDepending on what happens after that, project maintainers can choose to grease the wheels of the community, encouraging more people to participate actively than \u0026ldquo;lurking.\u0026rdquo;\nWhen it comes to sustenance, open source projects and maintainers have to take measures to build their community and not depend entirely on the goodwill of their users to support the project on their own. A community lacking participation, goals, or consensus can bring down an open source project more quickly than no community.\nMore and more projects and companies are realizing the benefits of communities4. Many more open source projects are actively funded today while being entirely community-driven.\nEncouraging open source projects to focus on the community can help avoid open source abandonware in the future. Open source foundations and maintainers should advocate fellow maintainers to follow suit and support their projects to grow sustainably.\n  GitHub shows 94,629 repositories and 31,501 packages that depend on gorilla/mux.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There might be a lot of factors that led to the Gorilla project going into archive mode, and I mentioned it to give an example of the problem at hand. I\u0026rsquo;m sure that the project maintainers had to archive the project. I also have a project in archive mode because I can no longer spend time actively maintaining it, and nobody stepped up to take the project from my co-maintainer and me.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The xkcd comic, Dependency, sums this up pretty well.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This article by Orbit CEO Patrick Woods explains this changing trend clearly and why it is the future.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://navendu.me/posts/open-source-communities/","summary":"A note on the role of communities in open source.","title":"On Open Source Communities"},{"content":"ChatGPT has been making waves all over the internet ever since its public beta release last month.\nEveryone is amazed at how well it works, and some even have concerns about ChatGPT replacing their jobs.\nRegardless of whether or not we won\u0026rsquo;t have jobs anymore, it is clear that ChatGPT and other AI advancements in this last year are paving the way to the future. So it might be important to understand, at least from a high level, what ChatGPT is and how it works.\nAnd who better to explain ChatGPT than ChatGPT itself (him/her/themselves?)?\nHere is my conversation with ChatGPT explaining what it is, how it works, and how you can use it.\nWhat is ChatGPT?  Navendu: Can you explain what ChatGPT is? The person you are explaining this to does not know AI, neural networks, and other technical jargon. Explain it so that such a person would get a concrete understanding of what ChatGPT is.\n  ChatGPT: Sure, I\u0026rsquo;d be happy to explain ChatGPT to you in a way that avoids technical jargon as much as possible.\nChatGPT is a type of artificial intelligence (AI) that is designed to carry out conversation with humans. It is based on a technology called natural language processing (NLP), which allows it to understand and generate human-like text.\nIn practical terms, this means that you can have a conversation with ChatGPT as if it were a real person. You can type out a message or ask a question, and ChatGPT will respond with a message that is generated based on its understanding of the conversation so far and its knowledge of the subject at hand.\nOne way that ChatGPT can be used is as a chatbot, which is a computer program that simulates conversation with human users, especially over the Internet. ChatGPT is a type of chatbot that uses advanced NLP techniques to generate more realistic and sophisticated responses than some other chatbots.\n The answer is clear and makes sense. But some people might not be familiar with a few technical terms.\nWhat Are These Technical Terms?  N: Thanks. Can you explain all those terms you mentioned—artificial intelligence and natural language processing?\n  CGPT: Certainly! Here are brief definitions of the terms I mentioned:\nArtificial intelligence (AI): This refers to the ability of a computer or machine to perform tasks that would normally require human intelligence, such as understanding language, recognizing patterns, and making decisions. There are different types of AI, including narrow AI (which is designed to perform a specific task) and general AI (which is designed to perform a wide range of tasks).\nNatural language processing (NLP): This refers to the ability of a computer or machine to process and understand human language. NLP is a subfield of AI that focuses on the interaction between computers and humans using natural language. It involves techniques for analyzing, interpreting, and generating human-like text and speech.\nI hope this helps give you a better understanding of these terms!\n This gives a good understanding of the underlying technologies used in ChatGPT. Now with this knowledge, can we try to understand a bit more about how ChatGPT works?\nHow Does ChatGPT Work?  N: Given that the person you are explaining this to understands the basics of what ChatGPT is from your responses, can you explain how ChatGPT works in a bit more detail?\n  CGPT: Certainly! Here\u0026rsquo;s a bit more detail on how ChatGPT works:\nChatGPT is a type of artificial intelligence (AI) that is designed to carry out conversation with humans. It is based on a technology called natural language processing (NLP), which allows it to understand and generate human-like text.\nTo carry out a conversation, ChatGPT uses a process called machine learning. This involves feeding the AI a large amount of data, such as transcripts of human conversations or written texts, and using algorithms to analyze this data and learn from it. As a result, ChatGPT is able to generate responses that are more human-like and sophisticated than those of some other chatbots.\nWhen you have a conversation with ChatGPT, it uses its understanding of the conversation so far and its knowledge of the subject at hand to generate a response to your message or question. It does this by analyzing the words and phrases you use, as well as the context of the conversation, and using this information to generate a response that is appropriate and relevant to the conversation.\n If you just heard about AI, NLP, and machine learning from this article, you might be confused about how ChatGPT learns. So let\u0026rsquo;s ask it!\nHow Does ChatGPT \u0026ldquo;Learn\u0026rdquo;?  N: But how does ChatGPT understand the user\u0026rsquo;s conversation? How does ChatGPT \u0026ldquo;learn\u0026rdquo;?\n  CGPT: Great question! Here\u0026rsquo;s a bit more detail on how ChatGPT understands user conversations and \u0026ldquo;learns\u0026rdquo;:\nWhen ChatGPT is fed a large amount of data, such as transcripts of human conversations or written texts, it uses algorithms to analyze this data and learn from it. These algorithms are designed to identify patterns and relationships in the data, and to use this information to make predictions or generate responses.\nFor example, if ChatGPT is fed a large number of conversations about a particular topic, it might learn to recognize common phrases and words that are associated with that topic, and to use these phrases and words in its own responses. It might also learn to recognize the context of a conversation and to generate responses that are appropriate to that context.\nIn this way, ChatGPT is able to \u0026ldquo;learn\u0026rdquo; from the data it is fed, and to use this learning to generate more human-like and sophisticated responses in conversation.\n Now you might be thinking, \u0026ldquo;how does this help me?\u0026rdquo; ChatGPT has answers for that too.\nHow Does This Help Me?  N: Awesome! Now how can someone use ChatGPT to improve their daily lives?\n  CGPT: There are a number of ways that ChatGPT or other natural language processing (NLP) technologies can be used to improve daily life:\nPersonal assistants: ChatGPT or other NLP technologies can be used to create personal assistants that can help with tasks such as scheduling, sending emails, or setting reminders. This can save time and make it easier to stay organized.\nCustomer service: ChatGPT or other NLP technologies can be used to create chatbots that can provide customer service for businesses. These chatbots can answer questions, resolve issues, and provide information to customers, freeing up human customer service representatives to focus on more complex tasks.\nEducation and language learning: ChatGPT or other NLP technologies can be used to create educational tools that can help people learn new subjects or improve their language skills. For example, a chatbot could be used to provide personalized language lessons or to offer explanations and practice exercises for a particular subject.\nEntertainment: ChatGPT or other NLP technologies can be used to create chatbots that can carry on interesting and engaging conversations with users. This can provide a fun and interactive way to pass the time or to learn about new topics.\n Jeez! That\u0026rsquo;s a lot. I\u0026rsquo;m excited and scared at the same time about losing my job to some computer code. Do we have something to worry about?\nWill ChatGPT Take My Job?  N: That\u0026rsquo;s cool! Does this mean that ChatGPT and other NLP technologies might replace humans for some specific tasks?\n  CGPT: It\u0026rsquo;s possible that ChatGPT and other natural language processing (NLP) technologies could be used to perform some tasks that are currently done by humans, especially tasks that involve processing and understanding large amounts of text or language data. For example, ChatGPT or other NLP technologies could be used to automate tasks such as customer service or data entry, which currently require human labor.\nHowever, it\u0026rsquo;s important to note that ChatGPT and other NLP technologies are not yet capable of replacing humans for all tasks. They are still limited by their inability to fully replicate human intelligence and creativity, and there are many tasks that require these qualities that cannot be automated.\nAdditionally, the use of ChatGPT and other NLP technologies to automate certain tasks could have implications for employment and the labor market. While automation can often improve efficiency and reduce costs, it can also lead to job displacement for some workers. As with any technological advancement, it is important to consider the potential impacts on society and to take steps to address any negative consequences.\nI hope this helps give you a better understanding of the potential and limitations of ChatGPT and other NLP technologies!\n Well, I think AI has the potential to change our jobs for good.\nWe might not need to do a lot of our current jobs, but AI could replace them with something novel. This could be similar to how computers replaced a lot of jobs when they first became mainstream.\nShould I Try ChatGPT? If you made it this far, you are really interested in ChatGPT. So give it a try. It will be fun.\nDon\u0026rsquo;t take my word for it. Ask the AI.\n N: Okay. What do you have to say to someone who hasn\u0026rsquo;t tried out ChatGPT yet but is interested in it?\n  CGPT: If you haven\u0026rsquo;t tried out ChatGPT yet but are interested in it, I would recommend giving it a try! ChatGPT is a fun and interactive way to learn about natural language processing (NLP) and artificial intelligence (AI), and it can be a great source of entertainment and information.\nThere are a number of ways you can try out ChatGPT or other NLP technologies:\nOnline demos: Many websites offer online demos of ChatGPT or other NLP technologies that you can try out for free. These demos often allow you to type in a message or question and see how the AI responds.\nChatbot apps: There are a number of chatbot apps that use NLP technologies, including ChatGPT, that you can download and use on your phone or computer. These apps can provide a fun and interactive way to have conversations with a chatbot.\nEducational resources: There are many online resources, such as tutorials and courses, that can help you learn more about NLP and AI, including ChatGPT. These resources can be a great way to get started with these technologies and understand how they work.\n You can try out ChatGPT for free at chat.openai.com.\n","permalink":"https://navendu.me/posts/chatgpt-explains-chatgpt/","summary":"A conversation with ChatGPT about ChatGPT. Who are you?","title":"ChatGPT Explained by ChatGPT"},{"content":"The default Kubernetes Ingress resource exposes many standard features provided by Ingress controller implementations. However, if you use Ingress controllers like Apache APISIX, the default Ingress resource will limit its full capabilities.\nThis tutorial will look at how you can use annotations, custom resource definitions (CRDs), and Plugins to extend Kubernetes Ingress to include the full capabilities of APISIX.\nBefore you move on, make sure you:\n Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install the sample application and APISIX in your Kubernetes cluster.  Annotations Ingress controller implementations use annotations for configuring additional parameters. Each of the implementations has different annotations that are unique to it.\nAPISIX supports 14 annotations which you can use to enable and configure features not exposed by the default Ingress resource.\nIn our example, we will configure APISIX to only allow traffic from a single IP address. This can be configured by using the annotation k8s.apisix.apache.org/allowlist-source-range as shown below:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes annotations: k8s.apisix.apache.org/allowlist-source-range: \u0026#34;172.17.0.1\u0026#34; spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api-v1 port: number: 8080 path: /v1 pathType: Exact - backend: service: name: bare-minimum-api-v2 port: number: 8080 path: /v2 pathType: Exact Now, if we make requests from a different IP address, we will get the response:\n{\u0026#34;message\u0026#34;:\u0026#34;Your IP address is not allowed\u0026#34;} But the problem with annotations is that they can get messy. Nginx Ingress has more than 100\u0026#43; annotations, and using them takes work.\nCustom CRDs Instead of restricting your additional configurations to annotations, you can use APISIX\u0026rsquo;s custom CRDs.\nThese are custom Kubernetes resources tailored for configuring APISIX. The configuration is similar if you are already familiar with APISIX, making it much easier to leverage the complete feature set of APISIX.\nThe example below shows how you can split traffic between two services using the ApisixRoute CRD:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: method-route spec: http: - name: method match: hosts: - local.navendu.me paths: - /api backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 70 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 30 Now, when you send requests, APISIX will split the traffic 70:30 between the two services:\nfor i in {1..20} do curl http://127.0.0.1:57761/api -H \u0026#39;host:local.navendu.me\u0026#39; done Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v2.0! Hello from API v2.0! Plugins APISIX comes with 80\u0026#43; Plugins out of the box. You can also create your own Plugins for tailored use cases. These Plugins allow you to extend APISIX\u0026rsquo;s capabilities to include features like authentication, security, traffic control, and observability.\nFor our example, we will use the limit-count Plugin to limit the number of requests a client can send in a given time. We can create a Route and configure the Plugin with the ApisixRoute resource:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: method-route spec: http: - name: method match: hosts: - local.navendu.me paths: - /api backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 50 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 50 plugins: - name: limit-count enable: true config: count: 10 time_window: 10 Now, APISIX will only allow ten requests every ten seconds for one client:\nHello from API v1.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v2.0! Hello from API v2.0! Hello from API v2.0! Hello from API v1.0! \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;503 Service Temporarily Unavailable\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;503 Service Temporarily Unavailable\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;openresty\u0026lt;/center\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Powered by \u0026lt;a href=\u0026#34;https://apisix.apache.org/\u0026#34;\u0026gt;APISIX\u0026lt;/a\u0026gt;.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;503 Service Temporarily Unavailable\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;503 Service Temporarily Unavailable\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;openresty\u0026lt;/center\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Powered by \u0026lt;a href=\u0026#34;https://apisix.apache.org/\u0026#34;\u0026gt;APISIX\u0026lt;/a\u0026gt;.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; What\u0026rsquo;s Next? This tutorial gave you an introduction to how you can extend APISIX Ingress. See the resources below to learn more about annotations, CRDs, and Plugins:\n List of available annotations APISIX CRDs documentation APISIX CRDs reference APISIX Plugin documentation  See the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress\u0026rdquo;.\n","permalink":"https://navendu.me/posts/extending-apisix-ingress/","summary":"A hands-on tutorial on leveraging the full features provided by APISIX in Kubernetes Ingress.","title":"Extending Apache APISIX Ingress with Annotations, CRDs, and Plugins"},{"content":"GNU Privacy Guard, also known as GPG, is a popular open source software for secure communication.\nI have been using GPG keys recently to sign my commits and for encrypted messages.\nIn this article, I will explain GPG keys, how they work, and how you can use them.\nWhat is GPG? GPG is an implementation of OpenPGP, a standard for authenticating or encrypting data using public key cryptography.\nA user will have a public key that they can share with anyone and a private key that should be secret. You can use the private key to sign or decrypt messages, while others will use you public key to verify your signature or encrypt messages.\nTo send you a confidential message, anyone can use your public key, encrypt the message, and send it to you. You can read it after decrypting it with your private key.\nflowchart TB m(\"👥 Confidential message\") -- |\"🔐 Encrypt (Public key)\"| em(Encrypted message) -- |\"🗝️ Decrypt (Private key)\"| dm(\"👨🏽 Decrypted/Original message\") style em stroke: red style dm stroke: green  You can also sign your messages with your private key, and others can verify your signature with your public key.\nflowchart TB sm(\"👨🏽 Sensitive message\") -- |\"🖊️ Sign (Private key)\"| sms(\"🔏 Sensitive message + signature\") -- |\"👥 Verify (Public key)\"| v(\"👥 Verified message\") style sms stroke: green  Installing GPG You can download and install GPG from the official website if it isn\u0026rsquo;t already installed.\nTo check if it is installed properly, run:\ngpg -h Generating a New GPG Key Pair You can create a new key pair by running:\ngpg --full-generate-key It will prompt you several times to configure your keys:\n Kind: RSA and RSA (default) Size: \u0026gt; 4096 (for signing commits) Validity: Could be days, weeks, months, or years. For example, 1y will set it to one year. Name, email, and comment: This email and name will be associated with your signature. Passphrase: Secure passphrase to unlock the private key. Make it strong. Use a password generator if possible.  Once you complete all the prompts, gpg will generate a key pair.\n Tip: As gpg uses entropy to generate the key pair, it will depend on how active your system is. To generate more entropy, you can use something like rng-tools.\n To verify whether the key pair was created, you can run:\ngpg --list-keys Generating a Revocation Certificate A revocation certificate can invalidate your public key if you forget your passphrase or compromise your private key.\nYou should generate this as soon as you create a key pair and store it in a separate location.\nTo generate a revocation certificate, run:\ngpg --output ~/revocation.crt --gen-revoke your-email@your-provider.com When you revoke a public key, people can no longer use it to send encrypted messages to you. But it can still verify your past signatures and decrypt past messages.\nExporting Your Public Key To use GPG keys, you must export your public key. In an upcoming section, I will discuss how you can share the exported key with others in a public keyserver.\nTo export your key, run:\ngpg --armor --export your-email@your-provider.com You can also export it to a file by running:\ngpg --output ~/public.key --armor --export your-email@your-provider.com Signing Commits with Your GPG Key Pair All of my projects and the projects I contribute to are on GitHub. If you are using other platforms like GitLab, you can follow their official documentation.\n Note: The email associated with your GPG key should match a verified email configured in your GitHub account.\n On GitHub, go to \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Access\u0026rdquo; \u0026gt; \u0026ldquo;SSH and GPG keys\u0026rdquo; \u0026gt; \u0026ldquo;New GPG key\u0026rdquo;.\n Configuring public key on GitHubThis will be used to verify our signature on commits\n  In the \u0026ldquo;Key\u0026rdquo; field, paste the public key you exported from -----BEGIN PGP PUBLIC KEY BLOCK----- to -----END PGP PUBLIC KEY BLOCK----- including both.\nClick \u0026ldquo;Add GPG key\u0026rdquo; and enter your password to confirm.\nNext, you have to configure Git to use your created GPG private key to sign your commits.\nFirst, find the long form of your key ID by running:\ngpg --list-secret-keys --keyid-format=long /Users/user1/.gnupg/pubring.kbx --------------------------------- sec rsa4096/3AA5C34371567BD2 2022-10-12 [SC] 29D5E24EA8EF21FD70A8F2D3B33049A4551D uid [ultimate] Firstname Lastname (comment) \u0026lt;your-email@your-provider.org\u0026gt; ssb rsa4096/4BB6D45482678BE3 2022-10-12 [E] In this example, the key ID is 3AA5C34371567BD2.\nNow, use this key ID to configure Git:\ngit config --global user.signingkey 3AA5C34371567BD2  Tip: To sign all commits by default, run:\ngit config --global commit.gpgsign true  Now, when you commit, use the -S flag. You should also be able to configure your IDE to do this for you automatically for every commit.\nIf you push these commits to GitHub, you will see that they are verified using your public key.\n Verified commitsAll my commits are verified now\n  Sending Encrypted Messages with Your GPG Key Pair Encrypting Messages Anyone can use your public key to send encrypted messages to you. But first, you have to make your public key, well, public.\nYou can copy your public key and share it with people who want to send you encrypted messages. I have shared my public key on my website, and people can copy it and use it to encrypt messages.\nYou can also upload your key to a public key server like pgp.mit.edu:\ngpg --send-keys --keyserver pgp.mit.edu 3AA5C34371567BD2  Note: 3AA5C34371567BD2 is the key ID. Replace it with your key ID.\n Now, anyone will be able to request your public key from the key server with the command:\ngpg --recv-keys keyid To encrypt messages, get the public key of the person you are sending the message to. Make sure to add yourself as a recipient if you want to decrypt the message in the future:\ngpg --encrypt --sign --armor -r receiver-email@receiver-provider.com -r sender-email@sender-provider.com name-of-file This will create a .asc file containing your encrypted message.\nDecrypting Messages To decrypt a message, you can run gpg, and it will prompt you as necessary:\ngpg name-of-file.asc This will create a new file with the decrypted message.\nYou can also configure your email clients or other applications to use this key pair for encrypted communication. I use the Thunderbird email client, and its documentation explains how you can configure your GPG key.\n That\u0026rsquo;s it! I\u0026rsquo;m not a security or cryptography expert, but I will probably look into where I can use these keys next.\nI will write a new article or update this when I do so.\n","permalink":"https://navendu.me/posts/encrypted-communication-with-gpg/","summary":"Exploring how symmetric-key and public-key cryptography works by using GPG keys to sign commits and send encrypted messages.","title":"Signed Commits and Encrypted Communication with GPG Keys"},{"content":"I was hit with a wave of nostalgia recently and decided to go back and play some of my favorite retro games (instead of buying a PS5). I remembered how much fun I used to have playing these games and wanted to experience that again.\nI decided to use a Raspberry Pi, RetroPie, and a GPi CASE 2 to make my own handheld retro gaming device.\nThis tutorial will walk you through how I set all these up.\nBefore you move on, make sure you have:\n Raspberry Pi Computer Module 4 (Lite version with WiFi recommended) GPi CASE 2 A Micro SD card (at least 16 GB) and an adapter  I will try to be comprehensive so that you don\u0026rsquo;t have to spend time searching for information elsewhere. I will include troubleshooting steps and additional resources to help you along the way. So let\u0026rsquo;s get started!\nInserting CM4 Into the GPi CASE 2 There is a best way to insert the Pi in the case:\n Take off the back cover by removing the five screws in the back Align the markings on the Pi and the case Click the Pi into place by applying even pressure  I tried inserting the Pi without removing the back cover, and it wasn\u0026rsquo;t inserted correctly. To ensure that you don\u0026rsquo;t damage anything, follow the abovementioned steps.\nWriting the RetroPie Image to the SD Card The easiest way to download and write the RetroPie image is by using Raspberry Pi Imager.\nYou can download the Raspberry Pi Imager and open it. Connect your SD card to your computer through the adapter (if needed). You are now ready to write the image.\nIn the Raspberry Pi Imager, select the operating system. Choose \u0026ldquo;Emulation and game OS\u0026rdquo;, \u0026ldquo;RetroPie\u0026rdquo;, and the \u0026ldquo;RetroPie 4.8 (RPI 4/400)\u0026rdquo; OS.\n Selecting the RetroPie OSMake sure to choose the RPI 4/400 version\n   Note: Make sure to choose the one for RPI 4/400 if there is a newer version than 4.8.\n Select your SD card from the \u0026ldquo;Storage\u0026rdquo; option and click on \u0026ldquo;Write\u0026rdquo;. It will take some time to download and write the image to your SD card. Grab a cup of coffee.\nInstalling Display Patch and Safe Shutdown Since RetroPie defaults output to HDMI, we have to install a patch to transfer the display output to the GPIO pins.\nOnce you have the RetroPie image written to your SD card, you can install this patch.\nDownload the patch from the Retroflag website and extract it on your computer and follow the steps below:\n From the SD card, make a backup of config.txt on your computer. You can revert to this config if you mess things up. Copy the contents of the GPi_Case2_patch_retropie folder to the SD card. If you are on Windows, click on install_patch.bat, and it will install the patch. If you are on Linux or macOS, copy the contents of the GPi_Case2_patch_retropie/patch_files folder to the SD card. The files in the patch_files/overlays folder should go in the overlays folder on the SD card.  Next, you need to configure safe shutdown. To make our lives easier and not type out the command in the GPi CASE, you can create a script.\nCreate a new file on the SD card named gpi.sh and add the following content to the file:\nwget -O - \u0026#34;https://raw.githubusercontent.com/RetroFlag/GPiCase2-Script/main/retropie_install_gpi2.sh\u0026#34; | sudo bash Configuring WiFi Credentials Running the gpi.sh script will pull some files from the internet. So, you need to set up WiFi on your GPi CASE.\nTo do this easily, you can create a new file called wifikeyfile.txt and add the following:\nssid=\u0026quot;name of your WiFi (case sensitive)\u0026quot; psk=\u0026quot;your WiFi password\u0026quot;  Note: If your Raspberry Pi does not have WiFi, you can see this Reddit post for workarounds.\n First Boot With your SD card configured, you can disconnect it from your PC and insert it into your GPi CASE.\nBefore turning it on, connect it to power to ensure the battery does not die on the first boot.\nThe first boot will take longer (a couple of minutes at least) than usual. Be patient, and don\u0026rsquo;t worry.\nOnce it is ready, RetroPie will walk you through some configurations you can follow on the screen.\n Hello RetroPie!Follow the instructions on the screen to configure controls\n  Set Up WiFi You are not ready to play games yet!\nGo to the RetroPie configuration menu from the home screen and scroll down to WiFi.\n Configure WiFiFor more information on all the configuration options, see the RetroPie website\n  Once you go into the WiFi configuration menu, you will see a message saying that you don\u0026rsquo;t have your WiFi country set.\nSelect \u0026ldquo;Yes\u0026rdquo;, and in the following menu, choose your country from \u0026ldquo;Localisation Options\u0026rdquo;.\nYou can then save and exit, and RetroPie will prompt you to reboot the device.\nOnce you reboot the device, you can connect to your WiFi.\nNow, if you go to the WiFi configuration menu, you will see an option to \u0026ldquo;Import WiFi credentials from /boot/wifikeyfile.txt\u0026rdquo;. Select it and wait for some time.\nYou will see that the WiFi is connected, and you can exit the menu.\nRunning the Safe Shutdown Install Script In your RetroPie configuration menu, select \u0026ldquo;File Manager\u0026rdquo;.\nGo back twice to reach the root and navigate to the /boot folder. Scroll down, find the gpi.sh file, and run it. It will install the safe shutdown scripts, and the Pi will reboot.\nTo test if the scripts are running, try turning off through the hardware switch, and you will see a shutdown message.\nThat\u0026rsquo;s it! You are all ready to install some retro games and play. You should be easily able to find ROMs by searching the internet. Some of these are pirated, so sharing the sites is not super legal.\n I\u0026rsquo;m currently obsessed with Pokemon FireRed. Gotta catch\u0026rsquo;em all!\nTroubleshooting If you frequently see an error message like \u0026ldquo;failed to find mixer elements!\u0026quot;, see this fix by Alan Pfahler.\nIf you are trying to configure audio and are running into issues, you can see this thread on the RetroPie forum.\n","permalink":"https://navendu.me/posts/retropie-gpi-case-2-setup/","summary":"A complete tutorial on how to set up RetroPie and play retro games on a GPi CASE 2 with a Raspberry Pi Compute Module 4 under the hood.","title":"Retro Gaming With RetroPie, GPi CASE 2, and a Raspberry Pi"},{"content":"The 13 years of Bitcoin and other cryptocurrencies have created a cult of gamblers, a Trojan horse for Ponzi schemes 1, and an ecosystem to help illicit financing 2, money laundering 3, and sanctions evasions 4.\nIts underlying technology, which some people fancy to be a liberal utopia, will never be a solution because it is trying to solve a financial problem that doesn\u0026rsquo;t exist.\nLaszlo Hanyecz became a part of the crypto history books in 2010 by using 10,000 Bitcoins to buy two large pizzas 5. These 10,000 Bitcoins worth $41 in 2010 are now worth $166,075,000. At its all-time high, 10,000 Bitcoins could get you $687,896,300 or 4,58,59,753 large pepperoni pizzas from Papa John\u0026rsquo;s.\nBitcoin\u0026rsquo;s \u0026ldquo;decentralized\u0026rdquo; nature, with its price dictated by celebrity tweets and subreddits, makes it too volatile to do anything useful. The \u0026ldquo;freedom\u0026rdquo; it gains from not having a central governing authority makes it unsuitable to be money 6.\nThe claim that Bitcoin and other cryptos are \u0026ldquo;decentralized\u0026rdquo; is also invalid 7. You depend on third-party wallets to store and manage your holdings, and the consensus algorithms that govern these are inherently centralized by a small set of elite players in the network 8.\nEssentially, crypto will take power from the government and distribute it to these private enterprises.\nWhile Silicon Valley is still fantasizing about crypto and Web3, India has been leveling up its digital payments game with Unified Payments Interface or UPI.\nUPI is a system that has centralized and standardized digital payments in India. It is managed by National Payments Corporation of India (NPCI), an entity regulated by the Reserve Bank of India (RBI) 9.\nIn the six years since it was introduced, there have been 600 million UPI users in India. A household population of 300 million means that two people in every Indian household use UPI 10.\nThe goal of digital payment systems should never have been to decentralize but democratize. UPI is the ultimate example of this in action.\nUPI does not rely on a single bank or company compared to private digital payment alternatives in countries like the US (PayPal, Apple Pay). Instead, it provides a standard set of APIs that companies and banks can use to facilitate online payments.\nThis means that there are a lot of UPI payment apps you can choose from, and all of them interoperate with each other. Facebook\u0026rsquo;s WhatsApp Payments, Google\u0026rsquo;s GPay, and Walmart\u0026rsquo;s PhonePe (Flipkart) are some popular ones.\nPayments are easy. You can use a unique UPI ID similar to an email address (navendupottekkat@okaxis is my real UPI ID) or scan a QR code that will work on any UPI app. Money will be moved directly from your bank account to the payee\u0026rsquo;s account without intermediaries. These transactions are instantly reflected in your bank account.\nIndia accounted for the largest number of real-time transactions in 2021, with 48.6 billion transactions. The combined number from the US, Canada, UK, France, and Germany was 7.5 billion 11.\n48.6 billion only represents 31.3% of all transactions in the country. By 2026, it is expected to increase to 70.7%.\nImplementing UPI was relatively more straightforward in India as there are lesser banks and all regulations are handled centrally by the RBI. Countries like China can be on par with India, but private digital wallets like WePay and AliPay still dominate.\nUPI transactions are currently free (maybe a small fee in the future). Along with the pandemic, this has helped accelerate UPI adoption.\nStill, UPI\u0026rsquo;s success with handling payments at such a large scale has prompted countries like UAE, Singapore, and more recently, France to integrate it with their systems 12.\nOther countries like the US are building their own UPI-like systems with the help of central entities like the Federal Reserve 13.\nCrypto has failed to be the payment system of the future. The current trend of fluctuating hype cycles with crypto, NFTs, and adjacent scams will inevitably end.\nRelying on a central authority and democratizing the system seems the best and proven way to move forward. I would not be surprised to see more countries adopting systems similar to UPI in the future.\n  Krugman, Paul. 2021. \u0026ldquo;Technobabble, Libertarian Derp and Bitcoin.” The New York Times. May 20, 2021. https://www.nytimes.com/2021/05/20/opinion/cryptocurrency-bitcoin.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dion-Schwarz, Cynthia, David Manheim, and Patrick B. Johnston. 2019. \u0026ldquo;Terrorist Use of Cryptocurrencies: Technical and Organizational Barriers and Future Threats.\u0026rdquo; Santa Monica, CA: RAND Corporation. 2019. https://www.rand.org/pubs/research_reports/RR3026.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Orcutt, Mike. 2020. “This Is How North Korea Uses Cutting-Edge Crypto Money Laundering to Steal Millions.” Web log. MIT Technology Review. March 5, 2020. https://www.technologyreview.com/2020/03/05/916688/north-korean-hackers-cryptocurrency-money-laundering/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Busch, Kristen, and Paul Tierno. 2022. “Russian Sanctions and Cryptocurrency.” Congressional Research Service. May 4, 2022. https://crsreports.congress.gov/product/pdf/IN/IN11920#:~:text=Potential%20Sanctions%20Evasion%20with%20Cryptocurrency,tamper%2Dresistant%20records%20of%20transactions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Ashmore, Dan, and Farran Powell. 2022. “Bitcoin Price History 2009 to 2022.” Forbes Advisor INDIA. November 1, 2022. https://www.forbes.com/advisor/in/investing/cryptocurrency/bitcoin-price-history-chart/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Taleb, Nassim N. 2021. \u0026ldquo;Bitcoin, Currencies, and Fragility.\u0026rdquo; arXiv. June 27, 2021. https://doi.org/10.48550/arXiv.2106.14204.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dailey, Natasha. 2022. “Crypto Isn’t Decentralized. It’s Actually Run by a Handful of Big Wigs Exploiting Low-Paid Workers, Says Long-Time Internet Academic.” Markets Insider. March 20, 2022. https://markets.businessinsider.com/news/currencies/crypto-isnt-decentralized-nft-bored-ape-yacht-club-buys-cryptopunks-2022-3.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n L. J. Valdivia, C. Del-Valle-Soto, J. Rodriguez and M. Alcaraz. 2019. \u0026ldquo;Decentralization: The Failed Promise of Cryptocurrencies,\u0026rdquo;. IT Professional, vol. 21, no. 2, pp. 33-40. 1 March-April 2019. https://ieeexplore.ieee.org/abstract/document/8676128.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n National Payments Corporation of India. “India’s Unified Payment Gateway for Real-Time Payment Transactions.” https://www.npci.org.in/PDF/npci/upi/Product-Booklet.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Gupta, Vikas. 2022. “Move over Bitcoin, Here Comes UPI, the next Big Investment Idea.” Moneycontrol. August 17, 2022. https://www.moneycontrol.com/news/business/personal-finance/move-over-bitcoin-here-comes-upi-the-next-big-investment-idea-9039791.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Barthe, Blanca, and Samuel Murrant. 2022. “Prime Time for Real-Time Global Payments Report.” ACI Worldwide. April 2022. https://www.aciworldwide.com/real-time-payments-report.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Jain, Sourabh. 2022. “You Will Soon Be Able to Use UPI in France in Addition to UAE, Singapore.” Business Insider. June 16, 2022. https://www.businessinsider.in/finance/news/upi-and-rupay-cards-will-soon-be-accepted-in-france/articleshow/92247190.cms.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n “Federal Reserve Updates FedNow Service Timing to Mid-2023, Marks Beginning of Full-Scale Pilot Testing.” 2022. Board of Governors of the Federal Reserve System. August 29, 2022. https://www.federalreserve.gov/newsevents/pressreleases/other20220829a.htm.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://navendu.me/posts/upi-vs-crypto/","summary":"Cryptocurrencies are trying to be a solution to a financial problem that does not exist. UPI solved digital payments years ago.","title":"No Thanks, Crypto, UPI Already Solved Digital Payments"},{"content":" If you are reading this, it\u0026rsquo;s just me thinking aloud about my future, reminiscing and critically evaluating my decisions throughout the last few years.\n I turned 25 this week.\nIt is the time of the year when I evaluate where I\u0026rsquo;m at and decide where I want to be next.\nMy overarching goal in the last few years was to focus on my career.\nI was out of college with absolutely no idea what I wanted to do next. I tried different jobs, worked almost always to get better, and had zero social life.\nThere wasn\u0026rsquo;t much else in my life outside of work. I knew I had to figure it out and set up my career. And today, I feel like I did that.\nI\u0026rsquo;m at a job I love. It is impactful work and aligned with my goals in life. I have immediately actionable and long-term plans to continue on this path.\n Goal 1: Learn continuously and do things that impact the world.\n Working on my career and having no social life has taken a toll on my relationships.\nAfter COVID hit, I stopped using my phone and stopped being active on social media. It helped my mental health and career but disconnected me from my friends and family. Now even though I have a phone, there is no going back to social media.\nThis year, I traveled a lot. I met people who I\u0026rsquo;ve known online. I met my old friends and made new ones. With my career on track, I can focus on maintaining relationships.\n Goal 2: Rebuild old relationships with friends, put more time and effort into maintaining relationships, meet new people and make new relationships.\n Being forced to work on my own had rekindled my creative aspirations in the past years. Being curious and obsessively building things led me to great opportunities.\nIn 2020, I left my job to work on things that interested me. I gave myself the time and freedom to explore whatever I liked until I found something I could see myself doing for a long time.\nBuilding up to a position to take that risk and taking that risk was probably the best decision I made in my life.\n Goal 3: Always make room to take risks, be creative, and start from scratch.\n Your experiences make who you are.\nPeople often say I\u0026rsquo;m an impressionable person. I don\u0026rsquo;t think I\u0026rsquo;m naive, but many of my experiences have shaped me into who I am.\n17-21 might have been when I frequently had many new experiences. Most people feel the same because that\u0026rsquo;s when you go into the real world.\nI feel like experiencing a lot of things when you are still susceptible to change can be pretty impactful. The last couple of years went by without a lot of new experiences. But I\u0026rsquo;m still young, and it is not too late to change things.\n Goal 4: Invest time, energy, and money in new experiences. Travel a lot, do a lot of different stuff, and be open-minded and impressionable.\n The one thing I would recommend everyone to learn actively is personal finance.\nYou don\u0026rsquo;t need to be an expert but being financially literate is necessary for the future.\nI\u0026rsquo;m earning more money than I ever had before. Apart from some responsibilities, I can afford to be liberal with my money. I have to start being systematic with how I spend and save money.\n Goal 5: Diversify income and investment streams. Be responsible financially.\n You can\u0026rsquo;t be genuinely wealthy without being healthy.\nIn the last few years after college, I have been taking care of my health pretty well. I exercise regularly and eat good food.\nBeing healthy physically has also helped my mental health. Recently I have been endorsing my friends to do the same.\nWhat I don\u0026rsquo;t often do now is sports. Not having your old mates back home is one primary issue, but I have met new people recently who invited me to their teams. I have to find them and play a couple of matches. Awkward!\nSetting up a routine to play sports and joining some clubs might be the best solution in the future.\n Goal 6: Take active steps to be healthy and participate in sports.\n Setting too many goals might make it harder to follow through.\nAn alternative might be to keep the goals short and see how I did on my next birthday.\nI want these goals to be high-level so that I can see them as North stars in all my decision-making processes.\nIdeally, all decisions I make and the things I do in the following years will be aligned with my goals. Only time will tell.\n","permalink":"https://navendu.me/posts/reasonable-birthday-goals-2022/","summary":"Evaluating where I am and deciding where I want to be next.","title":"My Reasonable Birthday Goals"},{"content":"I, like many others, recently started using Mastodon.\nIt was confusing initially, but a little exploration helped me find my bearings.\nThis article gives you a quick guide on how to get started with the platform.\nWhat is Mastodon? Mastodon is an open source, decentralized, microblogging platform that could be a viable alternative to Twitter.\nInstead of a central company (or an eccentric tech billionaire) managing the platform, Mastodon consists of multiple independent instances. Anyone can host their own instance of Mastodon and run it any way they want. The source code of Mastodon is public and maintained by open source contributors.\nAs a user, you don\u0026rsquo;t need to host your own instance. You can join any existing ones according to your niche of interest. You will still be able to follow and see posts from people on different instances (see below).\n General and niche serversSee joinmastodon.org/servers\n  Most Mastodon servers run on donations and are not looking to make money. So, you won\u0026rsquo;t see ads or sponsored posts like on other social media.\nSigning Up You can sign up using your email on any of the available servers. You can pick your niche or go to more general servers like mastodon.online.\n Using your email to sign upI signed up on the fosstodon.org server\n  Once you fill in your details, you will receive a confirmation email with an activation link. Click on it, and you are ready to go.\n Note: A Mastodon username would be in the form @user@instance.name. For example, my username is sudo_navendu@fosstodon.org.\n Navigating the UI Once you log in, a welcome wizard will walk you through the UI.\n Homepage of the Fosstodon serverThe UI is pretty intuitive especially if you are a Twitter user\n  You can edit your profile and make it more personal by adding:\n  A bio\n  An avatar and a header image\n Adding a bio and imagesA profile without a bio or avatar might been seen as fishy\n    Profile metadata\n Adding profile metadataYou can add links to your other social accounts or websites\n    In Mastodon, posts (tweets) are called Toots. You can favorite and boost a Toot like how you heart and retweet on Twitter.\n The anatomy of a tootYou can reply to the Toot, boost it, add it to your favorites, or bookmark it\n  Home, Local, and Federated Timelines Unlike platforms like Twitter with a single timeline, Mastodon has three timelines serving different content:\n Home: This is similar to your Twitter timeline. It contains all the Toots from people you follow across all Mastodon instances (the Fediverse). Local: This contains all the public Toots from your instance. Even if you don\u0026rsquo;t follow someone, you will find their public Toots here. You can use it to discover people in your niche. Federated: This timeline contains all public Toots from all the instances your server is federated with. The best analogy to understand this is email. There are multiple email providers, and you can create an email on any of them. But you can still send and receive emails across email providers if you have their address. Similarly, the federated timeline shows Toots from instances other that your server.   TimelinesThese timelines seem pretty useful in discovering people\n  These timelines are chronological, and there is no fancy algorithm trying to keep you engaged.\nMastodon Client Apps Mastodon has an official app for Android and iOS devices. Since Mastodon is open source and has a public API, many third-party apps are also available.\n Available third-party appsI have only used the web app till now. I will try out these apps and leave a review if you are interested\n  Wrap Up That\u0026rsquo;s it for the quick start guide. You should be able to pick more things up as you use Mastodon. You can also refer the documentation to learn more.\n","permalink":"https://navendu.me/posts/mastodon-quick-start-guide/","summary":"Learn to get up and running with the microblogging platform everyone is talking about.","title":"A Quick Start Guide for Mastodon"},{"content":"Do you need a college degree in 2022 to start a career in programming?\nMany big tech companies have publicly said they don\u0026rsquo;t require a college degree for certain jobs. Along with much press, these announcements also made a lot of tech \u0026ldquo;influencers\u0026rdquo; advocating naive students not to pursue college.\nA recent survey by Stack Overflow shows that 87% of software developers have a college degree. 24% of professional developers even have a Master\u0026rsquo;s degree.\nBut the survey also shows that only 62% of developers learn to code from schools or colleges compared to 71% learning from the internet.\nWhile the argument that \u0026ldquo;you can learn everything taught in computer science college courses from the internet\u0026rdquo; is valid, does it mean they are obsolete?\nMy answer is no. I would argue that stem degrees, especially computer science degrees, can help you start a career in programming.\nPeople advocating for no degrees make a lot of arguments like:\n it costs a lot a boot camp or an online course is just as effective a lot of companies don\u0026rsquo;t care about your degrees  But these arguments are always taken out of context and are downright wrong for some audiences.\nColleges costs a lot in countries like the US, which overwhelmingly has a lot of developers. Of course, most tech content creators are from the US and would look at things from a US perspective.\nBut what about countries like India?\nYou can get a computer science engineering degree from a good (not the best) public college with around $600 in tuition. Even if you consider $2,000 for a degree in India, it is significantly much lower than in the US (around $50,000 at least).\nA new graduate in India can make at least $4000 to $5000, giving a higher ROI than for a US graduate.\nA lucrative alternative for these costly college degrees comes in the form of online courses and coding boot camps. And they do make sense in the US, where companies might actually hire people without college degrees.\nCoding boot camps and paid online courses are also a thing in India. Some of these are good and help bridge the gap between what colleges teach and the industry.\nBut will companies hire you without a degree even if you completed courses or boot camps?\nMost won\u0026rsquo;t.\nIndia has a large number of programmers. Even with most of them having a degree, getting hired is still difficult. While you can argue that skill level is a factor, there is no argument that companies will filter you out based on your college degree.\nAnd this makes complete sense. How else do you initially filter out hundreds, if not thousands, of resumes for an entry-level job?\nWith that being said, there are a lot of new startups that are hiring programmers without college degrees.\nIn my four years of working full-time, I have worked at multiple companies in India and abroad, and no one has ever asked me for my college qualifications. But behind the scenes, it may have influenced some hiring decisions.\nArguments supporting no degrees make sense when given the correct context. But understanding your context and judging whether it makes sense for you is what you have to consider.\nWhile college degrees do not imply skill and competency for a job, it is still a huge factor in hiring decisions. There will be a future where college degrees won\u0026rsquo;t matter as much, but it overwhelmingly does today.\n","permalink":"https://navendu.me/posts/college-degree/","summary":"Many companies and tech gurus claim you don\u0026rsquo;t need a college degree for a programming job. But here\u0026rsquo;s why you might want to get one regardless.","title":"You Might Need a College Degree for a Programming Job"},{"content":"A canary release is a process of rolling out a new version of software to a small subset of users before making it generally available. Canary releases can help in testing and controlling new releases and rolling back if there are any issues.\nA simple canary release looks like this:\n 1. Route all traffic to existing version of the application    2. Route some traffic to the new version and test for bugs/issues    3. If everything is okay, route all traffic to the new version and keep the old version on standby   In this hands-on tutorial, we will set up a canary release in Kubernetes using Apache APISIX Ingress.\nBefore you move on, make sure you:\n Go through the previous tutorial for an introduction to Apache APISIX Ingress. Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install and configure kubectl to communicate with your cluster. Install Helm to deploy the APISIX Ingress controller.  Deploying a Sample Application As in the previous tutorial, we will use our sample HTTP server application, the bare-minimum-api. This will act as our versioned service:\n bare-minimum-api   To deploy the two \u0026ldquo;versions\u0026rdquo; of the application, you can run:\nkubectl run bare-minimum-api-v1 --image navendup/bare-minimum-api --port 8080 -- 8080 v1.0 kubectl expose pod bare-minimum-api-v1 --port 8080 kubectl run bare-minimum-api-v2 --image navendup/bare-minimum-api --port 8080 -- 8080 v2.0 kubectl expose pod bare-minimum-api-v2 --port 8080 We will now deploy APISIX Ingress and set up a canary release.\nDeploying APISIX Ingress You can install APISIX and APISIX Ingress controller using Helm:\nhelm repo add apisix https://charts.apiseven.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update kubectl create ns ingress-apisix helm install apisix apisix/apisix \\  --set gateway.type=NodePort \\  --set ingress-controller.enabled=true \\  --namespace ingress-apisix \\  --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix kubectl get pods --namespace ingress-apisix Once all the pods and services are running, you can test APISIX by accessing the Admin API:\nkubectl exec -n ingress-apisix deploy/apisix -- curl -s http://127.0.0.1:9180/apisix/admin/routes -H \u0026#39;X-API-Key: edd1c9f034335f136f87ad84b625c8f1\u0026#39; If you get a response similar to the one shown below, APISIX is up and running:\n{ \u0026#34;action\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;node\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;/apisix/routes\u0026#34;, \u0026#34;dir\u0026#34;: true, \u0026#34;nodes\u0026#34;: [] }, \u0026#34;count\u0026#34;: 0 } To access the Ingress, you can run:\nminikube service apisix-gateway --url -n ingress-apisix You need to keep it running depending on your operating system. Regardless, you will see the IP address of APISIX Ingress. You can then send requests to this address.\nhttp://127.0.0.1:56194 ❗ Because you are using a Docker driver on darwin, the terminal needs to be open to run it.  Note: See the previous tutorial to learn more.\n Configuring Canary Release After verifying that APISIX Ingress is running, you can configure a canary release with APISIX\u0026#39;s CRDs.\nWe will set weights for each service to route traffic proportionately.\nInitially, we want to route all requests to the old version of the service:\n Route all requests to bare-minimum-api-v1   To configure this, we can set the weight to 100 and 0 for the bare-minimum-api-v1 and bare-minimum-api-v2 services, respectively:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: canary-release spec: http: - name: route-v1 match: paths: - /* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 100 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 0 You can apply it to your cluster by running:\nkubectl apply -f canary-release.yaml This will route all traffic to the bare-minimum-api-v1 service. You can test it out by sending a request:\ncurl http://127.0.0.1:56194/  Note: This address is the address of your APISIX Ingress obtained by running minikube service apisix-gateway --url -n ingress-apisix on the installation step.\n If you keep sending multiple requests, you will see that the response is only from bare-minimum-api-v1:\n➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! Now, you can change the configuration to route some traffic, say 5%, to the new version, bare-minimum-api-v2:\n Split requests (95:5) between bare-minimum-api-v1 and bare-minimum-api-v2   You can configure this by editing your manifest file and applying it to your cluster:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: canary-release spec: http: - name: route-v1 match: paths: - /* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 95 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 5 APISIX will hot-reload the new configuration without needing to be restarted.\nNow, if you send requests to the Ingress controller, you will see that some of the requests (5%) are routed to the bare-minimum-api-v2 service:\n➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! Finally, you can route all traffic to the new version of the service.\n Route all requests to bare-minimum-api-v2   To configure this, you can set the weight to 0 for the bare-minimum-api-v1 service and 100 for the bare-minimum-api-v2 service:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: canary-release spec: http: - name: route-v1 match: paths: - /* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 0 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 100 Now, if you send requests, you will see that all requests are routed to bare-minimum-api-v2:\n➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! You successfully migrated your users from the old version of your service to your new version without any downtime!\nIn real-world scenarios, you can keep the older version of the service on standby to roll back if there are any issues.\nWhat\u0026rsquo;s Next? This tutorial taught you how to configure APISIX Ingress for simple canary releases. We tested it out with our sample application.\nYou can also configure complex release strategies with APISIX and its Plugins. I will try to cover these in the following articles. To learn more about APISIX, visit apisix.apache.org.\nSee the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress\u0026rdquo;.\n","permalink":"https://navendu.me/posts/canary-in-kubernetes/","summary":"A hands-on, from-scratch tutorial on setting up canary releases in Kubernetes with Apache APISIX Ingress.","title":"Canary Release in Kubernetes With Apache APISIX Ingress"},{"content":"I have been contributing to open source projects for the past three years. During this time, I have built, scaled, and maintained my own projects and projects in foundations, including CNCF and the ASF. This is my story and the lessons I learned along the way.\nFirst Contributions My first open source contributions were along the lines of \u0026ldquo;push everything you build to GitHub\u0026rdquo;. I was naive and thought pushing the toy projects I built to learn could help me build my resume.\nThese did help my resume at the time, but they were far from valuable open source contributions.\nAfter some time, I was a good enough developer to bring my ideas to fruition. I built a browser extension called NSFW Filter, which used deep learning to filter out NSFW content from the web.\nIt was a simple idea, but there wasn\u0026rsquo;t anything similar in the market. The project became popular and featured on Product Hunt and Hacker News, bringing in many users and contributors to the project.\n Featured on Product HuntI literally googled \u0026ldquo;how to market open source project\u0026rdquo; and found these platforms\n   Front page of Hacker NewsBack then I did not know how cool it was to be on the front page of HN\n  Solve Your Problems, Delegate Solve the problem you are facing and make it open source. Chances are more people face your exact problem and will be your users.\nEven if your project is entirely free and open source, you need to market it to get users. You don\u0026rsquo;t need a marketing team or money to find channels and promote your project. And if your project is valuable, it will market itself after some time.\nOnce your project is big, it might put a toll on you to maintain it yourself. This is when you have to start delegating and get more people to help you.\nIt might be weird at first to give someone else push access to your project, but it is the only way to scale the project and contributions to the project.\n Contributors to NSFW FilterThese people are the best\n  Mentoring Unlike many other things, being an open source contributor did not seem to have any roadmaps. So, when it came to contributing to other open source projects, I was confused. Which project do I contribute to? Where do I start? What if I\u0026rsquo;m not skilled enough?\nAs I sat there puzzled by these questions, I stumbled across the Linux Foundation Mentorship Program.\nThe LFX Mentorship Program provides opportunities for mentees to work on open source projects. Experienced project contributors and maintainers will be the program\u0026rsquo;s mentors. Mentees will also receive a stipend during their term in the program.\n LFX Mentorship Program homepageFinding about this program was life changing\n  I found a lot of open source projects that wanted contributors. It showed the skills they were looking for and what kind of project it will be. The program gave me insight into open source projects and communities. I applied to the program, and after a couple of months of contributing to a project, I was selected as a mentee.\nI can speak a lot about how LFX helped me, but to sum it up, the program set me on the path to working in open source.\n My LFX mentee profileI applied to three projects but did not try to work on the other two. Here’s my profile\n  Start Small, Find Mentors You can always prepare before you start contributing to open source (or doing anything in general). You can skill up, read all the right books, and watch all the right tutorials.\nBut the only way to progress is to start.\nYou can start small, but you have to start somewhere. In open source contributions, you can start by being a project user and raising issues as you find them. You can hang out in community meetings and provide feedback to the project contributors. You can sign up for alpha or beta programs and help remove bugs. All of these are low-hanging fruits ready to be plucked.\nIt can also be daunting to publish your code for the whole world to see. And yes, you will write a lot of bad code when you start. But contributing to open source ensures that your code is reviewed and improved each time.\nTo help navigate the open source waters, you can find mentors. Open source mentorship programs like LFX are a great way to find mentors if you are a student or a new developer.\nFrom my experience as a maintainer and a mentor, newcomers often find bugs that seasoned contributors often miss. Sometimes we are too close to the project to see the mistakes. So, new contributors are always welcome!\nMaintainer! Maintainer! Maintainer! At one point, I spent all my time on open source projects. I contributed code, helped design new features, and managed contributors. It did not take long for my fellow maintainers to nominate me formally to their team.\n Open source contributionsI don\u0026rsquo;t see a clear distinction of the time when I switched to writing less code and being more of a maintainer though\n  As a maintainer, I spent less time writing code and more time in system design, making decisions, and managing contributors. I still looked at code, but it was mostly during reviews.\nContributions Don\u0026rsquo;t Have to be Code A lot of open source does not involve code. These non-code contributions can involve writing documentation, creating tutorials, giving talks about the project at conferences, designing content, organizing events and meetings, managing the community, and more. The list is endless.\nOpen source is the default way to build software now. And non-code contributions help create, manage, and sustain these projects.\nBeing a Maintainer is Hard Being an open source maintainer is a job that never ends. It isn\u0026rsquo;t easy to separate from your work when it is public and a part of you.\nI spent a lot of time working insane hours because the work wouldn\u0026rsquo;t stop. This took a toll on my health and personal life, and I made an effort to set boundaries.\nIt is okay to take breaks, and it is important to take breaks.\nCommunity Over Code Open source projects are a byproduct of its community.\nBuilding and sustaining communities are essential as they lead to better projects.\nSo, welcome new community members, and lower the barrier to entry. Take active measures to build and maintain the community.\nFull-time Open Source Today I work in open source full-time. I contribute to Apache APISIX and lead initiatives to grow the project and community. If I had a dream job, this would be it. I know because I used to do this job for free before someone decided to pay me for it.\nPeople helped me when I wrote bad code, when I asked stupid questions, and when I was a beginner. Today, I pay it forward by being a mentor in open source mentorship programs.\n My LFX mentor profileYou can see the transition on my profile\n  Contributing is Rewarding It is rewarding, and it\u0026rsquo;s more than the money.\nBeing an open source contributor makes you part of something bigger than yourself or your company. And your contributions can have a significant impact on the world around you.\n Speaking at Open Source Summit EuropeTalking about how open source has helped me in my career\n  So if you are thinking of contributing to open source, do it. If you know an underfunded open source project that you rely on, sponsor it. If you see an open source alternative, promote it.\n","permalink":"https://navendu.me/posts/open-source-lessons/","summary":"Insights from my three year journey as an open source contributor.","title":"Lessons Learned From Three Years of Open Source Contributions"},{"content":"I\u0026rsquo;ve been writing blogs for almost three years now. Recently, I\u0026rsquo;ve been putting a lot of effort into building and maintaining my blog. This article documents my blog setup and my writing process from idea to publishing.\nLet\u0026rsquo;s first get the nerd stuff out of the way and look at how my blog is set up.\nThe Setup: Hugo, GitHub, and Netlify I use Hugo, the static site generator, to build my blog. Hugo is fast and heavily customizable with reasonable defaults. It handles a lot of boilerplate stuff, so people like me can focus on the content.\nI\u0026rsquo;m awful at building UI from scratch, so I use a theme. I have added some custom features and changed the look slightly for it to be more me.\nI like the way the site looks. I talked to my readers about the look and feel of the site, and they seem to like the minimalist approach.\nI push this site\u0026rsquo;s code and content to a GitHub repository. This repository is open source. When adding new posts, I usually make a pull request. This is to trigger a preview build that other reviewers and I can verify and comment on.\nPreview builds? What is that? That\u0026rsquo;s where Netlify does its magic.\nI use Netlify to deploy my site. I\u0026rsquo;m on the free tier, and it seems to be enough. But I will use it even if I have to pay for it.\nNetlify watches for pushes to my main branch and triggers a deployment. It also has a deploy preview feature which builds a preview for the website when you open a pull request against the main branch.\nSo, after writing a new post, I make a pull request, and Netlify builds a preview. Reviewers can make suggestions on the PR before merging it and publishing it on the site.\n Deploy previewsDeploy previews make my life much easier. See comment\n   Reviewing draft postsI also love my diagrams! See comment\n  Netlify has many more features like DNS management and A/B testing, but I haven\u0026rsquo;t tried them till now.\nThe Tools: VS Code, Shortcodes, hbt I\u0026rsquo;m only as good as my tools. I\u0026rsquo;m much faster and write much easier with these tools.\nVisual Studio Code is my CMS. I write on the code editor, and it feels natural.\nI\u0026rsquo;ve tried different open source CMS platforms, but none seem easy to migrate to. I have a non-trivial blog setup, making it non-trivial to migrate to these platforms. And I\u0026rsquo;m lazy to put in the work to make the switch.\nBut, to make my life easier, I use some tools and configurations.\nMy site contains two types of posts; the regular blog posts shown on the homepage and daily logs. Each of these posts has a custom front matter. I have configured archetypes to create new files with these front matters easily.\nThis makes it easy to create new posts. A new file is created and configured; I just have to worry about the content.\nBut the coolest configurations I have are custom shortcodes and snippets. Hugo supports shortcodes which are placeholders for custom templates. It lets me add images, code, quotes, and even raw HTML directly into markdown files which will be appropriately formatted on render.\n Custom shortcodes in HugoThis shorcode is for creating diagrams using Mermaid\n  To use these shortcodes easily, I have configured custom snippets on VS Code to add shortcodes with appropriate tab stops automatically.\n Custom snippets in VS CodeNotice how I can just hit TAB and it jumps to the next configuration\n  But, there are limits to what I can achieve with VS Code alone. So, I built a custom CLI to create files to write content and folders to store static content. I call it hbt—Hugo Blog Tool.\n hbt—Hugo Blog ToolIt ain\u0026rsquo;t much, but it\u0026rsquo;s honest work\n  It creates new files with the correct front matter and matching folders to store images—for now. I plan to add more features to it as I need them in the future. It is written in Go.\nWith this setup and tools, I start writing.\nThe Process: Eliciting, Planning, and Writing As I mentioned before, I write two types of posts on this blog. The regular posts on the homepage and daily logs.\nFor daily logs, I keep a blank page open on my computer from the start of the day. I write on it and add things I find during the day. By the end of the day, I will have some rough content which I rewrite crudely to form a daily log.\nI want to write it every day, but I have learned to give myself some slack if I have other important things to do.\nFor my regular blogs, I follow a process:\n First, I find topics to write about. When I find one, I immediately add it to my notes. I have been doing this for a few months and have a lot of topics in hand. So, if I want to write, I go through my collection and figure out what I want to write about. Once I have a topic, I create a page for it and add points. It would just be a dump of my thoughts which I will articulate when I actually start writing the post. I return to this dump to dump more ideas as I get them. I have a calendar planned for two weeks ahead. I plan what to write about and when to publish it. Currently, I post a new article every Friday and a newsletter issue every other Friday. So, I have two weeks worth of posts and newsletters ready or almost ready. When writing, I articulate all my thoughts from the dump and try to connect ideas. Once I write the article, I rewrite it on VS Code. Yes, I type it again. This helps me find errors and review the post before publishing. Once I\u0026rsquo;m done, I open a pull request to my repo which will trigger the deploy preview, and I will share the preview with my reviewers.   Writing a lot has helped me improve my writing. It has helped me develop my style and express my thoughts.\nTurning writing into a process may seem like it hinders creativity. But it is quite the opposite. When you are in the process, you will get conditioned to write. When you write, you write. It is much easier than winging it, especially if you are starting.\nI will try to keep this post updated as my writing process and setup evolve.\n","permalink":"https://navendu.me/posts/my-blog-setup-and-writing-process/","summary":"I\u0026rsquo;ve been writing blogs for almost three years now. Recently, I\u0026rsquo;ve been putting a lot of effort into building and maintaining my blog. This article documents my blog setup and my writing process from idea to publishing.","title":"My Blog Setup and Writing Process"},{"content":"In Kubernetes, Ingress is a native object that allows you to access your services externally by defining a set of rules. Using a reverse proxy, an Ingress controller implements these defined rules and routes external traffic to your services.\n Kubernetes Ingress   Apache APISIX is an open source API gateway (a souped-up reverse proxy) that provides features like authentication, traffic routing, load balancing, canary releases, monitoring, and more. APISIX also supports custom Plugins and integrates with popular open source projects like Apache SkyWalking and Prometheus. To learn more about APISIX, you can see the official documentation.\nThe Apache APISIX Ingress controller sits between the defined Ingress rules and the APISIX API gateway. It configures the proxy to route traffic based on the defined rules.\n APISIX Ingress   This hands-on tutorial will teach you how to set up the APISIX Ingress controller on your Kubernetes cluster and route traffic to your services.\nBefore you move on, make sure you:\n Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install and configure kubectl to communicate with your cluster. Install Helm to deploy the APISIX Ingress controller.  Deploying a Sample Application We will use a sample HTTP server application (bare-minimum-api) to demonstrate the working of the Ingress controller.\nWhile running the application, you can set a \u0026ldquo;version\u0026rdquo; and a port to listen to. For this example, we will create two \u0026ldquo;versions\u0026rdquo; of this application which will return different responses as shown below:\n bare-minimum-apiWe configure the v1 API as v1 and the v2 API as v2 manually while deploying them\n  You can deploy the application on your Kubernetes cluster by running:\nkubectl run bare-minimum-api-v1 --image navendup/bare-minimum-api --port 8080 -- 8080 v1.0 kubectl expose pod bare-minimum-api-v1 --port 8080 To test the application outside the cluster, you can use port-forward:\nkubectl port-forward bare-minimum-api-v1 8080:8080 Now, if you open up a new terminal window and run:\ncurl http://127.0.0.1:8080 You will get back a response from the application:\nHello from API v1.0! Similarly, you can deploy another \u0026ldquo;version\u0026rdquo; of the application by running:\nkubectl run bare-minimum-api-v2 --image navendup/bare-minimum-api --port 8080 -- 8080 v2.0 kubectl expose pod bare-minimum-api-v2 --port 8080 Now, we can deploy APISIX Ingress and expose these applications to external traffic.\nDeploying APISIX Ingress APISIX and APISIX Ingress controller can be installed using Helm:\nhelm repo add apisix https://charts.apiseven.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update kubectl create ns ingress-apisix helm install apisix apisix/apisix \\  --set gateway.type=NodePort \\  --set ingress-controller.enabled=true \\  --namespace ingress-apisix \\  --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix kubectl get pods --namespace ingress-apisix  Note: We are using NodePort as the Gateway service type. You can also set it to LoadBalancer if your cluster has one.\n Helm will create five resources in your cluster:\n apisix-gateway: The data plane that handles external traffic. apisix-admin: Control plane that processes configuration changes. apisix-ingress-controller: The ingress controller. apisix-etcd and 5. apisix-etcd headless: To store configuration and handle internal communication.  Once all the pods and services are running, you can test APISIX by accessing the Admin API:\nkubectl exec -n ingress-apisix deploy/apisix -- curl -s http://127.0.0.1:9180/apisix/admin/routes -H \u0026#39;X-API-Key: edd1c9f034335f136f87ad84b625c8f1\u0026#39; If you get a response similar to the one shown below, APISIX is up and running:\n{ \u0026#34;action\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;node\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;/apisix/routes\u0026#34;, \u0026#34;dir\u0026#34;: true, \u0026#34;nodes\u0026#34;: [] }, \u0026#34;count\u0026#34;: 0 } Configuring APISIX Ingress Once you have verified that the APISIX gateway and Ingress controller is running, you can create Routes to expose the deployed application to external traffic.\nThis will route traffic between the two application versions based on the client request:\n Routing requests to v1Requests to the path /v1 should be routed to the bare-minimum-api-v1 service\n   Routing requests to v2Requests to the path /v2 should be routed to the bare-minimum-api-v2 service\n  To configure Routes, APISIX comes with declarative and easy-to-use custom resource:\napiVersion: apisix.apache.org/v2beta3 kind: ApisixRoute metadata: name: api-routes spec: http: - name: route-1 match: hosts: - local.navendu.me paths: - /v1 backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 - name: route-2 match: hosts: - local.navendu.me paths: - /v2 backends: - serviceName: bare-minimum-api-v2 servicePort: 8080 The APISIX Ingress controller converts this resource to an APISIX gateway configuration.\nAPISIX also supports configuration using native Kubernetes Ingress resource:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api-v1 port: number: 8080 path: /v1 pathType: Exact - backend: service: name: bare-minimum-api-v2 port: number: 8080 path: /v2 pathType: Exact You can use either to configure APISIX but I prefer the easier APISIX custom resource. We can apply this manifest file to our cluster to create Routes in APISIX:\nkubectl apply -f apisix-ingress-manifest.yaml If the Ingress controller is configured correctly, you should see a response indicating that APISIX API gateway has been configured:\napisixroute.apisix.apache.org/api-routes created Now, let\u0026rsquo;s test these Routes.\nTesting the Created Routes If you were following along using minikube and NodePort, you should be able to access APISIX through the Node IP of the service apisix-gateway. If the Node IP is not reachable directly (if you are on Darwin, Windows, or WSL), you can create a tunnel to access the service on your machine:\nminikube service apisix-gateway --url -n ingress-apisix This will show the URL with which you can access the apisix-gateway service.\nYou can send a GET request to this URL and it would be Routed to the appropriate service:\ncurl http://127.0.0.1:51538/v2 -H \u0026#39;host:local.navendu.me\u0026#39; Hello from API v2.0! Now you have APISIX routing traffic to your applications! You can try the two configured Routes and see APISIX routing the requests to the appropriate application.\nWhat\u0026rsquo;s Next? In this tutorial, you learned to set up APISIX Ingress on your cluster. We tested it out by configuring basic Routes to a sample application.\nWith APISIX gateway and the Ingress controller, you can also configure Upstreams, Plugins, mTLS, and monitoring. To learn more about APISIX and how you can use these features, visit apisix.apache.org.\nSee the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress\u0026rdquo;.\n","permalink":"https://navendu.me/posts/hands-on-set-up-ingress-on-kubernetes-with-apache-apisix-ingress-controller/","summary":"A tutorial on using Ingress in your Kubernetes cluster with Apache APISIX.","title":"Set Up Ingress on Kubernetes With Apache APISIX Ingress Controller"},{"content":"Contributing to open source is nerve-wracking, especially if you are a beginner.\nA couple of years ago, I was this nervous beginner. I had no work experience as a software engineer, but I was determined to learn by contributing to open source projects.\nBut which project do I contribute to? Where do I start? What if I\u0026rsquo;m not skilled enough?\nAs I sat puzzled by these questions, I stumbled across the Linux Foundation Mentorship Program.\nThe LFX Mentorship Program provides opportunities for mentees to work on open source projects. Experienced project contributors and maintainers will be the program\u0026rsquo;s mentors. Mentees will also receive a stipend during their term in the program.\nI got selected as a mentee, received a full-time offer after the program, and returned to the program as a mentor.\nToday, I work in open source full-time.\nIn this article, I will share how I got involved in the LFX Mentorship Program and its life-changing impact on my career and personal life.\nStumble Beginnings After learning about the program on YouTube, I landed on the LFX Mentorship homepage. There, I found the answers to my questions.\nI found the list of open source projects participating in the program. It showed me what the project is about, what skills they expect mentees to have, and everything else I need to get started.\nI got interested in the Meshery project and joined its community. There were many helpful people there, and I started making small contributions. In a couple of months, I was learning new skills and making more impactful contributions.\nI applied to be an LFX mentee for the project and was accepted. I was already contributing to the project, and being part of the program was a bonus.\n My LFX mentee profileI applied to three projects but did not try to work on the other two. Here\u0026rsquo;s my profile\n  Levelling Up During the program, I developed skills to work on cloud native technologies. I learned to program in Go, built applications on Docker and Kubernetes and learned more about the ecosystem.\nI also improved my communication skills by working in a global, async team. I was writing design specifications, user-facing documentation, and blog posts.\nThe program also opened a lot of opportunities to speak at international conferences, work with maintainers from top tech companies, and interact with other open source communities.\nI gained the confidence to contribute to open source projects. Maybe it can be my career!\nFull-Time Open Sourcerer Making me realize that contributing to open source projects can be a career option was the most significant impact the LFX Mentorship Program had on me.\nAfter the program, I received a job offer to work on Meshery and related projects.\n🧙‍♂️ Getting paid to work on open source projects was (and is) the best. To this date, I don\u0026rsquo;t feel like I\u0026rsquo;m working at all. I\u0026rsquo;m doing things I like to do, and apparently, someone thinks it\u0026rsquo;s worth paying me for it.\nPaying it Forward The open source community is about paying it forward.\nWhen I started as an open source contributor, a lot of people helped me. They answered my noob questions, corrected mistakes in my code, and guided me through projects and communities.\nSo, after being promoted to an open source maintainer, I joined back in the LFX Mentorship Program. This time, as a mentor.\n My LFX mentor profileYou can see the transition on my profile\n  I learned a lot by being a mentor. I developed skills as a leader, and it opened up more career opportunities.\nLFX, the Catalyst The effect programs like LFX have on the open source ecosystem is monumental. As open source becomes the default way to build software, these programs help foster contributions from an ever-growing set of newcomers. And contributions from these newcomers ensure sustainability.\nOn the surface, the LFX Mentorship Program can look like any other internship program. But for someone without access to such opportunities because of where they are in the world, the program is highly impactful.\nAnd based on the stats, more people are applying to the program and actively participating in open source. This, in turn, drives organizations to make more of their code free and open source.\nSo, apply to the program if you are interested in contributing to open source, especially if you are a student. If you are a seasoned contributor, participate as a mentor and pay it forward.\nYou can also check out other programs like LFX in 20\u0026#43; Open Source Internship Programs that you can Apply to.\n","permalink":"https://navendu.me/posts/how-the-lfx-mentorship-program-helped-me-level-up-my-career/","summary":"Sharing how I got involved in the LFX Mentorship Program and its life-changing impact on my career and personal life.","title":"How the LFX Mentorship Program Helped Me Level Up My Career"},{"content":"I have been using diagrams.net and Excalidraw for all my diagram needs. But I often had to modify these diagrams when I made a mistake or when they were outdated. Maintaining these diagrams consumed a lot of my time and effort. There has to be a better way, right?\nI was writing a new blog post with many diagrams when I googled \u0026ldquo;diagrams in markdown\u0026rdquo; and found Mermaid.\nMermaid lets you create diagrams by writing text in markdown files. It has an intuitive syntax and verbose documentation to help you make diagrams quickly.\nIt is also easy to add Mermaid to your Hugo blog. You can use shortcodes to add your diagrams in Mermaid syntax and have them rendered on your blog posts.\nflowchart TB y(\u0026quot;👫 You\u0026quot;) --\u0026gt; s{Subscribed to navendu.me?} s --\u0026gt; |Yes| a(\u0026quot;🥳 You are awesome!\u0026quot;) s --\u0026gt; |No| su[/\u0026quot;Subscribe to navendu.me\u0026quot;/] --\u0026gt; a click su href \u0026quot;/subscribe\u0026quot; _blank flowchart TB y(\"👫 You\") -- s{Subscribed to navendu.me?} s -- |Yes| a(\"🥳 You are awesome!\") s -- |No| su[/\"Subscribe to navendu.me\"/] -- a click su href \"/subscribe\" _blank  Check out the official documentation to learn more about Mermaid and its syntax.\nThis article will look at how you can add Mermaid to your Hugo blog.\nImport and Configure Mermaid The first step is to import Mermaid. The simplest way to do this is by importing the minified file from a CDN.\nCreate a file mermaid.html in your layouts/partials directory. We will import and configure Mermaid here.\n\u0026lt;script type=\u0026#34;application/javascript\u0026#34; src=\u0026#34;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\u0026#34; \u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var config = { startOnLoad: true, theme:\u0026#39;{{ if site.Params.mermaid.theme }}{{ site.Params.mermaid.theme }}{{ else }}dark{{ end }}\u0026#39;, align:\u0026#39;{{ if site.Params.mermaid.align }}{{ site.Params.mermaid.align }}{{ else }}center{{ end }}\u0026#39;,  }; mermaid.initialize(config); \u0026lt;/script\u0026gt; The highlighted line looks for parameters in your site\u0026rsquo;s configuration file (config.yaml). This makes it easy to change Mermaid\u0026rsquo;s theme and alignment.\nYou can look for more configuration options in Mermaid docs. I only have these two configurations now and can add them to my site\u0026rsquo;s configuration file.\nparams: mermaid: theme: \u0026#34;dark\u0026#34; align: \u0026#34;center\u0026#34; Loading Mermaid on Required Pages It is unnecessary to load Mermaid on every page in your blog. You only need it on pages where there are Mermaid diagrams.\nWe will only add the layouts/partials/mermaid.html file when a page needs it. To set this up, we will create a front matter variable mermaid, which can be set to true to load Mermaid.\nNow, in your page layout, you can check if this variable is set and load the mermaid.html partial file. My page layout file is layouts/_default/single.html.\n\u0026lt;div class=\u0026#34;post-content\u0026#34;\u0026gt; {{- if not (.Param \u0026#34;disableAnchoredHeadings\u0026#34;) }} {{- partial \u0026#34;anchored_headings.html\u0026#34; .Content -}} {{- else }}{{ .Content }}{{ end }} \u0026lt;!-- Add mermaid min js file --\u0026gt; {{ if (.Params.mermaid) }} {{ partial \u0026#34;mermaid.html\u0026#34; }} {{ end }}  {{ partial \u0026#34;subscribe.html\u0026#34; . }} \u0026lt;/div\u0026gt; You can now import Mermaid by setting the mermaid page variable to true in your markdown files as shown below:\n--- title: \u0026#34;Adding Diagrams to Your Hugo Blog With Mermaid\u0026#34; date: 2022-08-26T15:46:16+05:30 draft: true weight: 14 ShowToc: true TocOpen: true mermaid: true summary: \u0026#34;This article shows how you can add diagrams to your Hugo site with Mermaid.\u0026#34; tags: [\u0026#34;hugo\u0026#34;, \u0026#34;mermaid\u0026#34;, \u0026#34;blogs\u0026#34;, \u0026#34;tutorials\u0026#34;] Creating the mermaid Shortcode The final step is to create a shortcode to add diagrams in Mermaid syntax to your markdown files.\nCreate a new file layouts/shortcodes/mermaid.html with the below content:\n\u0026lt;div class=\u0026#34;mermaid\u0026#34;\u0026gt;{{.Inner}}\u0026lt;/div\u0026gt; That\u0026rsquo;s it! Now to test it out, you can try using the shortcode as shown below:\n{{\u0026lt; mermaid \u0026gt;}} flowchart LR y(\u0026quot;👫 You\u0026quot;) --\u0026gt; h{\u0026quot;🤝 Found this helpful?\u0026quot;} h --\u0026gt; |Yes| r[/\u0026quot;⭐ Check out my featured posts!\u0026quot;/] h --\u0026gt; |No| su[/\u0026quot;📝 Suggest changes by clicking near the title\u0026quot;/] click r \u0026quot;/categories/featured\u0026quot; _blank {{\u0026lt; /mermaid \u0026gt;}} After you save and build your site, you should be able to see this diagram rendered:\nflowchart LR y(\"👫 You\") -- h{\"🤝 Found this helpful?\"} h -- |Yes| r[/\"⭐ Check out my featured posts!\"/] h -- |No| su[/\"📝 Suggest changes by clicking near the title\"/] click r \"/categories/featured\" _blank  Optional: Add CSS to Style the Diagrams You can optionally add custom CSS to the mermaid class to change how your diagram looks.\nYou can add this CSS by creating a new file, assets/css/mermaid.css. My CSS file has the following content to make my diagrams look better on my posts:\n.mermaid { display: flex; justify-content: center; margin: 10px 0px 25px 0px }  Mermaid is a free and open source project maintained by the community. You can sponsor the project on GitHub to support its development.\n","permalink":"https://navendu.me/posts/adding-diagrams-to-your-hugo-blog-with-mermaid/","summary":"This article shows how you can add diagrams to your Hugo site with Mermaid.","title":"Adding Diagrams to Your Hugo Blog With Mermaid"},{"content":"This blog is now deployed on Netlify! I spent a little more than an hour yesterday migrating it from GitHub Pages to Netlify.\nGitHub Pages is a perfect solution for deploying static websites. But, it made it challenging to implement some of the features I wanted on my blog.\nSo, on impulse and to procrastinate from finishing a blog post, I migrated the site to Netlify!\nWhat GitHub Pages Lacked GitHub Pages has been my go-to static website deployment solution for the past three years. And it worked like a charm even if it was free.\nI have been looking for ways to set up my Hugo website to show draft blog posts. The problem was that I didn\u0026rsquo;t want these drafts to show up on the homepage listing, but I also wanted a sharable link for people to review.\nThere are ways I can set this up, but even if I do that, there aren\u0026rsquo;t any ways for people/reviewers to leave feedback on my static website.\nI also used a lot of client-side redirects, which is not desirable as opposed to server-side redirects. GitHub Pages did not provide a way for you to configure server-side redirects. So I had to use these hacky, Jekyll redirects on a subdomain. It works, but it could be better.\nEnter Netlify Netlify has deploy previews. So, when you make a pull request to your production branch, Netlify will build the site for you and show a preview of what the change will look like.\n Deploy preview feature in NetlifyFrom github.com/navendu-pottekkat\n  You know what this is best for? Previewing draft blog posts!\nUsing Netlify, I can open a pull request with my draft post, and Netlify will generate a preview build of the site without affecting my production site. Reviewers can see the preview and suggest edits as comments on GitHub.\nNetlify also brings a better continuous integration experience.\n CI checks run by NetlifyFrom github.com/navendu-pottekkat\n  Netlify lets you configure the build settings through the Netlify UI or a configuration file (netlify.toml). This gives you a lot more control than the few configuration options GitHub Pages provides.\n Netlify dashboard for navendu.meI added a Plugin and to lint links and it broke the CI because there were a lot of broken links. Task for another day.\n  With Netlify, configuring redirects is as easy as adding two lines to your configuration file. You also have the option to configure the proper status code. Now that\u0026rsquo;s neat.\nNetlify also has a DNS service, supports storing large media (Git LFS), has split testing and rollback features, and more. But, these are only \u0026ldquo;nice to have\u0026rdquo; features for me now. I will not be using these anytime soon*.\n * I might use these sometime soon.\n How I Migrated to Netlify The process was pretty straightforward. It only took me a little over an hour to set everything up and test.\nIn the steps below, I have the following setup:\n A Hugo blog published on GitHub Pages. A custom domain registered on Hostgator (also my DNS service).  You can always refer to the official Netlify docs if you have a different setup somewhere along this guide.\nCreate a Netlify Account First, you must create a Netlify account if you don\u0026rsquo;t have one.\n Create your Netlify accountDo you also use GitHub for everything?\n  Import Your Website You can now add your website to Netlify. You should have the code for your website on a Git provider. If you don\u0026rsquo;t have one, now is the time to git push your code.\n  Click on \u0026ldquo;Add new site\u0026rdquo; and \u0026ldquo;Import project\u0026rdquo;.\n Importing your site     Select the Git provider where you have your website code.\n Select your provider     Pick the repository from the Git provider after granting access to Netlify.\n Pick the repository to deploy     You can then configure the build settings based on your blog engine. Since I\u0026rsquo;m using Hugo, I will add the following configurations.\n Build settings for my Hugo website--gc cleans up old resources, --minify reduces the size of the files and the public folder is where Hugo outputs the build\n    And voila! Netlify will automatically build your first deployment. Now, you will be able to see the production URL for your website.\n Production URL of your websiteThis screenshot is from Netlify\u0026rsquo;s docs\n  This URL is unique; you can change it to yoursitename.netlify.app or your custom domain.\nChanging the Website URL and Custom Domains You can change your site URL to anything unique. Once changed, this will be your website\u0026rsquo;s address.\n  From the \u0026ldquo;Site overview\u0026rdquo; page, Go to \u0026ldquo;Domain settings\u0026rdquo;.\n  Click on the \u0026ldquo;Options\u0026rdquo; next to the site name and click \u0026ldquo;Edit site name\u0026rdquo;.\n Edit the generated site namekung-fu-panda-23 is a cool name\n    Change the site name and save.\n   Note: If you use this URL, you might need to change the baseURL in your Hugo configuration file to ensure all the links work.\n In my setup, I\u0026rsquo;m using Hostgator as my DNS service and not the Netlify DNS service.\n  In \u0026ldquo;Domain settings\u0026rdquo;, click \u0026ldquo;Add custom domain\u0026rdquo;.\n  You can enter the domain you already own or enter something new and purchase the domain. Netlify will set everything up if you are buying a domain.\n  If you are using a domain you already own with an external DNS provider, you will see a warning. You can ignore that and click on \u0026ldquo;Add domain\u0026rdquo;.\n  You can also set up an SSL certificate from the HTTPS section.\n Secure your site with an SSL certificate     If you use a different DNS service, you need to configure it to point your domain to your Netlify website. You can check the Netlify docs if you have a different setup than mine.\n Login to your domain\u0026rsquo;s control panel and open up your DNS configuration. I\u0026rsquo;m using Hostgator for my domain. Create an A Record with your apex domain and point it to Netlify\u0026rsquo;s load balancer IP address 75.2.60.5. Then, create a CNAME record for the wwwsubdomain and point it to your website address yoursitename.netlify.app.  Now, you must wait for the changes to propagate, and you will have your domain configured.\nConfiguring Deploy Previews The main reason to migrate was the deploy previews feature. And it is super easy to set up.\n Go to \u0026ldquo;Site settings\u0026rdquo;. Select \u0026ldquo;Build \u0026amp; deploy\u0026rdquo; from the side menu and then \u0026ldquo;Continuous Deployment\u0026rdquo;. Scroll down to \u0026ldquo;Deploy previews\u0026rdquo; and set it up as desired. I have enabled it for any pull request against my production branch, and I have also enabled the Netlify Drawer.  That is it. You now have deploy previews!\n Note: To ensure that the deploy previews show drafts, I updated my Netlify configuration file (netlify.toml) to change the build command for deploy previews.\n[context.deploy-preview] command = \u0026#34;hugo --buildFuture --buildDrafts --gc --minify -b $DEPLOY_PRIME_URL\u0026#34; Here $DEPLOY_PRIME_URL is an environment variable that Netlify sets, used to update the site\u0026rsquo;s baseURL.\n That brings an end to my current setup. It is much better than my earlier setup with GitHub Pages and was pretty easy to migrate. I would definitely recommend Netlify for your static websites.\nFeatures I Might Add in the Future This was my first iteration with Netlify. I have a basic setup that more or less does everything I need. But, I might use these other features if they are fruitful.\nForm Handling I use Mailchimp to handle subscriptions to this blog. If I can find a way to send mass emails, I might set up the free form handling offered by Netlify.\nSplit Testing A/B test blog posts? Yes!\nCDN/git LFS My blog contains a lot of images. I do my best to compress these images, but I will reach a point where the images are taking too much space, increasing the repo size. If Netlify\u0026rsquo;s solution is better, I might switch to that.\nToo Good to be Free? Netlify seems too good to be free. I\u0026rsquo;m on the free tier, and it appears to be generous.\n My 24 hour Netlify usagePetition to add a section that shows the carbon footprint on my blog builds. Take that, people flying on private jets!\n  But, it will only be some time until I pass these limits and would end up needing to pay for the service. It is not too much money but seeing that the alternative, GitHub Pages, is free, I cannot stop thinking, \u0026ldquo;maybe I don\u0026rsquo;t need deploy previews\u0026rdquo;.\nI hope I don\u0026rsquo;t have to write a post titled \u0026ldquo;How and Why I Migrated My Blog Back From Netlify to GitHub Pages\u0026rdquo;!\n","permalink":"https://navendu.me/posts/how-and-why-i-migrated-my-blog-from-github-pages-to-netlify/","summary":"This blog is now deployed on Netlify. Here is how and why I did it.","title":"How and Why I Migrated My Blog From Github Pages to Netlify"},{"content":"I have been blogging ever since I started my career as a software engineer.\nI literally had zero professional experience then so I chose to write as I learn. I documented what I learned and shared my experience as I went.\nLooking back at this 2.5 years later, writing blogs has helped me develop my skills, build an audience and advance my career.\nHere are my top reasons why you should start writing blogs today.\nYou Get Better Technically Writing helps to reinforce what you learned.\nWhen you are learning a new technology or solving a very specific bug that dives deep into the language, you should write it down. Write down what you learned or how you fixed the bug. This document can then work as a reference for you in the future and help others in your same path.\nBut this process helps you even more by forcing you to articulate what you have learned, helping you learn and understand the concept better.\nYou Get Better at Communicating Communication is one of the best skills you can have as a developer. It is a much needed skill even if you are not writing technically in your job.\nA lot of what software engineering is requires you ot work in teams where you have to have good communication to get the best results.\nWriting forces you to communicate well. It forces you to think clearly, formulate your thoughts and communicate the thoughts with others.\nIf there is a feedback loop where you can get comments from your readers, use that feedback to improve your communication.\nYou are Helping Others There are always people who can benefit from your content.\nThis is regardless of where you are in your career. Even though I had very little professional experience, sharing what I\u0026rsquo;ve learned and my experience has helped people trying the same path.\nCreate content and there will be readers.\nYou are Building Your Personal Brand Welcome to 2022 where it is all about your personal brand.\nBuilding a personal brand can help you in many ways. Creating content on the internet and putting your name on it is a solid step towards this.\nWhat you write will be associated to your personal brand. And personal brands helps you build personal monopolies.\nYou are Setting Up Your Career When you write online you are building your professional network.\nThis network of people who share similar interests with you can help by providing opportunities that you might have not received otherwise. They already have a picture of you and your skills form the content you write.\n It is easy once you start writing.\nYou will find more reasons why you want to continue writing blog posts and you will form a habit.\nThe important step is to just start. Don\u0026rsquo;t wait till you are \u0026ldquo;good enough\u0026rdquo; or \u0026ldquo;experienced\u0026rdquo; to start writing. Things will fall in their places once you start.\n","permalink":"https://navendu.me/posts/why-developers-should-blog/","summary":"I have been blogging ever since I started out as a software engineer. Here is why you should do it too.","title":"Why Developers Should Blog"},{"content":"  Money’s greatest intrinsic value—and this can’t be overstated—is its ability to give you control over your time.\n Morgan Housel  The Psychology of Money    These are my key takeaways from Morgan Housel\u0026rsquo;s international bestseller “The Psychology of Money”.\nI have kept my learnings to be short and skimmable and I would urge everyone reading this article to read the entire book.\n The Psychology of MoneyPhoto by Morgan Housel on Unsplash\n  The book is only about 240 pages long and it is a fast read. I read it cover to cover in two sittings—it was a real page turner.\n Housel begins by talking about how people’s financial decisions are less mathematics, graphs and spreadsheets and are more influenced by their personal history, world view, ego, pride and odd incentives.\nHe drives this point throughout the book through 19 stories and teaches how you can make better financial decisions.\nThese are my notes—my key takeaways—on what each of these stories teach us.\n Everybody’s approach to money is different and is shaped by their own beliefs and experiences from different societies, cultures and economic conditions. The role of luck is often unaccounted. You can end up in the wrong side by making good financial decisions and vice versa. People at the top may have benefitted from luck while those at the bottom might have been the casualties of risk.  Be careful who you praise and admire. Be careful who you look down upon and wish to avoid becoming. To ease this risk, replicate broader patterns instead of trying to replicate specific individuals.   There is no reason to risk what you have and need for what you don’t have and don’t need. Your goals are never complete if you start comparing yourself with others. Keep your goalpost fixed. The role of time and compounding is important in building wealth. Ask Warren Buffet. Building wealth is easier compared to staying wealthy—it is a different game. Be optimistic that things will work out in the long term even while facing adversity in the short term. Raise your odds of success at a given level of risk. Failures are okay. Long list of failures are overcome by a small list of huge successes. Investment firms have huge portfolios and how many of them actually work out and make money? What wealth gives you is the control of time. This gives you control over doing what you want, when you want and with who you want. This is the most common variable that makes people happy. No one is impressed by your possessions as much as you are. People admire your possessions and not you. Trying to buy admiration and respect is a wild goose chase. Spending money to show you have money is the fastest way to loose money. Wealth is the money you don’t spend. It is what you don’t see. It is the ability to do/get things—time, freedom, possessions— in the future. You don’t need a reason to save money. You save money because you save money. With compounding, building wealth is more about savings rate than income (over a certain level of income). Investing strategies need not always be rational. Making reasonable choices like investing in your own country, investing in stocks you like and investing in family can give advantages. The best portfolio is the one that gives you peaceful sleep at night. Investing is not a hard science to work based on historical evidence. Investors are human and they are unpredictable.  Experiencing specific events doesn’t necessarily qualify you to know what would happen next. The events that greatly affected the economy could be outliers. Surprises are surprising.   Leave room for errors in your decision making. Think about what errors your can financially endure as well what you can emotionally endure. Your financial goals, your career goals, your life goals, change. Accounting for the unknown is hard and it is okay. The hidden price with investing is the ups and downs in the market, the uncertainty and the doubt in your mind. Be prepared to pay the price. Nothing is free. People are different and have different goals. Don’t be persuaded by the actions of people who are playing a different game than you. Understand your goals and your time horizon. Pessimism sounds smarter and people are drawn to its reasoning. But, the problems pessimists forecasts are solved by clever and novel solutions leading to optimistic futures. The more you want something to be true, the more likely you are to believe a story that overestimates the odds of it being true. These “appealing fictions” have significant impact on our investments.   This book is a must read if you are serious about investing, building wealth and being financially independent.\nThe book costs $16 and ₹269 on Amazon.\n","permalink":"https://navendu.me/posts/the-psychology-of-money/","summary":"Key takeaways from Morgan Housel’s international bestseller “The Psychology of Money”—brief and to the point.","title":"The Psychology of Money"},{"content":"Contributing to open source is a great way to hone your skills while working on real-world projects. But, for new developers, this process can seem daunting.\nAfter all, when you create a pull request, you open up your code for people to give feedback and criticize.\nThis is where open source mentorship programs like Google Summer of Code (GSoC) come in and bridge the gap between new developers and large open-source codebases.\nWhat is Google Summer of Code (GSoC)? GSoC is an annual program from Google focusing on bringing students and new developers into open source.\nEach summer, participating mentees work on open source projects from the hundreds of organizations participating in GSoC. The participating mentees are paired with mentors from the participating organizations and are paid a stipend during their mentorship period.\nHere is a quick summary of the program before we dive deep into the details.\n   Duration Timeline Stipend Eligibility     10-22 weeks January 2023 - November 2023 $1500 - $6600 Anyone about 18 years of age    Why Should I Participate in GSoC? According to Linux Foundation\u0026#39;s 2021 Open Source Jobs Report, 92% of employers say finding people with open source skills is difficult.\nOpen source skills are in demand, and there is a lack of supply.\nParticipating in programs like GSoC helps you:\n Gain real-world experience Build your network Boost your career Get a stable stipend  The goal of GSoC is to enable mentees to be good open source stewards. Completing GSoC can open doors for you and could even lead to a career in open source.\nAm I Eligible to Participate? Since 2022, everyone over the age of 18 is eligible to participate.\nYou should also have a strong desire to contribute to open source and help the community!\nWhen does GSoC Start? Mentoring organizations can submit their applications till Febrary 7th.\nMentees can start applying from March 20th till April 4th.\nSee the 2022 timeline for GSoC for other details.\nHow Long is the Program? GSoC provides flexibility and allows projects to extend the program from 12 weeks up to 22 weeks.\nSee the dates of each program milestones for a better idea.\nHow do I Apply? You can apply on the GSoC website when the application period starts.\nThe application process is mainly these 3 steps:\n Selecting an organization Selecting a project Preparing a project proposal  How do I Select an Organization? GSoC has hundreds of participating organizations each year. Mentees can apply to any of these organizations, but filtering through them and deciding which to contribute to can be difficult without a proper plan.\nSo, to be in the best position when the application process starts, you have to be prepared.\nYou can find organizations to contribute to even before they are officially announced. I suggest following the steps below:\n Write down what skills/ tech stack you know currently Write down the skills/ tech stack you want to learn Filter organizations based on these two criteria Look through the projects in the organization to find what interests you List down multiple projects across organizations that fit these criteria  How do I Select a Project? You would have narrowed it down to a few projects under the same or different organizations. How do you go from here? How do you know which project to submit a proposal to?\nThe key here is homework. You need to learn more about the projects.\nHere\u0026rsquo;s what I suggest doing:\n Start by using the project and see how it works (Is it cool?) Review the project’s documentation and contributing guidelines (Are there well-documented processes?) Join the project\u0026rsquo;s community channel (Slack, Discord, mailing lists) (Are they active and welcoming?) Pick up a \u0026ldquo;good first issue\u0026rdquo; and try to fix it (Is it doable? Do you get support from the mentors/maintainers?) Ask clarifying questions about the project on the communication channels (Do you get answers?)  If the answer to all these questions is a solid “yes,” you need to ask yourself the golden question.\n“Do I want to contribute to this project?”.\nHow do I Submit a Proposal? Now that you have a project(s) chosen, it is time for the most significant step yet. Submitting a proposal.\nLike other open source mentorship programs, GSoC can also get competitive. You have to make your proposal stand out to get accepted.\nGSoC\u0026rsquo;s official guides outline what is expected in a proposal and show some examples of good proposals.\nYou must communicate with your potential mentors before submitting a proposal to make sure you have the right idea. Clarify the deliverables and establish a realistic deadline before you start writing the proposal.\nIf you already have prior contributions before you submit the proposal, it would put your application in front.\nHow will I Get Selected? Once you submit the proposal, the mentors will review each submission and let you know if you are selected.\nIf you get selected, it\u0026rsquo;s time to celebrate!\nWhat if I don\u0026rsquo;t get Selected? Don\u0026rsquo;t stop contributing!\nGSoC is highly competitive, and it isn\u0026rsquo;t easy to get selected. But you don\u0026rsquo;t have to be a GSoC mentee to contribute to open source. You can do it independently!\nThe spirit of GSoC is open source and open collaboration. If you keep contributing, you will learn a lot and have a head start for the next year\u0026rsquo;s program.\nIf your project participates in any other open source internship programs, you can apply for it.\nI\u0026rsquo;m in GSoC! The first three weeks of the program will be a community bonding period. You can use this time to get a feel of the community before engaging.\nYou will get to code in the following weeks. GSoC will provide a part of the stipend once you complete one of the two evaluations.\nAnd as mentioned before, the timeline for 2023 is flexible.\nClosing Notes GSoC is a great program. It has and will continue to help attract students and new developers to work in open source.\nThe goal of this program is not to make some money (money is necessary) but to foster open source software.\nThe impact such programs have on the community is incredible. And as GSoC mentees, you inherit the responsibility to pay it forward.\nSo, don\u0026rsquo;t stop contributing after GSoC ends. Be a mentor next time. Help others in the community.\nBest of luck with your GSoC application!\n","permalink":"https://navendu.me/posts/everything-about-gsoc/","summary":"Google Summer of Code (GSoC) is a great way to start contributing to open source while getting paid to do so. This article dives deep into everything GSoC from the application procedure to tips from real experience on being successful","title":"Everything you Need to Know About Google Summer of Code (GSoC): Important Dates, Eligibility, Application, Getting Selected and Other Tips"},{"content":"Contributing to the documentation is an impactful way to contribute to open source projects.\nWithout clear and concise documentation, people won\u0026rsquo;t be able to understand and use the project.\nIn this article, I will share how you can contribute to documentation effectively.\nFinding a Project How do you find a project to contribute to? I have a two-step plan:\nStep 1: The best project to contribute to is the one you have been a user of for a while.\nIf you have been using a framework, library, tool, or any open source project, you can contribute to it.\nYou will already have a lot of context on the project, and you will be able to find ways to improve the documentation.\nStep 2: If you can\u0026rsquo;t find a project like the one mentioned in step 1, contribute to a project with an active community.\nOpen source is as much about community as it is about code. Having access to a thriving community will help you be an impactful contributor.\nUltimately, all this boils down to your interest. You should only contribute if you are interested in the project.\nBe a User A user of a project can be an impactful contributor.\nIf you aren\u0026rsquo;t a user of the project already, becoming one would be the first step.\nAs a user, you are likely to go through documentation and tutorials as you start out, and you are likely to find issues, outdated content, or areas to improve.\nWhen you find these, you can open issues and discuss them with project maintainers.\nMost project maintainers are too close to the project to see the gaps in the documentation. A new user could find these gaps.\nRead the Contributor Guidelines Open source projects usually have a contributor guide (CONTRIBUTING.md in the repository) outlining how to contribute to the project.\nIf it has guidelines on contributing to documentation (style guides, setup guides), follow them.\nIssues to Look For When you are looking through the documentation, look for these issues:\n Missing documentation: While following the documentation, you cannot understand how it got from step A to B. There seems to be a missing step that is not obvious. You can report, identify, and add the missing step. Outdated content/code/images: Software documentation can get outdated quickly with new releases and breaking changes. Keeping an eye out for outdated content is crucial. Translations: Some projects have documentation in multiple languages. If you know multiple languages, you can check to ensure parity between the two versions. Grammar: If you find pages with incorrect language, you can correct it as it might alter the meaning of the text.  If you don\u0026rsquo;t find any issues, you can always look for open issues in the project. Documentation issues are usually tagged with \u0026ldquo;documentation\u0026rdquo; or \u0026ldquo;docs\u0026rdquo; labels.\nIn-Code Documentation In-code documentation includes logs, error messages, help texts, and other textual interfaces with a user that doesn\u0026rsquo;t necessarily affect the \u0026ldquo;logic\u0026rdquo; of the code.\nPeople generally don\u0026rsquo;t think of this when they think of documentation. Still, it is imperative as it is the first user interaction point.\nBeing a new user will also help you spot these issues quickly.\nBeyond Documentation Good documentation is not limited to the docs website.\nIt can also involve:\n Writing blog posts that take a user through a new feature Writing a Twitter thread about the project Documenting processes that community members can use (like a contributor guide) Updating the project website  You can always ask project maintainers to help you find areas they need help.\nThere are many more ways to contribute to an open source project besides writing code and documentation. And all these contributions are impactful.\n","permalink":"https://navendu.me/posts/contributing-to-documentation/","summary":"Contributing to documentation is really impactful for an open-source project. It can also be a stepping stone to make code contributions. Learn how.","title":"Contributing to Documentation"},{"content":"When thinking about open source contributions, most people think about contributing code. This was true in the early days when open source was a way for people to share the cool thing they built.\nNow, as open source becomes the default way to build software, contributions required to create, manage, and sustain projects go beyond code.\nIn this article, you will learn how to make impactful contributions to open source that does not involve contributing code.\nWriting This might be an excellent way to contribute if you are good at technical writing and creating content.\nYou can use your writing skills in different ways.\nDocumentation People can\u0026rsquo;t use what they don\u0026rsquo;t understand.\nEven if the project is valuable, people would not be able to use it without proper documentation. Contributing to documentation can therefore be impactful.\n Tip: When you first test out a project, go through the existing documentation and see if you can clearly understand and use the project without any issues.\n Chances are you will find areas to improve. You can then open issues for it and fix them.\nIt\u0026rsquo;s always helpful to have new people go through the documentation. It will uncover missing information and issues that people close to the project might miss.\nArticles and Tutorials If you are a project user, you can help by writing articles and tutorials. This can help people who are trying to use the project.\nYou can add your insights and tips and publish them on your channels or the project\u0026rsquo;s blog.\n Tip: Most people look for well-written and up-to-date tutorials instead of documentation.\n Internationalization/Translation The language barrier is real in countries where English is not a primary language. People in China may prefer Mandarin over English.\nFor people from different parts of the world to use a project, it is necessary to internationalize the software and its documentation.\nIf you can write in two languages, you can translate documentation to open up the project to a new user base.\nSocial Media A social media presence can help a project get more users. It can also be a platform to share updates with existing users.\nYou can use your writing skills to craft social media posts. You can ask the project maintainers to post on their official account or use your own account.\nDesigning You can also contribute to open source projects with your design skills.\nArt Create artwork for social media, blog posts, and even swags. Good design is always impactful.\nStyle Guide Most open source projects have a large and geographically distributed set of contributors.\nAmong other challenges, this makes it difficult to maintain consistency in visual designs.\nAs a designer, you can create a style guide for the project and ensure consistency.\nTesting/Using You can make valuable contributions in multiple ways as an open source user.\nReporting Bugs Be a needy user. If you run into bugs, raise them. Open an issue with all the relevant details and steps to reproduce it.\nIdentifying bugs is more difficult than fixing them. Maintainers always welcome bug reports.\nAdvocating Open source projects are community driven. And most non-commercial open source projects don\u0026rsquo;t have a dedicated marketing team to publicize the project.\nYou can advocate for the project at events and on social media and encourage people to use the project.\n Tip: Don\u0026rsquo;t even ask the project owners before advocating for the project. Everyone likes free marketing.\n User Experience Project maintainers can often be too close to the project to realize bad and unintuitive UX. I am, and I know a lot of them.\nAs a user, you can report these issues, and they are well received.\nAlpha/Beta Testing Alpha/Beta tests are controlled tests of a new feature or release to ensure quality and user experience before making it available to the general user base.\nAs a user, you can sign up for the alpha/beta programs, test the project before the new features are released, and provide feedback.\nFeedback from these tests always provides insights that can help improve the features/releases.\nCommunity Managing An open source project is a byproduct of its community. But who builds and manages these communities?\nThat\u0026rsquo;s where a community manager comes in. As a community manager, you can wear different hats.\nProject Organiser Stale issues? Follow up! Issues without proper labels? Add them! Does this issue still exist? Verify and close them! Unclear issue descriptions? Ask for clarification! Unreviewed pull requests? Request for reviews!\nAll these help the project run smoothly.\nRelease Manager A release manager keeps track of what everybody is working on and ensures that a project is ready for a release.\nSome of the responsibilities of a release manager are:\n Checking with each team Ensuring different components and features are tested Organizing the alpha/beta programs  Event Organizer Organize community events and project meetings and maybe represent the project at conferences.\nGreat community managers who go above and beyond are key players in the success of an open source project.\nMentoring If you are a seasoned contributor, you can pay it forward to the community by being a mentor.\nCode Review Having more people review code can help improve the quality of the code. This means fewer bugs, faster reviews, and a better project.\nMost projects allow anyone to review code.\nHelping Newcomers Sharing your skills and experience as a contributor can help newcomers contribute to the project. There are a lot of open source mentorship programs that connect mentees to mentors.\nHaving new contributors will ensure the sustainability of the project.\n This list is not exhaustive and you can always find more ways to make impactful contributions.\nAs an open source maintainer, I see the value in every contribution. And I\u0026rsquo;m sure more maintainers see it too.\n","permalink":"https://navendu.me/posts/non-code-contributions-to-open-source/","summary":"As open source becomes the default way to build software, contributions required to create, manage, and sustain projects go beyond code. This article teaches you to make non-code contributions.","title":"Non-Code Contributions to Open Source"},{"content":"I have been maintaining open source code for the past three years.\nThese are my tips for making better pull requests.\nThis is a short article, but you can check out this Twitter thread for a faster read.\nTip 1: Follow Contributor Docs Most open source projects have contributing docs (usually a CONTRIBUTING.md file at the root of the repo) that cover how to set up your development environment, coding conventions, areas to contribute to, and more.\nIf you are looking to contribute to a project, it should be the second place you look into after the README file.\n CONTRIBUTING.mdFrom github.com/meshery/meshery\n  Tip 2: Make Small and Focused Pull Requests For the most part, your pull requests should do one thing and one thing only.\nThe problem with addressing multiple things in a single pull request is that the reviewers may only approve some of the changes, which could lead to lengthy discussions.\nSmall and focused pull requests reduce the time taken for reviews and make them likely to be merged.\n 13 lines of code to add a small featureFrom github.com/meshery/meshery\n  Tip 3: Add Tests If you are adding new code, make sure you also add automated tests.\nA general rule of thumb is \u0026ldquo;If it can be tested, it should be tested.\u0026rdquo;\nTip 4: Use Pull Request Templates Most open source projects have templates to guide contributors to document their pull requests.\nThe template shown below has sections for the contributors to fill in:\n Pull request templateFrom github.com/openservicemesh/osm\n  Tip 5: Make Sure the Automated Tests Pass Projects often run automated tests on all pull requests.\nThese tests are in place to ensure the code you add doesn\u0026rsquo;t break any rule or code. Most projects have linters, build checks, and unit and integration tests.\nIf these checks are not passing, fix them and make sure all the tests pass before requesting a review.\nYour code is unlikely to be merged if any of these checks are failing.\n All systems goFrom github.com/meshery/meshery\n  Tip 6: Document Your Pull Requests This is probably the most important tip.\nThe goal of documenting your pull requests is to make the review process as easy as possible. A reviewer should get the context of your pull requests by skimming through their descriptions.\nFor this, you can:\n  Write self-documenting code\n  Use comments liberally in your code\n  Write clear commit messages\n  And most importantly, write what your pull request does clearly\n  In the example below, the description shows what issue the pull request fixes, the changes made, and the change to the user experience.\nThis makes the pull request very easy to review.\n You can never explain too muchFrom github.com/meshery/meshery\n  Tip 7: Respond to Feedback When you open a pull request, you are also opening a discussion on why the change is needed and why your pull request is the right way to add that change.\nMake sure you respond to reviews, make changes as suggested, and ask for clarification if needed.\nThis would make the review process easier by getting you and the reviewer on the same page.\n Sometimes discussions can get longFrom github.com/meshery/meshery\n  Tip 8: Be Patient Some open source maintainers are volunteers and spend their free time on the projects.\nThey may take some time to get to your pull request and review it.\nBe patient while they do their thing. Take a break, drink a glass of water, and watch reruns of The Office.\nBonus Tip I write a lot about my experience in building, scaling, and maintaining open source projects.\nSo make sure you subscribe below for more such tips.\n","permalink":"https://navendu.me/posts/pull-requests-like-a-pro/","summary":"Tips to make High-Quality Pull Requests.","title":"Pull Requests like a Pro"},{"content":"Contributing to open source is one of the best ways to gain skills and build your resume as a student or a new developer.\nAs open source becomes the norm, more and more tech companies are investing in open source projects and internship programs.\nThese programs often offer interns a mentorship opportunity and a stable stipend so that they can invest their time to work on these projects.\nThis article contains all the details of these programs. You can skip to the summary for quick access to all these resources.\nGoogle Summer of Code (GSoC)    Program Name Duration Timeline Stipend     Google Summer of Code (GSOC) 10-22 Weeks (flexible) January 2023 - November 2023 $1500 - $6600    This is one of the most popular programs with 700+ participating organizations. The program is organized by Google and each year mentees identify projects and submit proposals to work on them. Accepted mentees are assigned a mentor by the participating organization and students spend their summer working with them.\nAs of 2022, anyone above the age of 18 can participate in GSoC.\nFor 2023, mentees and organizations can choose the project time and project length based on their ability to commit. Read the announcement blog for more details.\nLinux Foundation Mentorship Program (LFX)    Program Name Duration Timeline Stipend     Linux Foundation Mentorship Program (LFX) 12 Weeks, ˜3 Months March 1st - May 31st, June 1st - August 31st, September 1st - November 30th $3000 - $6600    This program is organized by The Linux Foundation and aims to pair open source talent with experienced mentors. The students identify a project and create a profile to submit an application. Mentees can apply to 3 projects at a time.\nMLH Fellowship    Program Name Duration Timeline Stipend     MLH Fellowship 12 Weeks January 30th - September 8th (3 batches year round) Up to $5000 (need based)    This program will provide you the opportunity to contribute to the type of open source projects that every company depends on.\nThere are also non-open source programs that MLH offers. See fellowship.mlh.io/#programs.\nGoogle Season of Docs (GSoD)    Program Name Duration Timeline Stipend     Google Season of Docs 6 Months April 14th - November 15th (2023 program not announced) Depends of budget of the organization    This program gives technical writers an opportunity to gain experience in open source.\nIf you are interested in contributing to documentation, this is a really good opportunity to work on some interesting projects.\nOutreachy    Program Name Duration Timeline Stipend     Outreachy 3 Months May - August, December - March (Applications due February 2023) $7000     Outreachy is a diversity initiative that provides paid, remote internships to people subject to systemic bias and impacted by underrepresentation in the technical industry where they are living.\n Season of KDE    Program Name Duration Timeline Stipend     Season of KDE 3 Months January - April (Applications due on January 15th 2023) No (Certificates and swags provided)    This program offers an opportunity for people to participate in both code and non-code projects that benefits the KDE ecosystem.\nFree Software Foundation (FSF) Internship    Program Name Duration Timeline Stipend     Free Software Foundation (FSF) Internship 12 Weeks 3 terms yearly No    This program provides an opportunity to work closely with the FSF staff members in your area of interest, such as campaign and community organizing, free software licensing, systems and network administration, GNU Project support, or Web development.\nLinux Kernel Mentorship Program    Program Name Duration Timeline Stipend     Linux Kernel Mentorship Program 12 Weeks Full-time, 24 Weeks Part-time March 1st - May 31st, June 1st - August 31st, September 1st - November 30th $3000 - $6600    This program from The Linux Foundation connects experienced Linux Kernel developers and maintainers with mentees to help the become contributors to the Linux Kernel.\n The program serves as a vehicle to reach out to students and developers to inject new talent into the Linux Kernel community. It aims to increase diversity in the Linux Kernel community and work towards making the kernel more secure and sustainable. We strongly encourage applicants who are from traditionally underrepresented or marginalized groups in the technology and open source communities, including, but not limited to: persons identifying as LGBTQ, women, persons of color, and/or persons with disabilities.\n Linux Foundation Networking (LFN) Mentorship Program    Program Name Duration Timeline Stipend     Linux Foundation Networking (LFN) Mentorship Program 12 Weeks FT, 24 Weeks PT June 1st - August 21st, September 1st - ? $3000 - $6600    This program - also from The Linux Foundation - aims to provide opportunity to gain exposure to LFN\u0026rsquo;s projects and technical communities.\nThe mentors in this program are active developers and technologists contributing to the industry\u0026rsquo;s leading open source networking projects such as ONAP, OPNFV, OpenDaylight, FD.io.\nGNOME Summer of Code    Program Name Duration Timeline Stipend     GNOME Summer of Code 10-22 Weeks (flexible) January 2023 - November 2023 $1500 - $6600    This is provided through GSoC to help underrepresented groups in free and open source software to get involved with GNOME projects.\nAlibaba Summer of Code    Program Name Duration Timeline Stipend     Alibaba Summer of Code 3 Months May 25th - August 31st (2020, not announced for 2023) Yes    Students will receive mentorship from the Alibaba team to work on a series of open source projects.\nFOSSASIA Codeheat    Program Name Duration Timeline Stipend     FOSSASIA Codeheat Year round October - June  (Not announced for 2023) Prizes for winners    This is a coding contest for FOSSASIA projects on GitHub and a jury wil choose winners from the top 10 contributors based on the code quality and the relevance of the commits.\nFOSSASIA Internship Program    Program Name Duration Timeline Stipend     FOSSASIA Internship Program 2 - 6 Months Decided individually Yes     In the program we are looking for people who would like to work on the project they choose continuously. Different to GSoC in the internship it is not only about a specific project proposal. We rather look for participants who are interested to advance the project and solve bugs or add features that are required to bring the project forward.\n Open Summer of Code    Program Name Duration Timeline Stipend     Open Summer of Code 16 days See Timeline Yes    This program will coach the students into working in different open innovation projects provided by partnering organizations, companies and governments.\nOpen Mainframe Project Mentorship Program    Program Name Duration Timeline Stipend     Open Mainframe Project Mentorship Program 3 Months Through GSoC or LFX Yes    This program would help the mentee to expand their knowledge of mainframe technology and would help them contribute to open source projects that make it easier for infrastructure applications to run on mainframe.\nCNCF Mentoring Initiatives    Program Name Duration Timeline Stipend     CNCF Mentoring Initiatives ˜3 Months See Mentoring Programs Yes    Cloud Native Computing Foundation (CNCF) offers a vibrant community of projects and offers internships throughout the year through different mentoring programs.\nX.Org Endless Vacation of Code (EVoC)    Program Name Duration Timeline Stipend     X.Org Endless Vacation of Code (EVoC) 3 - 4 Months Can be initiated anytime $3000    The EVoC program was initiated to help support more projects that would otherwise go rejected through GSoC. Students are welcome to either come up with an idea on their own or work up a proposal for an idea suggested by someone else.\nHyperledger Mentorship Program    Program Name Duration Timeline Stipend     Hyperledger Mentorship Program 3 Months FT - 6 Months PT June 1st - Auguest 24th (FT), November 16th (PT) $3000 - $6600    This program provides a structured and hands-on opportunity for students and new developers gain exposure to Hyperledger open source development and entry to the technical community.\nJulia Seasons of Contributions (JSoC)    Program Name Duration Timeline Stipend     Julia Seasons of Contributions (JSoC) - Through GSoC or LFX Yes    A set of seasonal programs for funding or mentoring students and other developers to contribute to the Julia open source ecosystem.\nSummer of Haskell    Program Name Duration Timeline Stipend     Summer of Haskell 10 Weeks, ˜3 Months Through GSoC $1500 - $3300    This program is an effort by Haskell.Org to reach out to students and encourage them to contribute to the Haskell community with the aid of experienced mentors.\n24 Pull Requests    Program Name Duration Timeline Stipend     24 Pull Requests 1 Month December 1st - December 24th -    As the name suggests, this program encourages new contributors to make 24 pull requests in the month of December. This is a very beginner friendly program.\nSummary Here is the entire article summarized into a table:\n   Program Name Duration Timeline Stipend     Google Summer of Code (GSOC) 10-22 Weeks (flexible) January 2023 - November 2023 $1500 - $6600   Linux Foundation Mentorship Program (LFX) 12 Weeks, ˜3 Months March 1st - May 31st, June 1st - August 31st, September 1st - November 30th $3000 - $6600   MLH Fellowship 12 Weeks January 30th - September 8th (3 batches year round) Up to $5000 (need based)   Google Season of Docs 6 Months April 14th - November 15th (2023 program not announced) Depends of budget of the organization   Outreachy 3 Months May - August, December - March (Applications due February 2023) $7000   Season of KDE 3 Months January - April (Applications due on January 15th 2023) No (Certificates and swags provided)   Free Software Foundation (FSF) Internship 12 Weeks 3 terms yearly No   Linux Kernel Mentorship Program 12 Weeks Full-time, 24 Weeks Part-time March 1st - May 31st, June 1st - August 31st, September 1st - November 30th $3000 - $6600   Linux Foundation Networking (LFN) Mentorship Program 12 Weeks FT, 24 Weeks PT June 1st - August 21st, September 1st - ? $3000 - $6600   GNOME Summer of Code 10-22 Weeks (flexible) January 2023 - November 2023 $1500 - $6600   Alibaba Summer of Code 3 Months May 25th - August 31st (2020, not announced for 2023) Yes   FOSSASIA Codeheat Year round October - June  (Not announced for 2023) Prizes for winners   FOSSASIA Internship Program 2 - 6 Months Decided individually Yes   Open Summer of Code 16 days See Timeline Yes   Open Mainframe Project Mentorship Program 3 Months Through GSoC or LFX Yes   CNCF Mentoring Initiatives ˜3 Months See Mentoring Programs Yes   X.Org Endless Vacation of Code (EVoC) 3 - 4 Months Can be initiated anytime $3000   Hyperledger Mentorship Program 3 Months FT - 6 Months PT June 1st - Auguest 24th (FT), November 16th (PT) $3000 - $6600   Julia Seasons of Contributions (JSoC) - Through GSoC or LFX Yes   Summer of Haskell 10 Weeks, ˜3 Months Through GSoC $1500 - $3300   24 Pull Requests 1 Month December 1st - December 24th -    ","permalink":"https://navendu.me/posts/open-source-internship-programs/","summary":"A curated list of open source internship/mentorship programs with all the necessary details.","title":"20+ Open Source Internship Programs (Updated for 2023)"},{"content":" I was a maintainer of the project at the time of recording this. I’m no longer involved with the project now.\n Meshery is an open source service mesh management plane that enables the adoption, operation, and management of any service mesh and their workloads.\nThe project is hosted by the Cloud Native Computing Foundation.\nIn this webinar, Lee and Navendu introduce Meshery and shows how you can use it to manage your service meshes.\n  ","permalink":"https://navendu.me/posts/cncf-webinar-meshery/","summary":"Meshery is an open source service mesh management plane that enables the adoption, operation, and management of any service mesh and their workloads.","title":"Meshery, the Service Mesh Manager"},{"content":" You can skip to the template here.\n People can\u0026rsquo;t use what they don\u0026rsquo;t understand.\nFor open source projects, good documentation is as important as the project itself.\nThis is a guide to create useful READMEs that is best suited for small open source projects without dedicated websites having extensive documentation.\n Lame attempt at humour by 2020 NavenduPhoto from imgflip\n  You can use this guide as a reference and build on top of it for your projects.\nA Useful README A README file is the first thing a user will see in your project. First impressions last, so it is important to make a good README.\nA good README also helps:\n your project cut through the noise and stand out your users understand how to use the project your contributors contribute to the project  There is not a one-size-fits-all when it comes to usability. But having the following sections can help:\n Title and description: Name of your project and an introduction to what it is about. Quickstart/Demo: A short introduction that users can follow to get started quickly with your project. Installation: A guide on how users can install your project. Usage: Details on how to use the project. Development: How users can build a development version of the project on their machines. Contribute: Guide for open source contributors looking to contribute to the project.  Considering all these, I have created a reusable template you can fork on GitHub.\nDepending on your project, you might have very different README files. The idea is to use this as a reference as you write your README.\n Lame attempt at humour by 2020 NavenduPhoto from imgflip\n  ","permalink":"https://navendu.me/posts/how-to-write-an-awesome-readme/","summary":"A template for writing useful READMEs.","title":"Awesome README: A Template for Writing Useful READMEs"}]