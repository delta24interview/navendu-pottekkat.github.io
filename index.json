[{"content":"ChatGPT has been making waves all over the internet ever since its public beta release last month.\nEveryone is amazed at how well it works, and some even have concerns about ChatGPT replacing their jobs.\nRegardless of whether or not we won\u0026rsquo;t have jobs anymore, it is clear that ChatGPT and other AI advancements in this last year are paving the way to the future. So it might be important to understand, at least from a high level, what ChatGPT is and how it works.\nAnd who better to explain ChatGPT than ChatGPT itself (him/her/themselves?)?\nHere is my conversation with ChatGPT explaining what it is, how it works, and how you can use it.\nWhat is ChatGPT?  Navendu: Can you explain what ChatGPT is? The person you are explaining this to does not know AI, neural networks, and other technical jargon. Explain it so that such a person would get a concrete understanding of what ChatGPT is.\n  ChatGPT: Sure, I\u0026rsquo;d be happy to explain ChatGPT to you in a way that avoids technical jargon as much as possible.\nChatGPT is a type of artificial intelligence (AI) that is designed to carry out conversation with humans. It is based on a technology called natural language processing (NLP), which allows it to understand and generate human-like text.\nIn practical terms, this means that you can have a conversation with ChatGPT as if it were a real person. You can type out a message or ask a question, and ChatGPT will respond with a message that is generated based on its understanding of the conversation so far and its knowledge of the subject at hand.\nOne way that ChatGPT can be used is as a chatbot, which is a computer program that simulates conversation with human users, especially over the Internet. ChatGPT is a type of chatbot that uses advanced NLP techniques to generate more realistic and sophisticated responses than some other chatbots.\n The answer is clear and makes sense. But some people might not be familiar with a few technical terms.\nWhat Are These Technical Terms?  N: Thanks. Can you explain all those terms you mentioned—artificial intelligence and natural language processing?\n  CGPT: Certainly! Here are brief definitions of the terms I mentioned:\nArtificial intelligence (AI): This refers to the ability of a computer or machine to perform tasks that would normally require human intelligence, such as understanding language, recognizing patterns, and making decisions. There are different types of AI, including narrow AI (which is designed to perform a specific task) and general AI (which is designed to perform a wide range of tasks).\nNatural language processing (NLP): This refers to the ability of a computer or machine to process and understand human language. NLP is a subfield of AI that focuses on the interaction between computers and humans using natural language. It involves techniques for analyzing, interpreting, and generating human-like text and speech.\nI hope this helps give you a better understanding of these terms!\n This gives a good understanding of the underlying technologies used in ChatGPT. Now with this knowledge, can we try to understand a bit more about how ChatGPT works?\nHow Does ChatGPT Work?  N: Given that the person you are explaining this to understands the basics of what ChatGPT is from your responses, can you explain how ChatGPT works in a bit more detail?\n  CGPT: Certainly! Here\u0026rsquo;s a bit more detail on how ChatGPT works:\nChatGPT is a type of artificial intelligence (AI) that is designed to carry out conversation with humans. It is based on a technology called natural language processing (NLP), which allows it to understand and generate human-like text.\nTo carry out a conversation, ChatGPT uses a process called machine learning. This involves feeding the AI a large amount of data, such as transcripts of human conversations or written texts, and using algorithms to analyze this data and learn from it. As a result, ChatGPT is able to generate responses that are more human-like and sophisticated than those of some other chatbots.\nWhen you have a conversation with ChatGPT, it uses its understanding of the conversation so far and its knowledge of the subject at hand to generate a response to your message or question. It does this by analyzing the words and phrases you use, as well as the context of the conversation, and using this information to generate a response that is appropriate and relevant to the conversation.\n If you just heard about AI, NLP, and machine learning from this article, you might be confused about how ChatGPT learns. So let\u0026rsquo;s ask it!\nHow Does ChatGPT \u0026ldquo;Learn\u0026rdquo;?  N: But how does ChatGPT understand the user\u0026rsquo;s conversation? How does ChatGPT \u0026ldquo;learn\u0026rdquo;?\n  CGPT: Great question! Here\u0026rsquo;s a bit more detail on how ChatGPT understands user conversations and \u0026ldquo;learns\u0026rdquo;:\nWhen ChatGPT is fed a large amount of data, such as transcripts of human conversations or written texts, it uses algorithms to analyze this data and learn from it. These algorithms are designed to identify patterns and relationships in the data, and to use this information to make predictions or generate responses.\nFor example, if ChatGPT is fed a large number of conversations about a particular topic, it might learn to recognize common phrases and words that are associated with that topic, and to use these phrases and words in its own responses. It might also learn to recognize the context of a conversation and to generate responses that are appropriate to that context.\nIn this way, ChatGPT is able to \u0026ldquo;learn\u0026rdquo; from the data it is fed, and to use this learning to generate more human-like and sophisticated responses in conversation.\n Now you might be thinking, \u0026ldquo;how does this help me?\u0026rdquo; ChatGPT has answers for that too.\nHow Does This Help Me?  N: Awesome! Now how can someone use ChatGPT to improve their daily lives?\n  CGPT: There are a number of ways that ChatGPT or other natural language processing (NLP) technologies can be used to improve daily life:\nPersonal assistants: ChatGPT or other NLP technologies can be used to create personal assistants that can help with tasks such as scheduling, sending emails, or setting reminders. This can save time and make it easier to stay organized.\nCustomer service: ChatGPT or other NLP technologies can be used to create chatbots that can provide customer service for businesses. These chatbots can answer questions, resolve issues, and provide information to customers, freeing up human customer service representatives to focus on more complex tasks.\nEducation and language learning: ChatGPT or other NLP technologies can be used to create educational tools that can help people learn new subjects or improve their language skills. For example, a chatbot could be used to provide personalized language lessons or to offer explanations and practice exercises for a particular subject.\nEntertainment: ChatGPT or other NLP technologies can be used to create chatbots that can carry on interesting and engaging conversations with users. This can provide a fun and interactive way to pass the time or to learn about new topics.\n Jeez! That\u0026rsquo;s a lot. I\u0026rsquo;m excited and scared at the same time about losing my job to some computer code. Do we have something to worry about?\nWill ChatGPT Take My Job?  N: That\u0026rsquo;s cool! Does this mean that ChatGPT and other NLP technologies might replace humans for some specific tasks?\n  CGPT: It\u0026rsquo;s possible that ChatGPT and other natural language processing (NLP) technologies could be used to perform some tasks that are currently done by humans, especially tasks that involve processing and understanding large amounts of text or language data. For example, ChatGPT or other NLP technologies could be used to automate tasks such as customer service or data entry, which currently require human labor.\nHowever, it\u0026rsquo;s important to note that ChatGPT and other NLP technologies are not yet capable of replacing humans for all tasks. They are still limited by their inability to fully replicate human intelligence and creativity, and there are many tasks that require these qualities that cannot be automated.\nAdditionally, the use of ChatGPT and other NLP technologies to automate certain tasks could have implications for employment and the labor market. While automation can often improve efficiency and reduce costs, it can also lead to job displacement for some workers. As with any technological advancement, it is important to consider the potential impacts on society and to take steps to address any negative consequences.\nI hope this helps give you a better understanding of the potential and limitations of ChatGPT and other NLP technologies!\n Well, I think AI has the potential to change our jobs for good.\nWe might not need to do a lot of our current jobs, but AI could replace them with something novel. This could be similar to how computers replaced a lot of jobs when they first became mainstream.\nShould I Try ChatGPT? If you made it this far, you are really interested in ChatGPT. So give it a try. It will be fun.\nDon\u0026rsquo;t take my word for it. Ask the AI.\n N: Okay. What do you have to say to someone who hasn\u0026rsquo;t tried out ChatGPT yet but is interested in it?\n  CGPT: If you haven\u0026rsquo;t tried out ChatGPT yet but are interested in it, I would recommend giving it a try! ChatGPT is a fun and interactive way to learn about natural language processing (NLP) and artificial intelligence (AI), and it can be a great source of entertainment and information.\nThere are a number of ways you can try out ChatGPT or other NLP technologies:\nOnline demos: Many websites offer online demos of ChatGPT or other NLP technologies that you can try out for free. These demos often allow you to type in a message or question and see how the AI responds.\nChatbot apps: There are a number of chatbot apps that use NLP technologies, including ChatGPT, that you can download and use on your phone or computer. These apps can provide a fun and interactive way to have conversations with a chatbot.\nEducational resources: There are many online resources, such as tutorials and courses, that can help you learn more about NLP and AI, including ChatGPT. These resources can be a great way to get started with these technologies and understand how they work.\n You can try out ChatGPT for free at chat.openai.com .\n","permalink":"https://navendu.me/posts/chatgpt-explains-chatgpt/","summary":"A conversation with ChatGPT about ChatGPT. Who are you?","title":"ChatGPT Explained by ChatGPT"},{"content":"This article is a part of the series \u0026ldquo;Hands-On With Apache APISIX Ingress \u0026rdquo;.\nThe default Kubernetes Ingress resource exposes many standard features provided by Ingress controller implementations. However, if you use Ingress controllers like Apache APISIX , the default Ingress resource will limit its full capabilities.\nThis tutorial will look at how you can use annotations , custom resource definitions (CRDs) , and Plugins to extend Kubernetes Ingress to include the full capabilities of APISIX.\nBefore you move on, make sure you:\n Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install the sample application and APISIX in your Kubernetes cluster.  Annotations Ingress controller implementations use annotations for configuring additional parameters. Each of the implementations has different annotations that are unique to it.\nAPISIX supports 14 annotations which you can use to enable and configure features not exposed by the default Ingress resource.\nIn our example, we will configure APISIX to only allow traffic from a single IP address. This can be configured by using the annotation k8s.apisix.apache.org/allowlist-source-range as shown below:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes annotations: k8s.apisix.apache.org/allowlist-source-range: \u0026#34;172.17.0.1\u0026#34; spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api-v1 port: number: 8080 path: /v1 pathType: Exact - backend: service: name: bare-minimum-api-v2 port: number: 8081 path: /v2 pathType: Exact Now, if we make requests from a different IP address, we will get the response:\n{\u0026#34;message\u0026#34;:\u0026#34;Your IP address is not allowed\u0026#34;} But the problem with annotations is that they can get messy. Nginx Ingress has more than 100\u0026#43; annotations , and using them takes work.\nCustom CRDs Instead of restricting your additional configurations to annotations, you can use APISIX\u0026rsquo;s custom CRDs .\nThese are custom Kubernetes resources tailored for configuring APISIX. The configuration is similar if you are already familiar with APISIX, making it much easier to leverage the complete feature set of APISIX.\nThe example below shows how you can split traffic between two services using the ApisixRoute CRD:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: method-route spec: http: - name: method match: hosts: - local.navendu.me paths: - /api backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 70 - serviceName: bare-minimum-api-v2 servicePort: 8081 weight: 30 Now, when you send requests, APISIX will split the traffic 70:30 between the two services:\nfor i in {1..20} do curl http://127.0.0.1:57761/api -H \u0026#39;host:local.navendu.me\u0026#39; done Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v2.0! Hello from API v2.0! Plugins APISIX comes with 80\u0026#43; Plugins out of the box. You can also create your own Plugins for tailored use cases. These Plugins allow you to extend APISIX\u0026rsquo;s capabilities to include features like authentication, security, traffic control, and observability.\nFor our example, we will use the limit-count Plugin to limit the number of requests a client can send in a given time. We can create a Route and configure the Plugin with the ApisixRoute resource:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: method-route spec: http: - name: method match: hosts: - local.navendu.me paths: - /api backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 50 - serviceName: bare-minimum-api-v2 servicePort: 8081 weight: 50 plugins: - name: limit-count enable: true config: count: 10 time_window: 10 Now, APISIX will only allow ten requests every ten seconds for one client:\nHello from API v1.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v2.0! Hello from API v1.0! Hello from API v2.0! Hello from API v2.0! Hello from API v2.0! Hello from API v1.0! \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;503 Service Temporarily Unavailable\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;503 Service Temporarily Unavailable\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;openresty\u0026lt;/center\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Powered by \u0026lt;a href=\u0026#34;https://apisix.apache.org/\u0026#34;\u0026gt;APISIX\u0026lt;/a\u0026gt;.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;503 Service Temporarily Unavailable\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;503 Service Temporarily Unavailable\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;openresty\u0026lt;/center\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Powered by \u0026lt;a href=\u0026#34;https://apisix.apache.org/\u0026#34;\u0026gt;APISIX\u0026lt;/a\u0026gt;.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; What\u0026rsquo;s Next? This tutorial gave you an introduction to how you can extend APISIX Ingress. See the resources below to learn more about annotations, CRDs, and Plugins:\n List of available annotations  APISIX CRDs documentation  APISIX CRDs reference  APISIX Plugin documentation   See the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress \u0026rdquo;.\n","permalink":"https://navendu.me/posts/extending-apisix-ingress/","summary":"A hands-on tutorial on leveraging the full features provided by APISIX in Kubernetes Ingress.","title":"Hands-On: Extending Apache APISIX Ingress with Annotations, CRDs, and Plugins"},{"content":"Continuous monitoring is critical in making microservice systems robust. Without proper monitoring, microservices can quickly become overwhelmed, leading to errors and loss in performance.\nThrough continuous monitoring, developers can detect issues with their services as soon as they arise and take measures to prevent significant damage. It also provides insights into how your services are performing, allowing you to make informed decisions.\nThis article will introduce how you can set up monitoring on your microservice application using two of the popular tools in this space, Prometheus , and Grafana .\nThe source code and Docker Compose file for this tutorial are available in monitoring-101 .\nPrometheus Basics Prometheus is an open source, monitoring and alerting tool. It \u0026ldquo;pulls\u0026rdquo; metrics (measurements) from microservices by sending HTTP requests and stores the results in a time-series database.\nYou can instrument your services by using client libraries provided by Prometheus. This will enable you to create and collect custom metrics from your services.\nPrometheus also has exporters that let you pull metrics that are not in Prometheus format. An exporter acts as a middleman and transforms exported data into Prometheus readable format.\nflowchart LR app(\"Instrumented service\\n or Exporter\")  |scrape metrics| p -- |push alerts| am(\"Alert\\nmanager\") style p stroke: #e6522c style db stroke: #e6522c subgraph p [\"Prometheus server\"] db(Time series\\ndatabase) end  Prometheus provides a powerful query language, PromQL , to work with this collected data. You can use PromQL to create complex queries to filter, aggregate, and transform the data to the desired format.\nIn addition to pulling metrics, Prometheus can also trigger alerts when set thresholds are breached. The alerting mechanism is highly configurable and can send notifications to places like Slack or email.\nPrometheus has a GUI that lets you visualize the collected metrics easily. It also integrates with other advanced visualization tools like Grafana.\n Prometheus DashboardShows the memory stats of a Go-based service\n  Metric Types Prometheus offers four core metric types:\n Counter: represents a single monotonically increasing counter. Its value can increase or reset to zero on restart. You can use it to represent metrics like the number of requests served. Gauge: represents a numerical value that can go up or down. You can use it to represent values like memory usage or the number of requests per second. Histogram: samples data into configurable buckets. Use it to represent values like request durations or response sizes. Summary: similar to histogram, it also calculates configurable values over a sliding time window.  You can learn more about these metric types and how to use them from the official documentation .\nSample Application Our sample application will consist of an API gateway, a Go app, and a Python app.\nflowchart LR u(User) -- |/hello/es?name=Navendu| a(Apache\\nAPISIX) -- |/es?name=Navendu| g(Go\\nservice) -- |/es| p(Python\\nservice) p -- |\"{message: 'Hola'}\"| g g -- |\"Hola Navendu!\"| a a -- |\"Hola Navendu!\"| u style a stroke: #e62129  The application will return, \u0026ldquo;Hello \u0026lt;name\u0026gt;!\u0026rdquo; in the language you choose with the \u0026lt;name\u0026gt; you provide. Apache APISIX will be the API gateway that directs traffic to your services.\nThe diagram below shows how the system works.\nsequenceDiagram autonumber actor u as User participant a as Apache APISIX participant g as go-app participant p as python-app link a: Website @ https://apisix.apache.org link g: Source code @ https://github.com/navendu-pottekkat/monitoring-101/blob/master/go-app/main.go link p: Source code @ https://github.com/navendu-pottekkat/monitoring-101/blob/master/python-app/main.py u-a: GET /hello/es?name=Navendu activate a a-g: GET /es?name=Navendu activate g g-p: GET /es activate p p--g: {message: \"Hola\"} deactivate p g--a: Hola Navendu! deactivate g a--u: Hola Navendu! deactivate a   The user sends a GET request to APISIX, the entry point for the application. APISIX forwards the request to the Go service. The Go service sends a GET request to the Python service to obtain \u0026ldquo;Hello\u0026rdquo; in the specified language. The Python service responds with the required translation of \u0026ldquo;Hello.\u0026rdquo; The Go service creates the required response by using the name provided in the query and sends it to APISIX. APISIX forwards the response back to the user.  Configuring Prometheus to Collect Metrics We will instrument and export metrics from all the services in our application and collect them in Prometheus. We will start with our API gateway, Apache APISIX .\nExporting Metrics from APISIX Apache APISIX is an open source, cloud native API gateway.\nYou don\u0026rsquo;t need to know about APISIX to follow along, and you can use the Docker Compose file provided to set everything up. To learn more about APISIX, visit apisix.apache.org .\nAPISIX offers a prometheus Plugin that easily exports metrics in the Prometheus format. You can configure the Plugin in your APISIX configuration file:\napisix: enable_admin: false # run APISIX in standalone mode config_center: yaml # use a YAML file for configuration instead of storing it in etcd plugin_attr: prometheus: export_uri: /prometheus/metrics # enable the prometheus Plugin and export the metrics to this URI enable_export_server: false # export the metrics in the default data-plane port  We can now enable the Plugin on every Route by making it a Global rule :\nroutes: # route requests to /hello/* to the go-app - uri: /hello/* upstream: type: roundrobin nodes: \u0026#34;go-app:8080\u0026#34;: 1 plugins: # remove the prefix \u0026#34;/hello\u0026#34; before forwarding the request to the go-app  proxy-rewrite: regex_uri: - \u0026#34;/hello/(.*)\u0026#34; - \u0026#34;/$1\u0026#34; # export Prometheus metrics to the specified URI - uri: /prometheus/metrics plugins: public-api: # enable the Prometheus Plugin globally on all Routes global_rules: - id: 1 plugins: prometheus: prefer_name: true #END This will export the metrics to the /prometheus/metrics endpoint in Apache APISIX.\nYou can learn more about the available metrics from the documentation .\nInstrumenting and Exporting Metrics from the Go Service Prometheus has an official Go client library for instrumenting Go applications.\nBy default, Prometheus will expose the default Go metrics. You can also create your own application-specific metrics.\nIn our service, we will expose the default metrics and create our own counter metric to track the number of requests:\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; // Prometheus packages \t\u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promauto\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) // Response stores the Message obtained from the python-app type Response struct { Message string `json:\u0026#34;message\u0026#34;` } // default language and name var ( lang = \u0026#34;en\u0026#34; name = \u0026#34;John\u0026#34; ) // create a new custom Prometheus counter metric var pingCounter = promauto.NewCounter( prometheus.CounterOpts{ Name: \u0026#34;go_app_request_count\u0026#34;, Help: \u0026#34;No of requests handled by the go-app\u0026#34;, }, ) // HelloHandler handles requests to the go-app func HelloHandler(w http.ResponseWriter, r *http.Request) { lang = r.URL.String() name = r.URL.Query()[\u0026#34;name\u0026#34;][0] fmt.Println(\u0026#34;Request for\u0026#34;, lang, \u0026#34;with name\u0026#34;, name) pingCounter.Inc() pUrl := os.Getenv(\u0026#34;PYTHON_APP_URL\u0026#34;) if len(pUrl) == 0 { pUrl = \u0026#34;localhost\u0026#34; } // call the python-app to obtain the translation \tresp, err := http.Get(\u0026#34;http://\u0026#34; + pUrl + \u0026#34;:8000\u0026#34; + lang) if err != nil { log.Fatalln(err) } body, err := ioutil.ReadAll(resp.Body) if err != nil { log.Fatalln(err) } resp.Body.Close() var m Response json.Unmarshal(body, \u0026amp;m) // send back response with \u0026#34;Hello name!\u0026#34; in the specified language \tfmt.Fprintf(w, \u0026#34;%s %s!\u0026#34;, m.Message, name) } func main() { // expose Prometheus metrics \thttp.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) http.HandleFunc(\u0026#34;/\u0026#34;, HelloHandler) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } This will expose the metrics to the endpoint /metrics. You can learn more about the Go client library from its GitHub repo .\nInstrumenting and Exporting Metrics from the Python Service Prometheus also has an official Python client library . There are also third-party libraries that are tailored to fit specific use cases.\nOur service uses FastAPI , and we will use the prometheus_fastapi_instrumentator library to instrument it:\nfrom fastapi import FastAPI from fastapi.middleware.cors import CORSMiddleware from prometheus_fastapi_instrumentator import Instrumentator app = FastAPI() hello = {\u0026#34;en\u0026#34;: \u0026#34;Hello\u0026#34;, \u0026#34;fr\u0026#34;: \u0026#34;Bonjour\u0026#34;, \u0026#34;es\u0026#34;: \u0026#34;Hola\u0026#34;, \u0026#34;ml\u0026#34;: \u0026#34;ഹലോ\u0026#34;} # expose the default Python metrics to the /metrics endpoint Instrumentator().instrument(app).expose(app) @app.get(\u0026#34;/{lang}\u0026#34;) async def get_hello(lang): return {\u0026#34;message\u0026#34;: hello[lang]} You can learn more about creating custom metrics from the documentation .\nConfiguring Prometheus We can now scrape and collect these metrics in Prometheus.\nYou can configure Prometheus to collect metrics from each of the services. By default, Prometheus checks for metrics in the /metrics path:\nglobal: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: \u0026#34;prometheus\u0026#34; static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] - job_name: \u0026#34;go-app\u0026#34; static_configs: - targets: [\u0026#34;go-app:8080\u0026#34;] - job_name: \u0026#34;python-app\u0026#34; static_configs: - targets: [\u0026#34;python-app:8000\u0026#34;] - job_name: \u0026#34;apisix\u0026#34; static_configs: - targets: [\u0026#34;apisix:9080\u0026#34;] metrics_path: \u0026#34;/prometheus/metrics\u0026#34; That\u0026rsquo;s it! Now, if you open up the Prometheus dashboard (default on port 9090) and click on \u0026ldquo;Status\u0026rdquo; from the navbar and \u0026ldquo;Targets,\u0026rdquo; you will be able to see the status of metrics being scraped from your services.\n Prometheus targetsMetrics are successfully being scraped from all sources\n  Querying and Visualizing Metrics in Prometheus Now, you can use the Prometheus dashboard to run queries and complex expressions.\n Querying PrometheusYou can get a lot more complex with your queries than this\n  You can learn more about querying Prometheus in the official documentation .\nUsing Grafana to Query Prometheus Grafana is an open source data visualization platform that works with Prometheus to provide a comprehensive tool for collecting, querying, and visualizing metrics.\nPrometheus is good at collecting metrics and querying but lacks in providing tooling for creating meaningful visualizations. Grafana overcomes this limitation by transforming the collected metrics into visualizations.\nGrafana is also compatible with many other data sources than Prometheus.\nOnce you have deployed Grafana, you can open the web UI (default on port 3000).\nFirst, you have to add Prometheus as a data source. To do this, go to `/datasources or \u0026ldquo;Configuration\u0026rdquo; and \u0026ldquo;Data sources.\u0026rdquo; Click on \u0026ldquo;Add data source\u0026rdquo; and select Prometheus. Specify where Prometheus is deployed, save and test the connection.\n Add Prometheus as a data sourceAdd the details of your Prometheus service, save, and test it\n  Using Pre-built Grafana Dashboards Grafana hosts a public dashboard repository that contains pre-built Grafana dashboards. You can use them in your Grafana instance to quickly visualize relevant metrics.\nWe will use the Go Processes dashboard which will process and visualize the process status published by the Prometheus Go client library.\nTo import this template, first, copy its ID (6671) from the dashboard repository. In your Grafana UI, go to \u0026ldquo;Dashboards\u0026rdquo; and select \u0026ldquo;Import.\u0026rdquo; Paste the ID you copied and click \u0026ldquo;Load.\u0026rdquo;\n Grafana dashboardVisualizes the process status of the Go service\n  You can also explore other pre-built dashboards or create your own. Refer to the documentation to learn more about this.\nWhat\u0026rsquo;s Next? That\u0026rsquo;s it for this tutorial!\nThis article was only an introduction to how you can set up monitoring on your services, and I encourage you to learn more about Prometheus and Grafana from the resources mentioned below:\n Prometheus Best Practices  Alerting based on metrics  Grafana Dashboards   The complete code and the Docker Compose file for this tutorial are available in monitoring-101 .\n","permalink":"https://navendu.me/posts/introduction-to-monitoring-microservices/","summary":"This tutorial walks you through setting up monitoring on a microservice application using Prometheus and Grafana.","title":"An Introduction to Monitoring Microservices with Prometheus and Grafana"},{"content":"GNU Privacy Guard, also known as GPG, is a popular open source software for secure communication.\nI have been using GPG keys recently to sign my commits and for encrypted messages.\nIn this article, I will explain GPG keys, how they work, and how you can use them.\nWhat is GPG? GPG is an implementation of OpenPGP, a standard for authenticating or encrypting data using public key cryptography.\nA user will have a public key that they can share with anyone and a private key that should be secret. You can use the private key to sign or decrypt messages, while others will use you public key to verify your signature or encrypt messages.\nTo send you a confidential message, anyone can use your public key, encrypt the message, and send it to you. You can read it after decrypting it with your private key.\nflowchart TB m(\"👥 Confidential message\") -- |\"🔐 Encrypt (Public key)\"| em(Encrypted message) -- |\"🗝️ Decrypt (Private key)\"| dm(\"👨🏽 Decrypted/Original message\") style em stroke: red style dm stroke: green  You can also sign your messages with your private key, and others can verify your signature with your public key.\nflowchart TB sm(\"👨🏽 Sensitive message\") -- |\"🖊️ Sign (Private key)\"| sms(\"🔏 Sensitive message + signature\") -- |\"👥 Verify (Public key)\"| v(\"👥 Verified message\") style sms stroke: green  Installing GPG You can download and install GPG from the official website if it isn\u0026rsquo;t already installed.\nTo check if it is installed properly, run:\ngpg -h Generating a New GPG Key Pair You can create a new key pair by running:\ngpg --full-generate-key It will prompt you several times to configure your keys:\n Kind: RSA and RSA (default) Size: \u0026gt; 4096 (for signing commits) Validity: Could be days, weeks, months, or years. For example, 1y will set it to one year. Name, email, and comment: This email and name will be associated with your signature. Passphrase: Secure passphrase to unlock the private key. Make it strong. Use a password generator if possible.  Once you complete all the prompts, gpg will generate a key pair.\n Tip: As gpg uses entropy to generate the key pair, it will depend on how active your system is. To generate more entropy, you can use something like rng-tools .\n To verify whether the key pair was created, you can run:\ngpg --list-keys Generating a Revocation Certificate A revocation certificate can invalidate your public key if you forget your passphrase or compromise your private key.\nYou should generate this as soon as you create a key pair and store it in a separate location.\nTo generate a revocation certificate, run:\ngpg --output ~/revocation.crt --gen-revoke your-email@your-provider.com When you revoke a public key, people can no longer use it to send encrypted messages to you. But it can still verify your past signatures and decrypt past messages.\nExporting Your Public Key To use GPG keys, you must export your public key. In an upcoming section , I will discuss how you can share the exported key with others in a public keyserver.\nTo export your key, run:\ngpg --armor --export your-email@your-provider.com You can also export it to a file by running:\ngpg --output ~/public.key --armor --export your-email@your-provider.com Signing Commits with Your GPG Key Pair All of my projects and the projects I contribute to are on GitHub. If you are using other platforms like GitLab, you can follow their official documentation .\n Note: The email associated with your GPG key should match a verified email configured in your GitHub account.\n On GitHub, go to \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Access\u0026rdquo; \u0026gt; \u0026ldquo;SSH and GPG keys\u0026rdquo; \u0026gt; \u0026ldquo;New GPG key\u0026rdquo;.\n Configuring public key on GitHubThis will be used to verify our signature on commits\n  In the \u0026ldquo;Key\u0026rdquo; field, paste the public key you exported from -----BEGIN PGP PUBLIC KEY BLOCK----- to -----END PGP PUBLIC KEY BLOCK----- including both.\nClick \u0026ldquo;Add GPG key\u0026rdquo; and enter your password to confirm.\nNext, you have to configure Git to use your created GPG private key to sign your commits.\nFirst, find the long form of your key ID by running:\ngpg --list-secret-keys --keyid-format=long /Users/user1/.gnupg/pubring.kbx --------------------------------- sec rsa4096/3AA5C34371567BD2 2022-10-12 [SC] 29D5E24EA8EF21FD70A8F2D3B33049A4551D uid [ultimate] Firstname Lastname (comment) \u0026lt;your-email@your-provider.org\u0026gt; ssb rsa4096/4BB6D45482678BE3 2022-10-12 [E] In this example, the key ID is 3AA5C34371567BD2.\nNow, use this key ID to configure Git:\ngit config --global user.signingkey 3AA5C34371567BD2  Tip: To sign all commits by default, run:\ngit config --global commit.gpgsign true  Now, when you commit, use the -S flag. You should also be able to configure your IDE to do this for you automatically for every commit.\nIf you push these commits to GitHub, you will see that they are verified using your public key.\n Verified commitsAll my commits are verified now\n  Sending Encrypted Messages with Your GPG Key Pair Encrypting Messages Anyone can use your public key to send encrypted messages to you. But first, you have to make your public key, well, public.\nYou can copy your public key and share it with people who want to send you encrypted messages. I have shared my public key on my website , and people can copy it and use it to encrypt messages.\nYou can also upload your key to a public key server like pgp.mit.edu:\ngpg --send-keys --keyserver pgp.mit.edu 3AA5C34371567BD2  Note: 3AA5C34371567BD2 is the key ID. Replace it with your key ID.\n Now, anyone will be able to request your public key from the key server with the command:\ngpg --recv-keys keyid To encrypt messages, get the public key of the person you are sending the message to. Make sure to add yourself as a recipient if you want to decrypt the message in the future:\ngpg --encrypt --sign --armor -r receiver-email@receiver-provider.com -r sender-email@sender-provider.com name-of-file This will create a .asc file containing your encrypted message.\nDecrypting Messages To decrypt a message, you can run gpg, and it will prompt you as necessary:\ngpg name-of-file.asc This will create a new file with the decrypted message.\nYou can also configure your email clients or other applications to use this key pair for encrypted communication. I use the Thunderbird email client, and its documentation explains how you can configure your GPG key.\n That\u0026rsquo;s it! I\u0026rsquo;m not a security or cryptography expert, but I will probably look into where I can use these keys next.\nI will write a new article or update this when I do so.\n","permalink":"https://navendu.me/posts/encrypted-communication-with-gpg/","summary":"Exploring how symmetric-key and public-key cryptography works by using GPG keys to sign commits and send encrypted messages.","title":"Signed Commits and Encrypted Communication with GPG Keys"},{"content":"I was hit with a wave of nostalgia recently and decided to go back and play some of my favorite retro games (instead of buying a PS5). I remembered how much fun I used to have playing these games and wanted to experience that again.\nI decided to use a Raspberry Pi, RetroPie, and a GPi CASE 2 to make my own handheld retro gaming device.\nThis tutorial will walk you through how I set all these up.\nBefore you move on, make sure you have:\n Raspberry Pi Computer Module 4 (Lite version with WiFi recommended) GPi CASE 2  A Micro SD card (at least 16 GB) and an adapter  I will try to be comprehensive so that you don\u0026rsquo;t have to spend time searching for information elsewhere. I will include troubleshooting steps and additional resources to help you along the way. So let\u0026rsquo;s get started!\nInserting CM4 Into the GPi CASE 2 There is a best way to insert the Pi in the case:\n Take off the back cover by removing the five screws in the back Align the markings on the Pi and the case Click the Pi into place by applying even pressure  I tried inserting the Pi without removing the back cover, and it wasn\u0026rsquo;t inserted correctly. To ensure that you don\u0026rsquo;t damage anything, follow the abovementioned steps.\nWriting the RetroPie Image to the SD Card The easiest way to download and write the RetroPie image is by using Raspberry Pi Imager .\nYou can download the Raspberry Pi Imager and open it. Connect your SD card to your computer through the adapter (if needed). You are now ready to write the image.\nIn the Raspberry Pi Imager, select the operating system. Choose \u0026ldquo;Emulation and game OS\u0026rdquo;, \u0026ldquo;RetroPie\u0026rdquo;, and the \u0026ldquo;RetroPie 4.8 (RPI 4/400)\u0026rdquo; OS.\n Selecting the RetroPie OSMake sure to choose the RPI 4/400 version\n   Note: Make sure to choose the one for RPI 4/400 if there is a newer version than 4.8.\n Select your SD card from the \u0026ldquo;Storage\u0026rdquo; option and click on \u0026ldquo;Write\u0026rdquo;. It will take some time to download and write the image to your SD card. Grab a cup of coffee.\nInstalling Display Patch and Safe Shutdown Since RetroPie defaults output to HDMI, we have to install a patch to transfer the display output to the GPIO pins.\nOnce you have the RetroPie image written to your SD card, you can install this patch.\nDownload the patch from the Retroflag website and extract it on your computer and follow the steps below:\n From the SD card, make a backup of config.txt on your computer. You can revert to this config if you mess things up. Copy the contents of the GPi_Case2_patch_retropie folder to the SD card. If you are on Windows, click on install_patch.bat, and it will install the patch. If you are on Linux or macOS, copy the contents of the GPi_Case2_patch_retropie/patch_files folder to the SD card. The files in the patch_files/overlays folder should go in the overlays folder on the SD card.  Next, you need to configure safe shutdown. To make our lives easier and not type out the command in the GPi CASE, you can create a script.\nCreate a new file on the SD card named gpi.sh and add the following content to the file:\nwget -O - \u0026#34;https://raw.githubusercontent.com/RetroFlag/GPiCase2-Script/main/retropie_install_gpi2.sh\u0026#34; | sudo bash Configuring WiFi Credentials Running the gpi.sh script will pull some files from the internet. So, you need to set up WiFi on your GPi CASE.\nTo do this easily, you can create a new file called wifikeyfile.txt and add the following:\nssid=\u0026quot;name of your WiFi (case sensitive)\u0026quot; psk=\u0026quot;your WiFi password\u0026quot;  Note: If your Raspberry Pi does not have WiFi, you can see this Reddit post for workarounds.\n First Boot With your SD card configured, you can disconnect it from your PC and insert it into your GPi CASE.\nBefore turning it on, connect it to power to ensure the battery does not die on the first boot.\nThe first boot will take longer (a couple of minutes at least) than usual. Be patient, and don\u0026rsquo;t worry.\nOnce it is ready, RetroPie will walk you through some configurations you can follow on the screen.\n Hello RetroPie!Follow the instructions on the screen to configure controls\n  Set Up WiFi You are not ready to play games yet!\nGo to the RetroPie configuration menu from the home screen and scroll down to WiFi.\n Configure WiFiFor more information on all the configuration options, see the RetroPie website\n  Once you go into the WiFi configuration menu, you will see a message saying that you don\u0026rsquo;t have your WiFi country set.\nSelect \u0026ldquo;Yes\u0026rdquo;, and in the following menu, choose your country from \u0026ldquo;Localisation Options\u0026rdquo;.\nYou can then save and exit, and RetroPie will prompt you to reboot the device.\nOnce you reboot the device, you can connect to your WiFi.\nNow, if you go to the WiFi configuration menu, you will see an option to \u0026ldquo;Import WiFi credentials from /boot/wifikeyfile.txt\u0026rdquo;. Select it and wait for some time.\nYou will see that the WiFi is connected, and you can exit the menu.\nRunning the Safe Shutdown Install Script In your RetroPie configuration menu, select \u0026ldquo;File Manager\u0026rdquo;.\nGo back twice to reach the root and navigate to the /boot folder. Scroll down, find the gpi.sh file, and run it. It will install the safe shutdown scripts, and the Pi will reboot.\nTo test if the scripts are running, try turning off through the hardware switch, and you will see a shutdown message.\nThat\u0026rsquo;s it! You are all ready to install some retro games and play. You should be easily able to find ROMs by searching the internet. Some of these are pirated, so sharing the sites is not super legal.\n I\u0026rsquo;m currently obsessed with Pokemon FireRed. Gotta catch\u0026rsquo;em all!\nTroubleshooting If you frequently see an error message like \u0026ldquo;failed to find mixer elements!\u0026quot;, see this fix by Alan Pfahler .\nIf you are trying to configure audio and are running into issues, you can see this thread on the RetroPie forum .\n","permalink":"https://navendu.me/posts/retropie-gpi-case-2-setup/","summary":"A complete tutorial on how to set up RetroPie and play retro games on a GPi CASE 2 with a Raspberry Pi Compute Module 4 under the hood.","title":"Retro Gaming With RetroPie, GPi CASE 2, and a Raspberry Pi"},{"content":"The 13 years of Bitcoin and other cryptocurrencies have created a cult of gamblers, a Trojan horse for Ponzi schemes1, and an ecosystem to help illicit financing2, money laundering3, and sanctions evasions4.\nIts underlying technology, which some people fancy to be a liberal utopia, will never be a solution because it is trying to solve a financial problem that doesn\u0026rsquo;t exist.\nLaszlo Hanyecz became a part of the crypto history books in 2010 by using 10,000 Bitcoins to buy two large pizzas5. These 10,000 Bitcoins worth $41 in 2010 are now worth $166,075,000. At its all-time high, 10,000 Bitcoins could get you $687,896,300 or 4,58,59,753 large pepperoni pizzas from Papa John\u0026rsquo;s.\nBitcoin\u0026rsquo;s \u0026ldquo;decentralized\u0026rdquo; nature, with its price dictated by celebrity tweets and subreddits, makes it too volatile to do anything useful. The \u0026ldquo;freedom\u0026rdquo; it gains from not having a central governing authority makes it unsuitable to be money6.\nThe claim that Bitcoin and other cryptos are \u0026ldquo;decentralized\u0026rdquo; is also invalid7. You depend on third-party wallets to store and manage your holdings, and the consensus algorithms that govern these are inherently centralized by a small set of elite players in the network8.\nEssentially, crypto will take power from the government and distribute it to these private enterprises.\nWhile Silicon Valley is still fantasizing about crypto and Web3, India has been leveling up its digital payments game with Unified Payments Interface or UPI.\nUPI is a system that has centralized and standardized digital payments in India. It is managed by National Payments Corporation of India (NPCI), an entity regulated by the Reserve Bank of India (RBI)9.\nIn the six years since it was introduced, there have been 600 million UPI users in India. A household population of 300 million means that two people in every Indian household use UPI10.\nThe goal of digital payment systems should never have been to decentralize but democratize. UPI is the ultimate example of this in action.\nUPI does not rely on a single bank or company compared to private digital payment alternatives in countries like the US (PayPal, Apple Pay). Instead, it provides a standard set of APIs that companies and banks can use to facilitate online payments.\nThis means that there are a lot of UPI payment apps you can choose from, and all of them interoperate with each other. Facebook\u0026rsquo;s WhatsApp Payments, Google\u0026rsquo;s GPay, and Walmart\u0026rsquo;s PhonePe (Flipkart) are some popular ones.\nPayments are easy. You can use a unique UPI ID similar to an email address (navendupottekkat@okaxis is my real UPI ID) or scan a QR code that will work on any UPI app. Money will be moved directly from your bank account to the payee\u0026rsquo;s account without intermediaries. These transactions are instantly reflected in your bank account.\nIndia accounted for the largest number of real-time transactions in 2021, with 48.6 billion transactions. The combined number from the US, Canada, UK, France, and Germany was 7.5 billion11.\n48.6 billion only represents 31.3% of all transactions in the country. By 2026, it is expected to increase to 70.7%.\nImplementing UPI was relatively more straightforward in India as there are lesser banks and all regulations are handled centrally by the RBI. Countries like China can be on par with India, but private digital wallets like WePay and AliPay still dominate.\nUPI transactions are currently free (maybe a small fee in the future). Along with the pandemic, this has helped accelerate UPI adoption.\nStill, UPI\u0026rsquo;s success with handling payments at such a large scale has prompted countries like UAE, Singapore, and more recently, France to integrate it with their systems12.\nOther countries like the US are building their own UPI-like systems with the help of central entities like the Federal Reserve13.\nCrypto has failed to be the payment system of the future. The current trend of fluctuating hype cycles with crypto, NFTs, and adjacent scams will inevitably end.\nRelying on a central authority and democratizing the system seems the best and proven way to move forward. I would not be surprised to see more countries adopting systems similar to UPI in the future.\n  Krugman, Paul. 2021. \u0026ldquo;Technobabble, Libertarian Derp and Bitcoin.” The New York Times. May 20, 2021. https://www.nytimes.com/2021/05/20/opinion/cryptocurrency-bitcoin.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dion-Schwarz, Cynthia, David Manheim, and Patrick B. Johnston. 2019. \u0026ldquo;Terrorist Use of Cryptocurrencies: Technical and Organizational Barriers and Future Threats.\u0026rdquo; Santa Monica, CA: RAND Corporation. 2019. https://www.rand.org/pubs/research_reports/RR3026.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Orcutt, Mike. 2020. “This Is How North Korea Uses Cutting-Edge Crypto Money Laundering to Steal Millions.” Web log. MIT Technology Review. March 5, 2020. https://www.technologyreview.com/2020/03/05/916688/north-korean-hackers-cryptocurrency-money-laundering/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Busch, Kristen, and Paul Tierno. 2022. “Russian Sanctions and Cryptocurrency.” Congressional Research Service. May 4, 2022. https://crsreports.congress.gov/product/pdf/IN/IN11920#:~:text=Potential%20Sanctions%20Evasion%20with%20Cryptocurrency,tamper%2Dresistant%20records%20of%20transactions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Ashmore, Dan, and Farran Powell. 2022. “Bitcoin Price History 2009 to 2022.” Forbes Advisor INDIA. November 1, 2022. https://www.forbes.com/advisor/in/investing/cryptocurrency/bitcoin-price-history-chart/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Taleb, Nassim N. 2021. \u0026ldquo;Bitcoin, Currencies, and Fragility.\u0026rdquo; arXiv. June 27, 2021. https://doi.org/10.48550/arXiv.2106.14204.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dailey, Natasha. 2022. “Crypto Isn’t Decentralized. It’s Actually Run by a Handful of Big Wigs Exploiting Low-Paid Workers, Says Long-Time Internet Academic.” Markets Insider. March 20, 2022. https://markets.businessinsider.com/news/currencies/crypto-isnt-decentralized-nft-bored-ape-yacht-club-buys-cryptopunks-2022-3.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n L. J. Valdivia, C. Del-Valle-Soto, J. Rodriguez and M. Alcaraz. 2019. \u0026ldquo;Decentralization: The Failed Promise of Cryptocurrencies,\u0026rdquo;. IT Professional, vol. 21, no. 2, pp. 33-40. 1 March-April 2019. https://ieeexplore.ieee.org/abstract/document/8676128.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n National Payments Corporation of India. “India’s Unified Payment Gateway for Real-Time Payment Transactions.” https://www.npci.org.in/PDF/npci/upi/Product-Booklet.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Gupta, Vikas. 2022. “Move over Bitcoin, Here Comes UPI, the next Big Investment Idea.” Moneycontrol. August 17, 2022. https://www.moneycontrol.com/news/business/personal-finance/move-over-bitcoin-here-comes-upi-the-next-big-investment-idea-9039791.html.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Barthe, Blanca, and Samuel Murrant. 2022. “Prime Time for Real-Time Global Payments Report.” ACI Worldwide. April 2022. https://www.aciworldwide.com/real-time-payments-report.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Jain, Sourabh. 2022. “You Will Soon Be Able to Use UPI in France in Addition to UAE, Singapore.” Business Insider. June 16, 2022. https://www.businessinsider.in/finance/news/upi-and-rupay-cards-will-soon-be-accepted-in-france/articleshow/92247190.cms.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n “Federal Reserve Updates FedNow Service Timing to Mid-2023, Marks Beginning of Full-Scale Pilot Testing.” 2022. Board of Governors of the Federal Reserve System. August 29, 2022. https://www.federalreserve.gov/newsevents/pressreleases/other20220829a.htm.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://navendu.me/posts/upi-vs-crypto/","summary":"Cryptocurrencies are trying to be a solution to a financial problem that does not exist. UPI solved digital payments years ago.","title":"No Thanks, Crypto, UPI Already Solved Digital Payments"},{"content":" If you are reading this, it\u0026rsquo;s just me thinking aloud about my future, reminiscing and critically evaluating my decisions throughout the last few years.\n I turned 25 this week.\nIt is the time of the year when I evaluate where I\u0026rsquo;m at and decide where I want to be next.\nMy overarching goal in the last few years was to focus on my career.\nI was out of college with absolutely no idea what I wanted to do next. I tried different jobs, worked almost always to get better, and had zero social life.\nThere wasn\u0026rsquo;t much else in my life outside of work. I knew I had to figure it out and set up my career. And today, I feel like I did that.\nI\u0026rsquo;m at a job I love. It is impactful work and aligned with my goals in life. I have immediately actionable and long-term plans to continue on this path.\n Goal 1: Learn continuously and do things that impact the world.\n Working on my career and having no social life has taken a toll on my relationships.\nAfter COVID hit, I stopped using my phone and stopped being active on social media. It helped my mental health and career but disconnected me from my friends and family. Now even though I have a phone, there is no going back to social media.\nThis year, I traveled a lot. I met people who I\u0026rsquo;ve known online. I met my old friends and made new ones. With my career on track, I can focus on maintaining relationships.\n Goal 2: Rebuild old relationships with friends, put more time and effort into maintaining relationships, meet new people and make new relationships.\n Being forced to work on my own had rekindled my creative aspirations in the past years. Being curious and obsessively building things led me to great opportunities.\nIn 2020, I left my job to work on things that interested me. I gave myself the time and freedom to explore whatever I liked until I found something I could see myself doing for a long time.\nBuilding up to a position to take that risk and taking that risk was probably the best decision I made in my life.\n Goal 3: Always make room to take risks, be creative, and start from scratch.\n Your experiences make who you are.\nPeople often say I\u0026rsquo;m an impressionable person. I don\u0026rsquo;t think I\u0026rsquo;m naive, but many of my experiences have shaped me into who I am.\n17-21 might have been when I frequently had many new experiences. Most people feel the same because that\u0026rsquo;s when you go into the real world.\nI feel like experiencing a lot of things when you are still susceptible to change can be pretty impactful. The last couple of years went by without a lot of new experiences. But I\u0026rsquo;m still young, and it is not too late to change things.\n Goal 4: Invest time, energy, and money in new experiences. Travel a lot, do a lot of different stuff, and be open-minded and impressionable.\n The one thing I would recommend everyone to learn actively is personal finance.\nYou don\u0026rsquo;t need to be an expert but being financially literate is necessary for the future.\nI\u0026rsquo;m earning more money than I ever had before. Apart from some responsibilities, I can afford to be liberal with my money. I have to start being systematic with how I spend and save money.\n Goal 5: Diversify income and investment streams. Be responsible financially.\n You can\u0026rsquo;t be genuinely wealthy without being healthy.\nIn the last few years after college, I have been taking care of my health pretty well. I exercise regularly and eat good food.\nBeing healthy physically has also helped my mental health. Recently I have been endorsing my friends to do the same.\nWhat I don\u0026rsquo;t often do now is sports. Not having your old mates back home is one primary issue, but I have met new people recently who invited me to their teams. I have to find them and play a couple of matches. Awkward!\nSetting up a routine to play sports and joining some clubs might be the best solution in the future.\n Goal 6: Take active steps to be healthy and participate in sports.\n Setting too many goals might make it harder to follow through.\nAn alternative might be to keep the goals short and see how I did on my next birthday.\nI want these goals to be high-level so that I can see them as North stars in all my decision-making processes.\nIdeally, all decisions I make and the things I do in the following years will be aligned with my goals. Only time will tell.\n","permalink":"https://navendu.me/posts/reasonable-birthday-goals-2022/","summary":"Evaluating where I am and deciding where I want to be next.","title":"My Reasonable Birthday Goals"},{"content":"I, like many others, recently started using Mastodon .\nIt was confusing initially, but a little exploration helped me find my bearings.\nThis article gives you a quick guide on how to get started with the platform.\nWhat is Mastodon? Mastodon is an open source, decentralized, microblogging platform that could be a viable alternative to Twitter.\nInstead of a central company (or an eccentric tech billionaire) managing the platform, Mastodon consists of multiple independent instances. Anyone can host their own instance of Mastodon and run it any way they want. The source code of Mastodon is public and maintained by open source contributors.\nAs a user, you don\u0026rsquo;t need to host your own instance. You can join any existing ones according to your niche of interest. You will still be able to follow and see posts from people on different instances (see below ).\n General and niche serversSee joinmastodon.org/servers\n  Most Mastodon servers run on donations and are not looking to make money. So, you won\u0026rsquo;t see ads or sponsored posts like on other social media.\nSigning Up You can sign up using your email on any of the available servers . You can pick your niche or go to more general servers like mastodon.online .\n Using your email to sign upI signed up on the fosstodon.org server\n  Once you fill in your details, you will receive a confirmation email with an activation link. Click on it, and you are ready to go.\n Note: A Mastodon username would be in the form @user@instance.name. For example, my username is sudo_navendu@fosstodon.org .\n Navigating the UI Once you log in, a welcome wizard will walk you through the UI.\n Homepage of the Fosstodon serverThe UI is pretty intuitive especially if you are a Twitter user\n  You can edit your profile and make it more personal by adding:\n  A bio\n  An avatar and a header image\n Adding a bio and imagesA profile without a bio or avatar might been seen as fishy\n    Profile metadata\n Adding profile metadataYou can add links to your other social accounts or websites\n    In Mastodon, posts (tweets) are called Toots. You can favorite and boost a Toot like how you heart and retweet on Twitter.\n The anatomy of a tootYou can reply to the Toot, boost it, add it to your favorites, or bookmark it\n  Home, Local, and Federated Timelines Unlike platforms like Twitter with a single timeline, Mastodon has three timelines serving different content:\n Home: This is similar to your Twitter timeline. It contains all the Toots from people you follow across all Mastodon instances (the Fediverse). Local: This contains all the public Toots from your instance. Even if you don\u0026rsquo;t follow someone, you will find their public Toots here. You can use it to discover people in your niche. Federated: This timeline contains all public Toots from all the instances your server is federated with. The best analogy to understand this is email. There are multiple email providers, and you can create an email on any of them. But you can still send and receive emails across email providers if you have their address. Similarly, the federated timeline shows Toots from instances other that your server.   TimelinesThese timelines seem pretty useful in discovering people\n  These timelines are chronological, and there is no fancy algorithm trying to keep you engaged.\nMastodon Client Apps Mastodon has an official app for Android and iOS devices. Since Mastodon is open source and has a public API , many third-party apps are also available.\n Available third-party appsI have only used the web app till now. I will try out these apps and leave a review if you are interested\n  Wrap Up That\u0026rsquo;s it for the quick start guide. You should be able to pick more things up as you use Mastodon. You can also refer the documentation to learn more.\n","permalink":"https://navendu.me/posts/mastodon-quick-start-guide/","summary":"Learn to get up and running with the microblogging platform everyone is talking about.","title":"A Quick Start Guide for Mastodon"},{"content":"Do you need a college degree in 2022 to start a career in programming?\nMany big tech companies have publicly said they don\u0026rsquo;t require a college degree for certain jobs. Along with much press , these announcements also made a lot of tech \u0026ldquo;influencers\u0026rdquo; advocating naive students not to pursue college.\nA recent survey by Stack Overflow shows that 87% of software developers have a college degree. 24% of professional developers even have a Master\u0026rsquo;s degree.\nBut the survey also shows that only 62% of developers learn to code from schools or colleges compared to 71% learning from the internet.\nWhile the argument that \u0026ldquo;you can learn everything taught in computer science college courses from the internet\u0026rdquo; is valid, does it mean they are obsolete?\nMy answer is no. I would argue that stem degrees, especially computer science degrees, can help you start a career in programming.\nPeople advocating for no degrees make a lot of arguments like:\n it costs a lot a boot camp or an online course is just as effective a lot of companies don\u0026rsquo;t care about your degrees  But these arguments are always taken out of context and are downright wrong for some audiences.\nColleges costs a lot in countries like the US, which overwhelmingly has a lot of developers. Of course, most tech content creators are from the US and would look at things from a US perspective.\nBut what about countries like India?\nYou can get a computer science engineering degree from a good (not the best) public college with around $600 in tuition . Even if you consider $2,000 for a degree in India, it is significantly much lower than in the US (around $50,000 at least).\nA new graduate in India can make at least $4000 to $5000, giving a higher ROI than for a US graduate.\nA lucrative alternative for these costly college degrees comes in the form of online courses and coding boot camps. And they do make sense in the US, where companies might actually hire people without college degrees.\nCoding boot camps and paid online courses are also a thing in India. Some of these are good and help bridge the gap between what colleges teach and the industry.\nBut will companies hire you without a degree even if you completed courses or boot camps?\nMost won\u0026rsquo;t.\nIndia has a large number of programmers. Even with most of them having a degree, getting hired is still difficult. While you can argue that skill level is a factor, there is no argument that companies will filter you out based on your college degree.\nAnd this makes complete sense. How else do you initially filter out hundreds, if not thousands, of resumes for an entry-level job?\nWith that being said, there are a lot of new startups that are hiring programmers without college degrees.\nIn my four years of working full-time, I have worked at multiple companies in India and abroad, and no one has ever asked me for my college qualifications. But behind the scenes, it may have influenced some hiring decisions.\nArguments supporting no degrees make sense when given the correct context. But understanding your context and judging whether it makes sense for you is what you have to consider.\nWhile college degrees do not imply skill and competency for a job, it is still a huge factor in hiring decisions. There will be a future where college degrees won\u0026rsquo;t matter as much, but it overwhelmingly does today.\n","permalink":"https://navendu.me/posts/college-degree/","summary":"Many companies and tech gurus claim you don\u0026rsquo;t need a college degree for a programming job. But here\u0026rsquo;s why you might want to get one regardless.","title":"You Might Need a College Degree for a Programming Job"},{"content":"This article is a part of the series \u0026ldquo;Hands-On With Apache APISIX Ingress \u0026rdquo;.\nA canary release is a process of rolling out a new version of software to a small subset of users before making it generally available. Canary releases can help in testing and controlling new releases and rolling back if there are any issues.\nA simple canary release looks like this:\n  Route all traffic to existing version of the application:\nflowchart LR u(Users) -- r(Router) -- |All Traffic| u1(Old version) r x-.-x |No Traffic| u2(New version) style r stroke: #e62129 linkStyle 1 stroke: green    Route some traffic to the new version and test if there are any bugs/issues:\nflowchart LR u(Users) -- r(Router) -- |Most Traffic 95%| u1(Old version) r -- |Some Traffic 5%| u2(New version) style r stroke: #e62129 linkStyle 1 stroke: green linkStyle 2 stroke: green    If everything is okay, route all traffic to the new version and keep the old version on standby:\nflowchart LR u(Users) -- r(Router) x-.-x |No Traffic| u1(Old version) r -- |All Traffic| u2(New version) style r stroke: #e62129 linkStyle 2 stroke: green    In this hands-on tutorial, we will set up a canary release in Kubernetes using Apache APISIX Ingress .\nBefore you move on, make sure you:\n Go through the previous tutorial for an introduction to Apache APISIX Ingress. Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install and configure kubectl to communicate with your cluster. Install Helm to deploy the APISIX Ingress controller.  Deploying a Sample Application As in the previous tutorial , we will use our sample HTTP server application, the bare-minimum-api . This will act as our versioned service:\nflowchart LR c(Clients) -- |:8080/ GET| a1(bare-minimum-api-v1) a1 -- |Hello from API v1.0!| c c -- |:8081/ GET| a2(bare-minimum-api-v2) a2 -- |Hello from API v2.0!| c  To deploy the two \u0026ldquo;versions\u0026rdquo; of the application, you can run:\nkubectl run bare-minimum-api-v1 --image navendup/bare-minimum-api --port 8080 -- 8080 v1.0 kubectl expose pod bare-minimum-api-v1 --port 8080 kubectl run bare-minimum-api-v2 --image navendup/bare-minimum-api --port 8080 -- 8080 v2.0 kubectl expose pod bare-minimum-api-v2 --port 8080 We will now deploy APISIX Ingress and set up a canary release.\nDeploying APISIX Ingress You can install APISIX and APISIX Ingress controller using Helm:\nhelm repo add apisix https://charts.apiseven.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update kubectl create ns ingress-apisix helm install apisix apisix/apisix \\  --set gateway.type=NodePort \\  --set ingress-controller.enabled=true \\  --namespace ingress-apisix \\  --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix kubectl get pods --namespace ingress-apisix Once all the pods and services are running, you can test APISIX by accessing the Admin API:\nkubectl exec -n ingress-apisix deploy/apisix -- curl -s http://127.0.0.1:9180/apisix/admin/routes -H \u0026#39;X-API-Key: edd1c9f034335f136f87ad84b625c8f1\u0026#39; If you get a response similar to the one shown below, APISIX is up and running:\n{ \u0026#34;action\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;node\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;/apisix/routes\u0026#34;, \u0026#34;dir\u0026#34;: true, \u0026#34;nodes\u0026#34;: [] }, \u0026#34;count\u0026#34;: 0 } To access the Ingress, you can run:\nminikube service apisix-gateway --url -n ingress-apisix You need to keep it running depending on your operating system. Regardless, you will see the IP address of APISIX Ingress. You can then send requests to this address.\nhttp://127.0.0.1:56194 ❗ Because you are using a Docker driver on darwin, the terminal needs to be open to run it.  Note: See the previous tutorial to learn more.\n Configuring Canary Release After verifying that APISIX Ingress is running, you can configure a canary release with APISIX\u0026#39;s CRDs .\nWe will set weights for each service to route traffic proportionately.\nInitially, we want to route all requests to the old version of the service:\nflowchart LR u(Users) -- a a -- |All Traffic| u1(\"☸ bare-minimum-api-v1\") a x-.-x |No Traffic| u2(\"☸ bare-minimum-api-v2\") ic(\"☸ APISIX Ingress controller\") --- p(\"☸ APISIX API gateway\") style p stroke: #e62129 linkStyle 0 stroke: #e62129 linkStyle 1 stroke: green linkStyle 3 stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] a u1 u2 end subgraph a[\"APISIX\"] direction TB ic p end  To configure this, we can set the weight to 100 and 0 for the bare-minimum-api-v1 and bare-minimum-api-v2 services, respectively:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: canary-release spec: http: - name: route-v1 match: paths: - /* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 100 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 0 You can apply it to your cluster by running:\nkubectl apply -f canary-release.yaml This will route all traffic to the bare-minimum-api-v1 service. You can test it out by sending a request:\ncurl http://127.0.0.1:56194/  Note: This address is the address of your APISIX Ingress obtained by running minikube service apisix-gateway --url -n ingress-apisix on the installation step .\n If you keep sending multiple requests, you will see that the response is only from bare-minimum-api-v1:\n➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! Now, you can change the configuration to route some traffic, say 5%, to the new version, bare-minimum-api-v2:\nflowchart LR u(Users) -- a a -- |Most Traffic 95%| u1(\"☸ bare-minimum-api-v1\") a -- |Some Traffic 5%| u2(\"☸ bare-minimum-api-v2\") ic(\"☸ APISIX Ingress controller\") --- p(\"☸ APISIX API gateway\") style p stroke: #e62129 linkStyle 0 stroke: #e62129 linkStyle 1 stroke: green linkStyle 2 stroke: green linkStyle 3 stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] a u1 u2 end subgraph a[\"APISIX\"] direction TB ic p end  You can configure this by editing your manifest file and applying it to your cluster:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: canary-release spec: http: - name: route-v1 match: paths: - /* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 95 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 5 APISIX will hot-reload the new configuration without needing to be restarted.\nNow, if you send requests to the Ingress controller, you will see that some of the requests (5%) are routed to the bare-minimum-api-v2 service:\n➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! ➜ curl http://127.0.0.1:56194/ Hello from API v1.0! Finally, you can route all traffic to the new version of the service.\nflowchart LR u(Users) -- a a x-.-x |No Traffic| u1(\"☸ bare-minimum-api-v1\") a -- |All Traffic| u2(\"☸ bare-minimum-api-v2\") ic(\"☸ APISIX Ingress controller\") --- p(\"☸ APISIX API gateway\") style p stroke: #e62129 linkStyle 0 stroke: #e62129 linkStyle 2 stroke: green linkStyle 3 stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] a u1 u2 end subgraph a[\"APISIX\"] direction TB ic p end  To configure this, you can set the weight to 0 for the bare-minimum-api-v1 service and 100 for the bare-minimum-api-v2 service:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: canary-release spec: http: - name: route-v1 match: paths: - /* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 0 - serviceName: bare-minimum-api-v2 servicePort: 8080 weight: 100 Now, if you send requests, you will see that all requests are routed to bare-minimum-api-v2:\n➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! ➜ curl http://127.0.0.1:56194/ Hello from API v2.0! You successfully migrated your users from the old version of your service to your new version without any downtime!\nIn real-world scenarios, you can keep the older version of the service on standby to roll back if there are any issues.\nWhat\u0026rsquo;s Next? This tutorial taught you how to configure APISIX Ingress for simple canary releases. We tested it out with our sample application.\nYou can also configure complex release strategies with APISIX and its Plugins . I will try to cover these in the following articles. To learn more about APISIX, visit apisix.apache.org .\nSee the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress \u0026rdquo;.\n","permalink":"https://navendu.me/posts/canary-in-kubernetes/","summary":"A hands-on, from-scratch tutorial on setting up canary releases in Kubernetes with Apache APISIX Ingress.","title":"Hands-On: Canary Release in Kubernetes With Apache APISIX Ingress"},{"content":"A couple of months ago, the new Kubernetes Gateway API graduated to beta .\nWhy do you need another API to handle external traffic when you have the stable Kubernetes Ingress API and dozens of implementations ? What problems of the Ingress API does the new Gateway API solve? Does this mean the end of the Ingress API?\nI will try to answer these questions in this article by getting hands-on with these APIs and looking at how they evolved.\nStandardizing External Access to Services: The Ingress API The Kubernetes Ingress API was created to standardize exposing services in Kubernetes to external traffic. The Ingress API overcame the limitations of the default service types, NodePort and LoadBalancer, by introducing features like routing and SSL termination.\nflowchart LR c(Clients) -- i(\"☸ Ingress\") i -- s1(\"☸ Service 1\") i -- s2(\"☸ Service 2\") i -- s3(\"☸ Service 3\") style i stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] i s1 s2 s3 end  There are over 20 implementations of Ingress controllers available. In this article, I will use Apache APISIX and its Ingress controller for examples.\nflowchart LR c(Clients) -- a a -- s1(\"☸ Service 1\") a -- s2(\"☸ Service 2\") a -- s3(\"☸ Service 3\") ic(\"☸ APISIX Ingress controller\") --- p(\"☸ APISIX API gateway\") style p stroke: #e62129 linkStyle 0 stroke: #e62129 linkStyle 1 stroke: #e62129 linkStyle 2 stroke: #e62129 linkStyle 3 stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] a s1 s2 s3 end subgraph a[\"APISIX\"] direction TB ic p end  You can create an Ingress resource to configure APISIX or any other Ingress implementations.\nThe example below shows how you can route traffic between two versions of an application with APISIX Ingress:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api-v1 port: number: 8080 path: /v1 pathType: Prefix - backend: service: name: bare-minimum-api-v2 port: number: 8081 path: /v2 pathType: Prefix  Tip: You can check out this hands-on tutorial to learn more about setting up Ingress on Kubernetes with Apache APISIX Ingress controller.\n Since the Ingress API is not tied to any particular controller implementation, you can swap APISIX with any other Ingress controller, and it will work similarly.\nThis is okay for simple routing. But the API is limited, and if you want to use the full features provided by your Ingress controller, you are stuck with annotations .\nFor example, the Kubernetes Ingress API does not provide a schema to configure rewrites. Rewrites are useful when your upstream/backend URL differs from the path configured in your Ingress rule.\nAPISIX supports this feature, and you have to use custom annotations to leverage it:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes annotations: k8s.apisix.apache.org/rewrite-target-regex: \u0026#34;/app/(.*)\u0026#34; k8s.apisix.apache.org/rewrite-target-regex-template: \u0026#34;/$1\u0026#34; spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api port: number: 8080 path: /app pathType: Prefix This creates an Ingress resource that configures APISIX to route any requests with the /app prefix to the backend with the prefix removed. For example, a request to /app/version will be forwarded to /version.\nAnnotations are specific to your choice of an Ingress controller. These \u0026ldquo;proprietary\u0026rdquo; extensions limited the scope of portability intended initially with the Ingress API.\nCustom CRDs \u0026gt; Ingress API Being stuck with annotations also sacrifice the usability of the Ingress controllers.\nControllers therefore solved the limitations of the Ingress API by creating their own custom resources . The example below shows configuring Ingress to route traffic between two versions of an application using APISIX\u0026rsquo;s custom resource:\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: api-routes spec: http: - name: route-1 match: hosts: - local.navendu.me paths: - /v1 backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 - name: route-2 match: hosts: - local.navendu.me paths: - /v2 backends: - serviceName: bare-minimum-api-v2 servicePort: 8081 These CRDs made it much easier to configure Ingress, but you are tied to the specific Ingress control implementation. Without the Ingress API evolving, you had to choose between usability or portability.\nExtending Ingress and Evolution to Gateway API Ingress API was not broken; it was limited. The Gateway API was designed to overcome these limitations.\n (Gateway API) aim to evolve Kubernetes service networking through expressive, extensible, and role-oriented interfaces \u0026hellip;\n gateway-api.sigs.k8s.io  What is the Gateway API?    It takes inspiration from the custom CRDs of different Ingress controllers mentioned earlier.\nThe Gateway API adds many features \u0026ldquo;on top\u0026rdquo; of the Ingress API\u0026rsquo;s capabilities. This includes HTTP header-based matching, weighted traffic splitting, and other features that require custom proprietary annotations with the Ingress API.\nTraffic split with APISIX Ingress resource (see ApisixRoute/v2 reference ):\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: traffic-split spec: http: - name: rule-1 match: hosts: - local.navendu.me paths: - /get* backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 weight: 90 - serviceName: bare-minimum-api-v2 servicePort: 8081 weight: 10 Traffic split with Gateway API (see Canary traffic rollout ):\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: traffic-split spec: hostnames: - local.navendu.me rules: - backendRefs: - name: bare-minimum-api-v1 port: 8080 weight: 90 - name: bare-minimum-api-v2 port: 8081 weight: 10 Another improvement from the Ingress API is how the Gateway API separates concerns . With Ingress, the application developer and the cluster operator work on the same Ingress object, unaware of the other\u0026rsquo;s responsibilities and opening the door for misconfigurations.\nThe Gateway API separates the configurations into Route and Gateway objects providing autonomy for the application developer and the cluster operator. The diagram below explains this clearly:\nflowchart TB gc(\"GatewayClass\\n\\n👷🏼 👷🏾‍♀️\\nInfrastructure\\nprovider\") ---- g(\"Gateway\\n\\n👩🏻‍🔧 👨🏿‍🔧\\nCluster\\noperator\") g --- hr1(\"HTTPRoute\\n\\n👨🏾‍💻 Application developer\") --- s1(Service) g --- hr2(\"HTTPRoute\\n\\n👩🏼‍💻 Application developer\") --- s2(Service) click gc href \"https://gateway-api.sigs.k8s.io/api-types/gatewayclass/\" _blank click g href \"https://gateway-api.sigs.k8s.io/api-types/gateway/\" _blank click hr1 href \"https://gateway-api.sigs.k8s.io/api-types/httproute/\" _blank click hr2 href \"https://gateway-api.sigs.k8s.io/api-types/httproute/\" _blank subgraph bar[\"Namespace bar\"] hr1 s1 end subgraph foo[\"Namespace foo\"] hr2 s2 end  Is This the End of Ingress API? The Gateway API is relatively new, and its implementations are constantly breaking. On the contrary, the Ingress API is in stable release and has stood the test of time.\nIf your use case only involves simple routing and if you are okay with using custom annotations to get extra features, the Ingress API is still a solid choice.\nWith the Gateway API being a superset of the Ingress API, it might make sense to consolidate both. Thanks to the SIG Network community, Gateway API is still growing and will soon be production ready.\nMost Ingress controllers and service meshes have already implemented the Gateway API along with the Ingress API, and as the project evolves, more implementations will surface.\nPersonally, at least for now, I would stick with custom CRDs provided by the Ingress controllers like Apache APISIX instead of the Ingress or Gateway API.\n","permalink":"https://navendu.me/posts/gateway-vs-ingress-api/","summary":"Exploring the new Kubernetes Gateway API and comparing it with the existing Kubernetes Ingress API for handling external traffic.","title":"Comparing Kubernetes Gateway and Ingress APIs"},{"content":"I have been contributing to open source projects for the past three years. During this time, I have built, scaled, and maintained my own projects and projects in foundations, including CNCF and the ASF. This is my story and the lessons I learned along the way.\nFirst Contributions My first open source contributions were along the lines of \u0026ldquo;push everything you build to GitHub\u0026rdquo;. I was naive and thought pushing the toy projects I built to learn could help me build my resume.\nThese did help my resume at the time, but they were far from valuable open source contributions.\nAfter some time, I was a good enough developer to bring my ideas to fruition. I built a browser extension called NSFW Filter , which used deep learning to filter out NSFW content from the web.\nIt was a simple idea, but there wasn\u0026rsquo;t anything similar in the market. The project became popular and featured on Product Hunt and Hacker News , bringing in many users and contributors to the project.\n Featured on Product HuntI literally googled \u0026ldquo;how to market open source project\u0026rdquo; and found these platforms\n   Front page of Hacker NewsBack then I did not know how cool it was to be on the front page of HN\n  Solve Your Problems, Delegate Solve the problem you are facing and make it open source. Chances are more people face your exact problem and will be your users.\nEven if your project is entirely free and open source, you need to market it to get users. You don\u0026rsquo;t need a marketing team or money to find channels and promote your project. And if your project is valuable, it will market itself after some time.\nOnce your project is big, it might put a toll on you to maintain it yourself. This is when you have to start delegating and get more people to help you.\nIt might be weird at first to give someone else push access to your project, but it is the only way to scale the project and contributions to the project.\n Contributors to NSFW FilterThese people are the best\n  Mentoring Unlike many other things, being an open source contributor did not seem to have any roadmaps. So, when it came to contributing to other open source projects, I was confused. Which project do I contribute to? Where do I start? What if I\u0026rsquo;m not skilled enough?\nAs I sat there puzzled by these questions, I stumbled across the Linux Foundation Mentorship Program .\nThe LFX Mentorship Program provides opportunities for mentees to work on open source projects. Experienced project contributors and maintainers will be the program\u0026rsquo;s mentors. Mentees will also receive a stipend during their term in the program.\n LFX Mentorship Program homepageFinding about this program was life changing\n  I found a lot of open source projects that wanted contributors. It showed the skills they were looking for and what kind of project it will be. The program gave me insight into open source projects and communities. I applied to the program, and after a couple of months of contributing to a project, I was selected as a mentee.\nI can speak a lot about how LFX helped me , but to sum it up, the program set me on the path to working in open source.\n My LFX mentee profileI applied to three projects but did not try to work on the other two. Here’s my profile\n  Start Small, Find Mentors You can always prepare before you start contributing to open source (or doing anything in general). You can skill up, read all the right books, and watch all the right tutorials.\nBut the only way to progress is to start.\nYou can start small, but you have to start somewhere. In open source contributions, you can start by being a project user and raising issues as you find them. You can hang out in community meetings and provide feedback to the project contributors. You can sign up for alpha or beta programs and help remove bugs. All of these are low-hanging fruits ready to be plucked.\nIt can also be daunting to publish your code for the whole world to see. And yes, you will write a lot of bad code when you start. But contributing to open source ensures that your code is reviewed and improved each time.\nTo help navigate the open source waters, you can find mentors. Open source mentorship programs like LFX are a great way to find mentors if you are a student or a new developer.\nFrom my experience as a maintainer and a mentor, newcomers often find bugs that seasoned contributors often miss. Sometimes we are too close to the project to see the mistakes. So, new contributors are always welcome!\nMaintainer! Maintainer! Maintainer! At one point, I spent all my time on open source projects. I contributed code, helped design new features, and managed contributors. It did not take long for my fellow maintainers to nominate me formally to their team.\n Open source contributionsI don\u0026rsquo;t see a clear distinction of the time when I switched to writing less code and being more of a maintainer though\n  As a maintainer, I spent less time writing code and more time in system design, making decisions, and managing contributors. I still looked at code, but it was mostly during reviews.\nContributions Don\u0026rsquo;t Have to be Code A lot of open source does not involve code. These non-code contributions can involve writing documentation, creating tutorials, giving talks about the project at conferences, designing content, organizing events and meetings, managing the community, and more. The list is endless.\nOpen source is the default way to build software now. And non-code contributions help create, manage, and sustain these projects.\nBeing a Maintainer is Hard Being an open source maintainer is a job that never ends. It isn\u0026rsquo;t easy to separate from your work when it is public and a part of you.\nI spent a lot of time working insane hours because the work wouldn\u0026rsquo;t stop. This took a toll on my health and personal life, and I made an effort to set boundaries.\nIt is okay to take breaks, and it is important to take breaks.\nCommunity Over Code Open source projects are a byproduct of its community.\nBuilding and sustaining communities are essential as they lead to better projects.\nSo, welcome new community members, and lower the barrier to entry. Take active measures to build and maintain the community.\nFull-time Open Source Today I work in open source full-time. I contribute to Apache APISIX and lead initiatives to grow the project and community. If I had a dream job, this would be it. I know because I used to do this job for free before someone decided to pay me for it.\nPeople helped me when I wrote bad code, when I asked stupid questions, and when I was a beginner. Today, I pay it forward by being a mentor in open source mentorship programs.\n My LFX mentor profileYou can see the transition on my profile\n  Contributing is Rewarding It is rewarding, and it\u0026rsquo;s more than the money.\nBeing an open source contributor makes you part of something bigger than yourself or your company. And your contributions can have a significant impact on the world around you.\n Speaking at Open Source Summit EuropeTalking about how open source has helped me in my career\n  So if you are thinking of contributing to open source, do it. If you know an underfunded open source project that you rely on, sponsor it. If you see an open source alternative, promote it.\n","permalink":"https://navendu.me/posts/open-source-lessons/","summary":"Insights from my three year journey as an open source contributor.","title":"Lessons Learned From Three Years of Open Source Contributions"},{"content":"When thinking about open source contributions, most people think about contributing code. This was true in the early days when open source was a way for people to share the cool thing they built.\nNow, as open source becomes the default way to build software, contributions required to create, manage, and sustain projects go beyond code.\nIn this article, you will learn how to make impactful contributions to open source that does not involve contributing code.\nWriting This might be an excellent way to contribute if you are good at technical writing and creating content.\nYou can use your writing skills in different ways.\nDocumentation People can\u0026rsquo;t use what they don\u0026rsquo;t understand.\nEven if the project is valuable, people would not be able to use it without proper documentation. Contributing to documentation can therefore be impactful.\n Tip: When you first test out a project, go through the existing documentation and see if you can clearly understand and use the project without any issues.\n Chances are you will find areas to improve. You can then open issues for it and fix them.\nIt\u0026rsquo;s always helpful to have new people go through the documentation. It will uncover missing information and issues that people close to the project might miss.\nArticles and Tutorials If you are a project user, you can help by writing articles and tutorials. This can help people who are trying to use the project.\nYou can add your insights and tips and publish them on your channels or the project\u0026rsquo;s blog.\n Tip: Most people look for well-written and up-to-date tutorials instead of documentation.\n Internationalization/Translation The language barrier is real in countries where English is not a primary language. People in China may prefer Mandarin over English.\nFor people from different parts of the world to use a project, it is necessary to internationalize the software and its documentation.\nIf you can write in two languages, you can translate documentation to open up the project to a new user base.\nSocial Media A social media presence can help a project get more users. It can also be a platform to share updates with existing users.\nYou can use your writing skills to craft social media posts. You can ask the project maintainers to post on their official account or use your own account.\nDesigning You can also contribute to open source projects with your design skills.\nArt Create artwork for social media, blog posts, and even swags. Good design is always impactful.\nStyle Guide Most open source projects have a large and geographically distributed set of contributors.\nAmong other challenges, this makes it difficult to maintain consistency in visual designs.\nAs a designer, you can create a style guide for the project and ensure consistency.\nTesting/Using You can make valuable contributions in multiple ways as an open source user.\nReporting Bugs Be a needy user. If you run into bugs, raise them. Open an issue with all the relevant details and steps to reproduce it.\nIdentifying bugs is more difficult than fixing them. Maintainers always welcome bug reports.\nAdvocating Open source projects are community driven. And most non-commercial open source projects don\u0026rsquo;t have a dedicated marketing team to publicize the project.\nYou can advocate for the project at events and on social media and encourage people to use the project.\n Tip: Don\u0026rsquo;t even ask the project owners before advocating for the project. Everyone likes free marketing.\n User Experience Project maintainers can often be too close to the project to realize bad and unintuitive UX. I am, and I know a lot of them.\nAs a user, you can report these issues, and they are well received.\nAlpha/Beta Testing Alpha/Beta tests are controlled tests of a new feature or release to ensure quality and user experience before making it available to the general user base.\nAs a user, you can sign up for the alpha/beta programs, test the project before the new features are released, and provide feedback.\nFeedback from these tests always provides insights that can help improve the features/releases.\nCommunity Managing An open source project is a byproduct of its community. But who builds and manages these communities?\nThat\u0026rsquo;s where a community manager comes in. As a community manager, you can wear different hats.\nProject Organiser Stale issues? Follow up! Issues without proper labels? Add them! Does this issue still exist? Verify and close them! Unclear issue descriptions? Ask for clarification! Unreviewed pull requests? Request for reviews!\nAll these help the project run smoothly.\nRelease Manager A release manager keeps track of what everybody is working on and ensures that a project is ready for a release.\nSome of the responsibilities of a release manager are:\n Checking with each team Ensuring different components and features are tested Organizing the alpha/beta programs  Event Organizer Organize community events and project meetings and maybe represent the project at conferences.\nGreat community managers who go above and beyond are key players in the success of an open source project.\nMentoring If you are a seasoned contributor, you can pay it forward to the community by being a mentor.\nCode Review Having more people review code can help improve the quality of the code. This means fewer bugs, faster reviews, and a better project.\nMost projects allow anyone to review code.\nHelping Newcomers Sharing your skills and experience as a contributor can help newcomers contribute to the project. There are a lot of open source mentorship programs that connect mentees to mentors.\nHaving new contributors will ensure the sustainability of the project.\n This list is not exhaustive and you can always find more ways to make impactful contributions.\nAs an open source maintainer, I see the value in every contribution. And I\u0026rsquo;m sure more maintainers see it too.\nIf you found this helpful, checkout related posts .\n","permalink":"https://navendu.me/posts/non-code-contributions-to-open-source/","summary":"As open source becomes the default way to build software, contributions required to create, manage, and sustain projects go beyond code. This article teaches you to make non-code contributions.","title":"Non-Code Contributions to Open Source"},{"content":"I\u0026rsquo;ve been writing blogs for almost three years now. Recently, I\u0026rsquo;ve been putting a lot of effort into building and maintaining my blog. This article documents my blog setup and my writing process from idea to publishing.\nLet\u0026rsquo;s first get the nerd stuff out of the way and look at how my blog is set up.\nThe Setup: Hugo, GitHub, and Netlify I use Hugo , the static site generator, to build my blog. Hugo is fast and heavily customizable with reasonable defaults. It handles a lot of boilerplate stuff, so people like me can focus on the content.\nI\u0026rsquo;m awful at building UI from scratch, so I use a theme. I have added some custom features and changed the look slightly for it to be more me.\nI like the way the site looks. I talked to my readers about the look and feel of the site, and they seem to like the minimalist approach.\nI push this site\u0026rsquo;s code and content to a GitHub repository . This repository is open source. When adding new posts, I usually make a pull request . This is to trigger a preview build that other reviewers and I can verify and comment on.\nPreview builds? What is that? That\u0026rsquo;s where Netlify does its magic.\nI use Netlify to deploy my site. I\u0026rsquo;m on the free tier, and it seems to be enough. But I will use it even if I have to pay for it.\nNetlify watches for pushes to my main branch and triggers a deployment. It also has a deploy preview feature which builds a preview for the website when you open a pull request against the main branch.\nSo, after writing a new post, I make a pull request, and Netlify builds a preview. Reviewers can make suggestions on the PR before merging it and publishing it on the site.\n Deploy previewsDeploy previews make my life much easier. See comment\n   Reviewing draft postsI also love my diagrams! See comment\n  Netlify has many more features like DNS management and A/B testing, but I haven\u0026rsquo;t tried them till now.\nThe Tools: VS Code, Shortcodes, hbt I\u0026rsquo;m only as good as my tools. I\u0026rsquo;m much faster and write much easier with these tools.\nVisual Studio Code is my CMS. I write on the code editor, and it feels natural.\nI\u0026rsquo;ve tried different open source CMS platforms, but none seem easy to migrate to. I have a non-trivial blog setup, making it non-trivial to migrate to these platforms. And I\u0026rsquo;m lazy to put in the work to make the switch.\nBut, to make my life easier, I use some tools and configurations.\nMy site contains two types of posts; the regular blog posts shown on the homepage and daily logs . Each of these posts has a custom front matter. I have configured archetypes to create new files with these front matters easily.\nThis makes it easy to create new posts. A new file is created and configured; I just have to worry about the content.\nBut the coolest configurations I have are custom shortcodes and snippets . Hugo supports shortcodes which are placeholders for custom templates. It lets me add images, code, quotes, and even raw HTML directly into markdown files which will be appropriately formatted on render.\n Custom shortcodes in HugoThis shorcode is for creating diagrams using Mermaid\n  To use these shortcodes easily, I have configured custom snippets on VS Code to add shortcodes with appropriate tab stops automatically.\n Custom snippets in VS CodeNotice how I can just hit TAB and it jumps to the next configuration\n  But, there are limits to what I can achieve with VS Code alone. So, I built a custom CLI to create files to write content and folders to store static content. I call it hbt—Hugo Blog Tool .\n hbt—Hugo Blog ToolIt ain\u0026rsquo;t much, but it\u0026rsquo;s honest work\n  It creates new files with the correct front matter and matching folders to store images—for now. I plan to add more features to it as I need them in the future. It is written in Go.\nWith this setup and tools, I start writing.\nThe Process: Eliciting, Planning, and Writing As I mentioned before, I write two types of posts on this blog. The regular posts on the homepage and daily logs.\nFor daily logs, I keep a blank page open on my computer from the start of the day. I write on it and add things I find during the day. By the end of the day, I will have some rough content which I rewrite crudely to form a daily log.\nI want to write it every day, but I have learned to give myself some slack if I have other important things to do.\nFor my regular blogs, I follow a process:\n First, I find topics to write about. When I find one, I immediately add it to my notes. I have been doing this for a few months and have a lot of topics in hand. So, if I want to write, I go through my collection and figure out what I want to write about. Once I have a topic, I create a page for it and add points. It would just be a dump of my thoughts which I will articulate when I actually start writing the post. I return to this dump to dump more ideas as I get them. I have a calendar planned for two weeks ahead. I plan what to write about and when to publish it. Currently, I post a new article every Friday and a newsletter issue every other Friday. So, I have two weeks worth of posts and newsletters ready or almost ready. When writing, I articulate all my thoughts from the dump and try to connect ideas. Once I write the article, I rewrite it on VS Code. Yes, I type it again. This helps me find errors and review the post before publishing. Once I\u0026rsquo;m done, I open a pull request to my repo which will trigger the deploy preview, and I will share the preview with my reviewers.   Writing a lot has helped me improve my writing. It has helped me develop my style and express my thoughts.\nTurning writing into a process may seem like it hinders creativity. But it is quite the opposite. When you are in the process, you will get conditioned to write. When you write, you write. It is much easier than winging it, especially if you are starting.\nI will try to keep this post updated as my writing process and setup evolve.\n","permalink":"https://navendu.me/posts/my-blog-setup-and-writing-process/","summary":"I\u0026rsquo;ve been writing blogs for almost three years now. Recently, I\u0026rsquo;ve been putting a lot of effort into building and maintaining my blog. This article documents my blog setup and my writing process from idea to publishing.","title":"My Blog Setup and Writing Process"},{"content":"This article is a part of the series \u0026ldquo;Hands-On With Apache APISIX Ingress \u0026rdquo;.\nIn Kubernetes, Ingress is a native object that allows you to access your services externally by defining a set of rules. Using a reverse proxy, an Ingress controller implements these defined rules and routes external traffic to your services.\nflowchart LR c(Clients) -- i(\"☸ Ingress\") i -- s1(\"☸ Service 1\") i -- s2(\"☸ Service 2\") i -- s3(\"☸ Service 3\") style i stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] i s1 s2 s3 end  Apache APISIX is an open source API gateway (a souped-up reverse proxy) that provides features like authentication, traffic routing, load balancing, canary releases, monitoring, and more. APISIX also supports custom Plugins and integrates with popular open source projects like Apache SkyWalking and Prometheus . To learn more about APISIX, you can see the official documentation .\nThe Apache APISIX Ingress controller sits between the defined Ingress rules and the APISIX API gateway. It configures the proxy to route traffic based on the defined rules.\nflowchart LR c(Clients) -- a a -- s1(\"☸ Service 1\") a -- s2(\"☸ Service 2\") a -- s3(\"☸ Service 3\") ic(\"☸ APISIX Ingress controller\") --- p(\"☸ APISIX API gateway\") style p stroke: #e62129 linkStyle 0 stroke: #e62129 linkStyle 1 stroke: #e62129 linkStyle 2 stroke: #e62129 linkStyle 3 stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] a s1 s2 s3 end subgraph a[\"APISIX\"] direction TB ic p end  This hands-on tutorial will teach you how to set up the APISIX Ingress controller on your Kubernetes cluster and route traffic to your services.\nBefore you move on, make sure you:\n Have access to a Kubernetes cluster. This tutorial uses minikube for creating a cluster. Install and configure kubectl to communicate with your cluster. Install Helm to deploy the APISIX Ingress controller.  Deploying a Sample Application We will use a sample HTTP server application (bare-minimum-api ) to demonstrate the working of the Ingress controller.\nWhile running the application, you can set a \u0026ldquo;version\u0026rdquo; and a port to listen to. For this example, we will create two \u0026ldquo;versions\u0026rdquo; of this application which will return different responses as shown below:\nflowchart LR c(Clients) -- |:8080/ GET| a1(bare-minimum-api-v1) a1 -- |Hello from API v1.0!| c c -- |:8081/ GET| a2(bare-minimum-api-v2) a2 -- |Hello from API v2.0!| c  You can deploy the application on your Kubernetes cluster by running:\nkubectl run bare-minimum-api-v1 --image navendup/bare-minimum-api --port 8080 -- 8080 v1.0 kubectl expose pod bare-minimum-api-v1 --port 8080 To test the application outside the cluster, you can use port-forward:\nkubectl port-forward bare-minimum-api-v1 8080:8080 Now, if you open up a new terminal window and run:\ncurl http://127.0.0.1:8080 You will get back a response from the application:\nHello from API v1.0! Similarly, you can deploy another \u0026ldquo;version\u0026rdquo; of the application by running:\nkubectl run bare-minimum-api-v2 --image navendup/bare-minimum-api --port 8081 -- 8081 v2.0 kubectl expose pod bare-minimum-api-v2 --port 8081 Now, we can deploy APISIX Ingress and expose these applications to external traffic.\nDeploying APISIX Ingress APISIX and APISIX Ingress controller can be installed using Helm:\nhelm repo add apisix https://charts.apiseven.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update kubectl create ns ingress-apisix helm install apisix apisix/apisix \\  --set gateway.type=NodePort \\  --set ingress-controller.enabled=true \\  --namespace ingress-apisix \\  --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix kubectl get pods --namespace ingress-apisix  Note: We are using NodePort as the Gateway service type. You can also set it to LoadBalancer if your cluster has one.\n Helm will create five resources in your cluster:\n apisix-gateway: The data plane that handles external traffic. apisix-admin: Control plane that processes configuration changes. apisix-ingress-controller: The ingress controller. apisix-etcd and 5. apisix-etcd headless: To store configuration and handle internal communication.  Once all the pods and services are running, you can test APISIX by accessing the Admin API:\nkubectl exec -n ingress-apisix deploy/apisix -- curl -s http://127.0.0.1:9180/apisix/admin/routes -H \u0026#39;X-API-Key: edd1c9f034335f136f87ad84b625c8f1\u0026#39; If you get a response similar to the one shown below, APISIX is up and running:\n{ \u0026#34;action\u0026#34;: \u0026#34;get\u0026#34;, \u0026#34;node\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;/apisix/routes\u0026#34;, \u0026#34;dir\u0026#34;: true, \u0026#34;nodes\u0026#34;: [] }, \u0026#34;count\u0026#34;: 0 } Configuring APISIX Ingress Once you have verified that the APISIX gateway and Ingress controller is running, you can create Routes to expose the deployed application to external traffic.\nThis will route traffic between the two application versions based on the client request:\nflowchart LR c(Client)  |\"Req: /v1 GET\\nResp: Hello from API v1.0!\"| a a -- |Route| s1(\"☸ bare-minimum-api-v1\") a -- s2(\"☸ bare-minimum-api-v2\") ic(\"☸ APISIX Ingress controller\") --- p(\"☸ APISIX API gateway\") style p stroke: #e62129 linkStyle 0 stroke: #e62129 linkStyle 1 stroke: #e62129 linkStyle 3 stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] a s1 s2 end subgraph a[\"APISIX\"] direction TB ic p end  flowchart LR c(Client)  |\"Req: /v2 GET\\nResp: Hello from API v2.0!\"| a a -- s1(\"☸ bare-minimum-api-v1\") a -- |Route| s2(\"☸ bare-minimum-api-v2\") ic(\"☸ APISIX Ingress controller\") --- p(\"☸ APISIX API gateway\") style p stroke: #e62129 linkStyle 0 stroke: #e62129 linkStyle 2 stroke: #e62129 linkStyle 3 stroke: #e62129 subgraph k[\"☸ Kubernetes cluster\"] a s1 s2 end subgraph a[\"APISIX\"] direction TB ic p end  To configure Routes, APISIX comes with declarative and easy-to-use custom resource :\napiVersion: apisix.apache.org/v2beta3 kind: ApisixRoute metadata: name: api-routes spec: http: - name: route-1 match: hosts: - local.navendu.me paths: - /v1 backends: - serviceName: bare-minimum-api-v1 servicePort: 8080 - name: route-2 match: hosts: - local.navendu.me paths: - /v2 backends: - serviceName: bare-minimum-api-v2 servicePort: 8081 The APISIX Ingress controller converts this resource to an APISIX gateway configuration.\nAPISIX also supports configuration using native Kubernetes Ingress resource :\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: api-routes spec: ingressClassName: apisix rules: - host: local.navendu.me http: paths: - backend: service: name: bare-minimum-api-v1 port: number: 8080 path: /v1 pathType: Exact - backend: service: name: bare-minimum-api-v2 port: number: 8081 path: /v2 pathType: Exact You can use either to configure APISIX but I prefer the easier APISIX custom resource. We can apply this manifest file to our cluster to create Routes in APISIX:\nkubectl apply -f apisix-ingress-manifest.yaml If the Ingress controller is configured correctly, you should see a response indicating that APISIX API gateway has been configured:\napisixroute.apisix.apache.org/api-routes created Now, let\u0026rsquo;s test these Routes.\nTesting the Created Routes If you were following along using minikube and NodePort, you should be able to access APISIX through the Node IP of the service apisix-gateway. If the Node IP is not reachable directly (if you are on Darwin, Windows, or WSL), you can create a tunnel to access the service on your machine:\nminikube service apisix-gateway --url -n ingress-apisix This will show the URL with which you can access the apisix-gateway service.\nYou can send a GET request to this URL and it would be Routed to the appropriate service:\ncurl http://127.0.0.1:51538/v2 -H \u0026#39;host:local.navendu.me\u0026#39; Hello from API v2.0! Now you have APISIX routing traffic to your applications! You can try the two configured Routes and see APISIX routing the requests to the appropriate application.\nWhat\u0026rsquo;s Next? In this tutorial, you learned to set up APISIX Ingress on your cluster. We tested it out by configuring basic Routes to a sample application.\nWith APISIX gateway and the Ingress controller, you can also configure Upstreams, Plugins, mTLS, and monitoring. To learn more about APISIX and how you can use these features, visit apisix.apache.org .\nSee the complete list of articles in the series \u0026ldquo;Hands-On With Apache APISIX Ingress \u0026rdquo;.\n","permalink":"https://navendu.me/posts/hands-on-set-up-ingress-on-kubernetes-with-apache-apisix-ingress-controller/","summary":"A tutorial on using Ingress in your Kubernetes cluster with Apache APISIX.","title":"Hands-On: Set Up Ingress on Kubernetes With Apache APISIX Ingress Controller"},{"content":"Contributing to open source is nerve-wracking, especially if you are a beginner.\nA couple of years ago, I was this nervous beginner. I had no work experience as a software engineer, but I was determined to learn by contributing to open source projects.\nBut which project do I contribute to? Where do I start? What if I\u0026rsquo;m not skilled enough?\nAs I sat puzzled by these questions, I stumbled across the Linux Foundation Mentorship Program .\nThe LFX Mentorship Program provides opportunities for mentees to work on open source projects. Experienced project contributors and maintainers will be the program\u0026rsquo;s mentors. Mentees will also receive a stipend during their term in the program.\nI got selected as a mentee, received a full-time offer after the program, and returned to the program as a mentor.\nToday, I work in open source full-time.\nIn this article, I will share how I got involved in the LFX Mentorship Program and its life-changing impact on my career and personal life.\nStumble Beginnings After learning about the program on YouTube, I landed on the LFX Mentorship homepage . There, I found the answers to my questions.\nI found the list of open source projects participating in the program. It showed me what the project is about, what skills they expect mentees to have, and everything else I need to get started.\nI got interested in the Meshery project and joined its community. There were many helpful people there, and I started making small contributions. In a couple of months, I was learning new skills and making more impactful contributions.\nI applied to be an LFX mentee for the project and was accepted. I was already contributing to the project, and being part of the program was a bonus.\n My LFX mentee profileI applied to three projects but did not try to work on the other two. Here\u0026rsquo;s my profile\n  Levelling Up During the program, I developed skills to work on cloud native technologies. I learned to program in Go, built applications on Docker and Kubernetes and learned more about the ecosystem.\nI also improved my communication skills by working in a global, async team. I was writing design specifications, user-facing documentation, and blog posts.\nThe program also opened a lot of opportunities to speak at international conferences, work with maintainers from top tech companies, and interact with other open source communities.\nI gained the confidence to contribute to open source projects. Maybe it can be my career!\nFull-Time Open Sourcerer Making me realize that contributing to open source projects can be a career option was the most significant impact the LFX Mentorship Program had on me.\nAfter the program, I received a job offer to work on Meshery and related projects.\n🧙‍♂️ Getting paid to work on open source projects was (and is) the best. To this date, I don\u0026rsquo;t feel like I\u0026rsquo;m working at all. I\u0026rsquo;m doing things I like to do, and apparently, someone thinks it\u0026rsquo;s worth paying me for it.\nPaying it Forward The open source community is about paying it forward.\nWhen I started as an open source contributor, a lot of people helped me. They answered my noob questions, corrected mistakes in my code, and guided me through projects and communities.\nSo, after being promoted to an open source maintainer, I joined back in the LFX Mentorship Program. This time, as a mentor.\n My LFX mentor profileYou can see the transition on my profile\n  I learned a lot by being a mentor. I developed skills as a leader, and it opened up more career opportunities.\nLFX, the Catalyst The effect programs like LFX have on the open source ecosystem is monumental. As open source becomes the default way to build software, these programs help foster contributions from an ever-growing set of newcomers. And contributions from these newcomers ensure sustainability.\nOn the surface, the LFX Mentorship Program can look like any other internship program. But for someone without access to such opportunities because of where they are in the world, the program is highly impactful.\nAnd based on the stats, more people are applying to the program and actively participating in open source. This, in turn, drives organizations to make more of their code free and open source.\nSo, apply to the program if you are interested in contributing to open source, especially if you are a student. If you are a seasoned contributor, participate as a mentor and pay it forward.\nYou can also check out other programs like LFX in 20\u0026#43; Open Source Internship Programs that you can Apply to .\n","permalink":"https://navendu.me/posts/how-the-lfx-mentorship-program-helped-me-level-up-my-career/","summary":"Sharing how I got involved in the LFX Mentorship Program and its life-changing impact on my career and personal life.","title":"How the LFX Mentorship Program Helped Me Level-Up My Career"},{"content":"As your APIs scale, the need for making them reliable and robust increases.\nThis article discusses the best practices for building reliable APIs by introducing a special kind of reverse proxies called API gateways.\nWe will look into:\n Problems with traditional API designs What API gateways are How API gateways improve APIs and Patterns and examples using API gateways  But first, what are \u0026ldquo;reliable\u0026rdquo; APIs?\nWhat Makes an API Reliable? As a service provider, you might have service-level agreements (SLAs) with your customers, usually quoted in uptime—the amount of time the service is guaranteed to be online and operational.\nUptime is a myopic view of reliability. To understand what it means to be reliable, you have to look at the factors that affect uptime. Once you understand these factors, you will be in a better position to build reliable services.\nLet\u0026rsquo;s look at these factors and the questions they pose:\n Latency: How fast does your API respond to requests? Security: Who can access your API? Is it secure? Downtime Frequency: How frequently is your API down? Consistency: Are your API endpoints constant? Do consumers need to change their code often? Monitoring and Reporting: Can you observe issues and failures in your API? Are you reporting them to your consumers?  flowchart LR c1(Web App) -- |Consistency| m1(Service 1) c1 -- m2(Service 2) c2(iOS App) -- |Latency| m1 c2 -- |Downtime| m3(Service 3) c2 -- m4(Service 4) c3(Android App) -- |Security| m2 c3 -- |Monitoring| m4 subgraph Clients c1 c2 c3 end subgraph API m1 m2 m3 m4 end  As organizations move to cloud native architectures, it becomes difficult for the development teams to account for these factors on each of their services. And as these systems scale, it would be much easier to delegate these responsibilities to a single, separate system. Say hello to API gateways!\nAPI Gateway, the Unified Entrypoint An API gateway acts as a middleman between your clients and your APIs. It will accept all traffic (API calls) like reverse proxies, forwards the request to the required services in your backend, and returns the needed results.\nflowchart LR c1(Web App) --- ag(API Gateway) c2(iOS App) --- ag c3(Android App) --- ag ag --- m1(Service 1) ag --- m2(Service 2) ag --- m3(Service 3) ag --- m4(Service 4) style ag stroke: #e62129 subgraph Clients c1 c2 c3 end subgraph API m1 m2 m3 m4 end  An API gateway can be the central point that handles all the authentication, security, traffic control, and monitoring concerns, leaving the API developers to focus on business needs and making it easier to improve reliability.\nflowchart LR subgraph ag[API Gateway] f1(Authentication, Security, Monitoring, Traffic Control) end c(Client) -- f1 -- a(API/Upstream)  There are a lot of open source and managed API gateway offerings available. In this article, I will be using Apache APISIX .\nThe following section will describe some of the best practices to make your APIs reliable using API gateways.\nReliability Best Practices with API Gateways We will focus more on the pattern underneath than the actual implementation, as it can vary based on your API gateway choice.\nI will divide these patterns into three categories:\n Authentication and security Monitoring and observability Version control and zero downtime  We will look into each category in detail below.\nAuthentication and Security User Authentication Authenticated requests with API gateways secure client-API interactions. After a client authenticates, your API gateway can use the obtained client details for fine-grained control.\nflowchart LR u(\"👤 User\") -- ag(API Gateway) -- |User Info| s1(Service 1) \u0026 s2(Service 2) \u0026 s3(Service 3) style ag stroke: #e62129  APISIX handles authentication directly through plugins like key-auth and jwt-auth . APISIX also supports OAuth authentication and role-based access control systems like wolf through plugins like openid-connect and wolf-rbac , respectively.\nRate Limiting Intentional (DoS attacks) and unintentional (clients making too many requests) traffic spikes to your APIs can bring them down like a house of cards. Setting up rate limiting will improve the reliability of your systems in handling such scenarios.\nYou can set up rate limiting on your API gateway, and if the number of requests increases above a threshold, the API gateway could either delay or reject the exceeding requests.\nflowchart LR START1[ ] -- rd{  Threshold ? } -- r(Router) -- u(Upstream/API) START2[ ] -- rd -- r START3[ ] -- rd ---x |Reject/Delay| r START4[ ] -- rd ---x |Reject/Delay| r style START1 height:0px style START2 height:0px style START3 height:0px style START4 height:0px linkStyle 1 stroke:green linkStyle 4 stroke:green linkStyle 6 stroke:red linkStyle 8 stroke:red subgraph ag[ API Gateway] r rd end  With APISIX, you can use any of the three plugins to configure rate limits based on number of requests, number of concurrent requests per client, and count (limit-req , limit-conn , limit-count ).\nMonitoring and Observability Your API\u0026rsquo;s reliability and your monitoring setup go hand in hand. You can monitor your reliability metrics by setting up monitoring on your API gateway.\nflowchart LR c(Clients) -- ag -- u(Upstream/API) subgraph ag[API Gateway] mo(\"🔍 Tracers, 📃 Loggers, and 📈 Metrics\") end  API logs and traces provide detailed information about an API call. This information will help you know when your API has failed or has an error as soon as possible. Silent fails lead to unfixed errors which can cause problems in the future.\nWith some configuration, you will also be able to predict and anticipate traffic for the future, helping you scale reliably.\nAPISIX has plugins that integrate with logging (Apache SkyWalking , RocketMQ ), metrics (Prometheus , Datadog ), and tracing (OpenTelemetry , Zipkin ) platforms/specifications. You can read more on API Observability with APISIX Plugins .\nVersion Control and Zero Downtime Canary Release When switching to new versions of your APIs, you must ensure that you don\u0026rsquo;t drop your traffic. Clients should still be able to make requests to your API and get back the correct response.\nWith an API gateway, you can setup canary releases. This will ensure that your API remains functional during the transition, and you can also roll back to the older version if there are any issues.\nInitially, the API gateway will route all traffic to your API\u0026rsquo;s old version.\nflowchart LR c(Clients) -- ag(API Gateway) -- |All Traffic| u1(Upstream v1.0) ag x-.-x |No Traffic| u2(Upstream v2.0) style ag stroke: #e62129 linkStyle 1 stroke: green subgraph u[Upstream/API] u1 u2 end  When you have a new version, you can configure the API gateway to route some of your traffic to this new version. You can keep increasing the percentage of traffic to your new service and check if everything is working as expected.\nflowchart LR c(Clients) -- ag(API Gateway) -- |Most Traffic 95%| u1(Upstream v1.0) ag -- |Some Traffic 5%| u2(Upstream v2.0) style ag stroke: #e62129 linkStyle 1 stroke: green linkStyle 2 stroke: green subgraph u[Upstream/API] u1 u2 end  Finally, you can route all traffic to your new API.\nflowchart LR c(Clients) -- ag(API Gateway) x-.-x |No Traffic| u1(Upstream v1.0) ag -- |All Traffic| u2(Upstream v2.0) style ag stroke: #e62129 linkStyle 2 stroke: green subgraph u[Upstream/API] u1 u2 end  APISIX uses the traffic-split plugin that lets you control the traffic to your services. You can use it to set up canary releases or your custom release configuration.\nCircuit Breaking When one of your upstream services is unavailable or is experiencing high latency, it needs to be cut off from your system. Otherwise, the client will keep retrying the request, leading to resource exhaustion. This failure can creep into other services in your system and bring them down.\nLike how electrical circuit breakers isolate faulty components from a circuit, API gateways have a circuit breaker feature that disconnects faulty services, keeping the system healthy. Traffic to these services are rerouted or delayed until the service becomes healthy.\nflowchart LR c(Clients) -- ag(API Gateway) x-.-x |\"⚡ Break\"| u1(Upstream A 1) ag -- u2(Upstream A 2) ag -- u3(Upstream A 3) style ag stroke: #e62129 style u1 stroke: red linkStyle 1 stroke: red subgraph u[Upstream/API] u1 u2 u3 end  APISIX comes with an api-breaker plugin that implements this pattern.\nRedirects As you update your APIs, their endpoints might undergo some change. Traditionally, this would mean that the client application should send requests to the /new-api-endpoint instead of the /old-api-endpoint, meaning your consumers must manually change each call to this API endpoint.\nIf unanticipated, this can break client applications.\nWith an API gateway, you can provide an abstraction layer and redirect requests to the /new-api-endpoint without having the clients change their requests. With proper redirect status codes and messages, you can gradually depreciate the /old-api-endpoint without your consumers experiencing any downtime.\nflowchart LR c(Clients) -- |/old-api-endpoint| ag(API Gateway) ag -- |\"3xx: 'Depreciation notice'\"| c ag x-.-x u1(/old-api-endpoint) ag --- |Redirect| u2(/new-api-endpoint) style ag stroke: #e62129 linkStyle 2 stroke: grey subgraph u[Upstream/API] u1 u2 end  With APISIX, you can use the redirect plugin to configure redirects.\nConclusion When reliability becomes a primary concern, it is evident that API gateways are necessary as more organizations split their monoliths into microservices and move to cloud native architectures.\nHowever, this does not mean API gateways are for everyone. Depending on your API\u0026rsquo;s size and usage, an API gateway might be overkill, and you can get away with using a reverse proxy with basic routing and load balancing capabilities.\nThe use cases mentioned here only scratch the surface of an API gateway\u0026rsquo;s capabilities. You can learn more about API gateways and Apache APISIX at apisix.apache.org .\n","permalink":"https://navendu.me/posts/best-practices-for-building-reliable-apis/","summary":"As the size of your APIs increase, the need for making them reliable and robust also increases. This article discusses the best practices for designing reliable APIs by introducing you to a special kind of reverse proxies called API gateways.","title":"Best Practices for Building Reliable APIs"},{"content":"I ask a lot of questions to my peers and strangers on public forums on the internet. This year, I have been trying to improve how I ask questions. Here is how I do it.\nBut first of all,\nWhat are Good Questions? Good questions are the ones that are easy to answer.\nOur goal for asking a question is to have the other person explain what they know in a way you can understand. A series of good questions is the key to a good answer.\nBad:\n J: What happens when we strip the binaries? (Too vague and broad)\nN: Stripped binaries don’t have debugging information. So its size is reduced \u0026hellip; (Answers with a lot of irrelevant information)\n Good:\n J: I see that we are stripping the binaries to reduce their size before publishing. I found that it shouldn’t affect the performance. Is that right? What other implications does this have? (Clear question, easy to answer)\n  N: Stripping only removes the debugging information. It wouldn’t affect the performance in any way. But it will be difficult to debug if we run into any issues as debug symbols are removed from the traceback.\n The Problem with Bad Questions Bad questions can derail a conversation quickly.\nFor me, asking bad questions has often resulted in:\n  the person explaining things irrelevant to my question.\n  the person explaining things I have no clue of.\n  the person explaining what I already know.\n  the person not answering the question at all (especially for under-researched questions).\n  All of this boils down to you or both of you walking away frustrated and without a clear answer.\nAt this point, it should be obvious why you should focus on asking questions properly. So, here is my process.\nWho are you Asking? Who you are asking a question should impact how you ask the question. Let me explain.\nIf you are asking your coworker who works on your project or is familiar with the particular niche, you can reasonably assume that the person has some context on what you are asking.\nThis means there would be fewer things to explain, and you can build your explanation from your shared knowledge. But it is a different game when you are asking questions to the people of the interwebs.\nWhen I began programming, I had my share of bashing from people in Stack Overflow. I get that having a high bar for quality assurance helps Stack Overflow be the go-to place to ask questions, but some of the moderators are so trigger-happy that they will shoot you (your question) down right away.\nBut anyway, the important thing to remember here is that the person reading your question has very little context about your situation. It is obvious when a person has put little to no effort into the question, and these questions are the first to get the bashing.\nWhen to Ask? If you have a lot of questions or if you think answering your question will take time, it is better to schedule a time when you are both available.\nIf your questions are quick, it is better to ask them right away if it saves you a lot of time.\nGoogle First, Ask Later One of my biggest pet peeves is people who ask technical questions that can be answered by the first result of a Google search. It shows little effort on their part, and now I just ask people to Google it and do not bother to answer until they do their homework.\n I maintain a project called Meshery, and one of the new contributors (who came in to get a GSoC internship ) literally asked if I could explain what Meshery is.\nWe have a website, 100+ pages of documentation, recordings of conference talks, and technical documentation, all sent to the user as they join the community.\nYou know how that conversation went.\nIt would have been different if they had asked me something like, “I have been going through Meshery’s docs and been trying it out locally. I’m unclear how Meshery adds value if a person already uses a service mesh. Could you point me to any docs where this is explained better?”.\nThink for a moment about how you would have answered in these scenarios.\n Doing a bit of research can help you build some foundational knowledge to ask a set of better questions.\nThe “Google first, ask later” motto is only good as a rule of thumb. Nothing has stopped me from asking obvious, googleable (it is a real word ) questions when in conversation with someone.\nTo sum it up, make some effort, do your homework, and then ask your questions. Don’t expect to be spoon-fed.\nIs that Right? Let’s go back to the “stripped binary” example.\n J: I see that we are stripping the binaries to reduce their size before publishing. I found that it shouldn’t affect the performance. Is that right? What other implications does this have?\n See how stating what you already know lets you build the rest of the conversation.\nTo ask this question, you must spend some time digging through what a stripped binary is and how it is different from a “normal” binary. The time taken to understand and formulate that question is time well spent.\nOn the receiving end, the person will see that you have spent time in this and are not just asking them to do your work. It will also be easier to answer your question by building on your foundational knowledge.\nVague Precise Questions  J: How do I use a Kind cluster to set up my development environment?\n If you ask me this, I would reply with a link to the Kind docs. But this wasn’t what they intended to ask. So they say,\n J: I tried this, but it is not working.\n Well, there are million different reasons for this not to work. I am not Doctor Strange to evaluate all the possibilities in a second! A little bit more context might help.\nI will cut to the chase and say how I would ask this question.\n N: I was trying to set up Kind for my local development environment. I am on macOS. I have Docker Desktop and Kind running. I also have set up Metallb LoadBalancer, and I see the external IP of the service, as shown on the logs below. Still, I am not able to reach it from my host machine. Is there something I’m missing?\n Then that senior engineer with years of experience can jump right in and say,\n S: On macOS, Docker does not expose the docker network to the host. You can try port-forwarding to reach the pods.\n See how easy it was to answer?\nThis goes for all questions. The more precise you are with your questions, the easier it is to answer.\nThis also prevents the person answering from going off on a tangent, explaining irrelevant details that you may either not care about or aren’t relevant to your actual question.\nAnother way to prevent shooting off on a tangent is to ask questions that can be answered by a simple yes/no.\n J: Why are we using this gRPC middleware instead of directly calling the required service?\n    J: Are we using this gRPC middleware to convert between two different configuration formats?\n  N: Yes.\n The person usually goes to explain why yes/no after this, but these questions are easy to answer, and I almost always get quick responses.\nThese questions are pretty helpful when you are in conversation with a person, and they are explaining something to you. This segues into my next point.\nWhen in Doubt, Ask More Questions Imposter syndrome is real.\nWhen I started working with others, I often stopped myself from saying, “I don’t understand”, thinking I would look stupid.\nI have then come to learn that if you ask a “stupid” question, you are stupid for the day, but if you don’t, you are stupid for life (because you will always stop yourself from asking questions, ending up not understanding things completely\u0026hellip; umm, you get it right?).\nThis means when you get an answer, and you are not completely satisfied,\n  say what you don’t understand.\n  ask more clarifying questions.\n  stop the speaker and ask more specific questions.\n  Confronting the imposter syndrome is hard, but it has been helpful to me in knowing that everyone else faces this too .\nWhen you start thinking, “maybe I’m just not smart enough to understand the answer”, remember that people want to help you. You just have to help them help you!\nLearning in Public Ask questions in a public channel instead of DMs.\nThis may not work in every situation, but I try to do this more often now.\nThis will document the discussions publicly and would also help any others looking in. You can always point people to this discussion if they ask the same question.\nTake Stack Overflow, for example. You almost always find answers to problems you face from questions someone else asks.\nThe imposter syndrome shifts to the next gear here. Face it head-on.\nAsking Good Questions is a Skill And like all skills, it is sharpened with practice.\nAsking the right questions will help you extract the answers you want. In most scenarios, it is not that the person answering is incapable, but you are not asking the right questions.\nI have improved over the year and am still working out the kinks in my process.\nThis might be a good post to come back to in a year to reflect on and improve.\nTo summarise this post in a sentence,\nMake it easy for people to help you.\n","permalink":"https://navendu.me/posts/how-i-ask-questions/","summary":"I ask a lot of questions to my peers and to strangers on public forums in the internet. This year, I have been trying to improve this process to ask better questions. Here is how I do it.","title":"How I Ask Questions as a Software Engineer"},{"content":"I have been using diagrams.net and Excalidraw for all my diagram needs. But I often had to modify these diagrams when I made a mistake or when they were outdated. Maintaining these diagrams consumed a lot of my time and effort. There has to be a better way, right?\nI was writing a new blog post with many diagrams when I googled \u0026ldquo;diagrams in markdown\u0026rdquo; and found Mermaid .\nMermaid lets you create diagrams by writing text in markdown files. It has an intuitive syntax and verbose documentation to help you make diagrams quickly.\nIt is also easy to add Mermaid to your Hugo blog. You can use shortcodes to add your diagrams in Mermaid syntax and have them rendered on your blog posts.\nflowchart TB y(\u0026quot;👫 You\u0026quot;) --\u0026gt; s{Subscribed to navendu.me?} s --\u0026gt; |Yes| a(\u0026quot;🥳 You are awesome!\u0026quot;) s --\u0026gt; |No| su[/\u0026quot;Subscribe to navendu.me\u0026quot;/] --\u0026gt; a click su href \u0026quot;/subscribe\u0026quot; _blank flowchart TB y(\"👫 You\") -- s{Subscribed to navendu.me?} s -- |Yes| a(\"🥳 You are awesome!\") s -- |No| su[/\"Subscribe to navendu.me\"/] -- a click su href \"/subscribe\" _blank  Check out the official documentation to learn more about Mermaid and its syntax.\nThis article will look at how you can add Mermaid to your Hugo blog.\nImport and Configure Mermaid The first step is to import Mermaid. The simplest way to do this is by importing the minified file from a CDN.\nCreate a file mermaid.html in your layouts/partials directory. We will import and configure Mermaid here.\n\u0026lt;script type=\u0026#34;application/javascript\u0026#34; src=\u0026#34;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\u0026#34; \u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var config = { startOnLoad: true, theme:\u0026#39;{{ if site.Params.mermaid.theme }}{{ site.Params.mermaid.theme }}{{ else }}dark{{ end }}\u0026#39;, align:\u0026#39;{{ if site.Params.mermaid.align }}{{ site.Params.mermaid.align }}{{ else }}center{{ end }}\u0026#39;,  }; mermaid.initialize(config); \u0026lt;/script\u0026gt; The highlighted line looks for parameters in your site\u0026rsquo;s configuration file (config.yaml). This makes it easy to change Mermaid\u0026rsquo;s theme and alignment.\nYou can look for more configuration options in Mermaid docs. I only have these two configurations now and can add them to my site\u0026rsquo;s configuration file.\nparams: mermaid: theme: \u0026#34;dark\u0026#34; align: \u0026#34;center\u0026#34; Loading Mermaid on Required Pages It is unnecessary to load Mermaid on every page in your blog. You only need it on pages where there are Mermaid diagrams.\nWe will only add the layouts/partials/mermaid.html file when a page needs it. To set this up, we will create a front matter variable mermaid, which can be set to true to load Mermaid.\nNow, in your page layout, you can check if this variable is set and load the mermaid.html partial file. My page layout file is layouts/_default/single.html.\n\u0026lt;div class=\u0026#34;post-content\u0026#34;\u0026gt; {{- if not (.Param \u0026#34;disableAnchoredHeadings\u0026#34;) }} {{- partial \u0026#34;anchored_headings.html\u0026#34; .Content -}} {{- else }}{{ .Content }}{{ end }} \u0026lt;!-- Add mermaid min js file --\u0026gt; {{ if (.Params.mermaid) }} {{ partial \u0026#34;mermaid.html\u0026#34; }} {{ end }}  {{ partial \u0026#34;subscribe.html\u0026#34; . }} \u0026lt;/div\u0026gt; You can now import Mermaid by setting the mermaid page variable to true in your markdown files as shown below:\n--- title: \u0026#34;Adding Diagrams to Your Hugo Blog With Mermaid\u0026#34; date: 2022-08-26T15:46:16+05:30 draft: true weight: 14 ShowToc: true TocOpen: true mermaid: true summary: \u0026#34;This article shows how you can add diagrams to your Hugo site with Mermaid.\u0026#34; tags: [\u0026#34;hugo\u0026#34;, \u0026#34;mermaid\u0026#34;, \u0026#34;blogs\u0026#34;, \u0026#34;tutorials\u0026#34;] Creating the mermaid Shortcode The final step is to create a shortcode to add diagrams in Mermaid syntax to your markdown files.\nCreate a new file layouts/shortcodes/mermaid.html with the below content:\n\u0026lt;div class=\u0026#34;mermaid\u0026#34;\u0026gt;{{.Inner}}\u0026lt;/div\u0026gt; That\u0026rsquo;s it! Now to test it out, you can try using the shortcode as shown below:\n{{\u0026lt; mermaid \u0026gt;}} flowchart LR y(\u0026quot;👫 You\u0026quot;) --\u0026gt; h{\u0026quot;🤝 Found this helpful?\u0026quot;} h --\u0026gt; |Yes| r[/\u0026quot;⭐ Check out my featured posts!\u0026quot;/] h --\u0026gt; |No| su[/\u0026quot;📝 Suggest changes by clicking near the title\u0026quot;/] click r \u0026quot;/categories/featured\u0026quot; _blank {{\u0026lt; /mermaid \u0026gt;}} After you save and build your site, you should be able to see this diagram rendered:\nflowchart LR y(\"👫 You\") -- h{\"🤝 Found this helpful?\"} h -- |Yes| r[/\"⭐ Check out my featured posts!\"/] h -- |No| su[/\"📝 Suggest changes by clicking near the title\"/] click r \"/categories/featured\" _blank  Optional: Add CSS to Style the Diagrams You can optionally add custom CSS to the mermaid class to change how your diagram looks.\nYou can add this CSS by creating a new file, assets/css/mermaid.css. My CSS file has the following content to make my diagrams look better on my posts:\n.mermaid { display: flex; justify-content: center; margin: 10px 0px 25px 0px }  Mermaid is a free and open source project maintained by the community. You can sponsor the project on GitHub to support its development.\n","permalink":"https://navendu.me/posts/adding-diagrams-to-your-hugo-blog-with-mermaid/","summary":"This article shows how you can add diagrams to your Hugo site with Mermaid.","title":"Adding Diagrams to Your Hugo Blog With Mermaid"},{"content":"This blog is now deployed on Netlify ! I spent a little more than an hour yesterday migrating it from GitHub Pages to Netlify.\nGitHub Pages is a perfect solution for deploying static websites. But, it made it challenging to implement some of the features I wanted on my blog.\nSo, on impulse and to procrastinate from finishing a blog post, I migrated the site to Netlify!\nWhat GitHub Pages Lacked GitHub Pages has been my go-to static website deployment solution for the past three years. And it worked like a charm even if it was free.\nI have been looking for ways to set up my Hugo website to show draft blog posts. The problem was that I didn\u0026rsquo;t want these drafts to show up on the homepage listing, but I also wanted a sharable link for people to review.\nThere are ways I can set this up, but even if I do that, there aren\u0026rsquo;t any ways for people/reviewers to leave feedback on my static website.\nI also used a lot of client-side redirects, which is not desirable as opposed to server-side redirects. GitHub Pages did not provide a way for you to configure server-side redirects. So I had to use these hacky, Jekyll redirects on a subdomain. It works, but it could be better.\nEnter Netlify Netlify has deploy previews . So, when you make a pull request to your production branch, Netlify will build the site for you and show a preview of what the change will look like.\n Deploy preview feature in NetlifyFrom github.com/navendu-pottekkat\n  You know what this is best for? Previewing draft blog posts!\nUsing Netlify, I can open a pull request with my draft post, and Netlify will generate a preview build of the site without affecting my production site. Reviewers can see the preview and suggest edits as comments on GitHub.\nNetlify also brings a better continuous integration experience.\n CI checks run by NetlifyFrom github.com/navendu-pottekkat\n  Netlify lets you configure the build settings through the Netlify UI or a configuration file (netlify.toml). This gives you a lot more control than the few configuration options GitHub Pages provides.\n Netlify dashboard for navendu.meI added a Plugin and to lint links and it broke the CI because there were a lot of broken links. Task for another day.\n  With Netlify, configuring redirects is as easy as adding two lines to your configuration file. You also have the option to configure the proper status code. Now that\u0026rsquo;s neat.\nNetlify also has a DNS service, supports storing large media (Git LFS), has split testing and rollback features, and more . But, these are only \u0026ldquo;nice to have\u0026rdquo; features for me now. I will not be using these anytime soon*.\n * I might use these sometime soon.\n How I Migrated to Netlify The process was pretty straightforward. It only took me a little over an hour to set everything up and test.\nIn the steps below, I have the following setup:\n A Hugo blog published on GitHub Pages. A custom domain registered on Hostgator (also my DNS service).  You can always refer to the official Netlify docs if you have a different setup somewhere along this guide.\nCreate a Netlify Account First, you must create a Netlify account if you don\u0026rsquo;t have one.\n Create your Netlify accountDo you also use GitHub for everything?\n  Import Your Website You can now add your website to Netlify. You should have the code for your website on a Git provider. If you don\u0026rsquo;t have one, now is the time to git push your code.\n  Click on \u0026ldquo;Add new site\u0026rdquo; and \u0026ldquo;Import project\u0026rdquo;.\n Importing your site     Select the Git provider where you have your website code.\n Select your provider     Pick the repository from the Git provider after granting access to Netlify.\n Pick the repository to deploy     You can then configure the build settings based on your blog engine. Since I\u0026rsquo;m using Hugo, I will add the following configurations.\n Build settings for my Hugo website--gc cleans up old resources, --minify reduces the size of the files and the public folder is where Hugo outputs the build\n    And voila! Netlify will automatically build your first deployment. Now, you will be able to see the production URL for your website.\n Production URL of your websiteThis screenshot is from Netlify\u0026rsquo;s docs\n  This URL is unique; you can change it to yoursitename.netlify.app or your custom domain.\nChanging the Website URL and Custom Domains You can change your site URL to anything unique. Once changed, this will be your website\u0026rsquo;s address.\n  From the \u0026ldquo;Site overview\u0026rdquo; page, Go to \u0026ldquo;Domain settings\u0026rdquo;.\n  Click on the \u0026ldquo;Options\u0026rdquo; next to the site name and click \u0026ldquo;Edit site name\u0026rdquo;.\n Edit the generated site namekung-fu-panda-23 is a cool name\n    Change the site name and save.\n   Note: If you use this URL, you might need to change the baseURL in your Hugo configuration file to ensure all the links work.\n In my setup, I\u0026rsquo;m using Hostgator as my DNS service and not the Netlify DNS service.\n  In \u0026ldquo;Domain settings\u0026rdquo;, click \u0026ldquo;Add custom domain\u0026rdquo;.\n  You can enter the domain you already own or enter something new and purchase the domain. Netlify will set everything up if you are buying a domain.\n  If you are using a domain you already own with an external DNS provider, you will see a warning. You can ignore that and click on \u0026ldquo;Add domain\u0026rdquo;.\n  You can also set up an SSL certificate from the HTTPS section.\n Secure your site with an SSL certificate     If you use a different DNS service, you need to configure it to point your domain to your Netlify website. You can check the Netlify docs if you have a different setup than mine.\n Login to your domain\u0026rsquo;s control panel and open up your DNS configuration. I\u0026rsquo;m using Hostgator for my domain. Create an A Record with your apex domain and point it to Netlify\u0026rsquo;s load balancer IP address 75.2.60.5. Then, create a CNAME record for the wwwsubdomain and point it to your website address yoursitename.netlify.app.  Now, you must wait for the changes to propagate, and you will have your domain configured.\nConfiguring Deploy Previews The main reason to migrate was the deploy previews feature. And it is super easy to set up.\n Go to \u0026ldquo;Site settings\u0026rdquo;. Select \u0026ldquo;Build \u0026amp; deploy\u0026rdquo; from the side menu and then \u0026ldquo;Continuous Deployment\u0026rdquo;. Scroll down to \u0026ldquo;Deploy previews\u0026rdquo; and set it up as desired. I have enabled it for any pull request against my production branch, and I have also enabled the Netlify Drawer.  That is it. You now have deploy previews!\n Note: To ensure that the deploy previews show drafts, I updated my Netlify configuration file (netlify.toml) to change the build command for deploy previews.\n[context.deploy-preview] command = \u0026#34;hugo --buildFuture --buildDrafts --gc --minify -b $DEPLOY_PRIME_URL\u0026#34; Here $DEPLOY_PRIME_URL is an environment variable that Netlify sets, used to update the site\u0026rsquo;s baseURL.\n That brings an end to my current setup. It is much better than my earlier setup with GitHub Pages and was pretty easy to migrate. I would definitely recommend Netlify for your static websites.\nFeatures I Might Add in the Future This was my first iteration with Netlify. I have a basic setup that more or less does everything I need. But, I might use these other features if they are fruitful.\nForm Handling I use Mailchimp to handle subscriptions to this blog. If I can find a way to send mass emails, I might set up the free form handling offered by Netlify.\nSplit Testing A/B test blog posts? Yes!\nCDN/git LFS My blog contains a lot of images. I do my best to compress these images, but I will reach a point where the images are taking too much space, increasing the repo size. If Netlify\u0026rsquo;s solution is better, I might switch to that.\nToo Good to be Free? Netlify seems too good to be free. I\u0026rsquo;m on the free tier, and it appears to be generous.\n My 24 hour Netlify usagePetition to add a section that shows the carbon footprint on my blog builds. Take that, people flying on private jets!\n  But, it will only be some time until I pass these limits and would end up needing to pay for the service. It is not too much money but seeing that the alternative, GitHub Pages, is free, I cannot stop thinking, \u0026ldquo;maybe I don\u0026rsquo;t need deploy previews\u0026rdquo;.\nI hope I don\u0026rsquo;t have to write a post titled \u0026ldquo;How and Why I Migrated My Blog Back From Netlify to GitHub Pages\u0026rdquo;!\n","permalink":"https://navendu.me/posts/how-and-why-i-migrated-my-blog-from-github-pages-to-netlify/","summary":"This blog is now deployed on Netlify. Here is how and why I did it.","title":"How and Why I Migrated My Blog From Github Pages to Netlify"},{"content":"I have been blogging ever since I started my career as a software engineer.\nI literally had zero professional experience then so I chose to write as I learn. I documented what I learned and shared my experience as I went.\nLooking back at this 2.5 years later, writing blogs has helped me develop my skills, build an audience and advance my career.\nHere are my top reasons why you should start writing blogs today.\nYou Get Better Technically Writing helps to reinforce what you learned.\nWhen you are learning a new technology or solving a very specific bug that dives deep into the language, you should write it down. Write down what you learned or how you fixed the bug. This document can then work as a reference for you in the future and help others in your same path.\nBut this process helps you even more by forcing you to articulate what you have learned, helping you learn and understand the concept better.\nYou Get Better at Communicating Communication is one of the best skills you can have as a developer. It is a much needed skill even if you are not writing technically in your job.\nA lot of what software engineering is requires you ot work in teams where you have to have good communication to get the best results.\nWriting forces you to communicate well. It forces you to think clearly, formulate your thoughts and communicate the thoughts with others.\nIf there is a feedback loop where you can get comments from your readers, use that feedback to improve your communication.\nYou are Helping Others There are always people who can benefit from your content.\nThis is regardless of where you are in your career. Even though I had very little professional experience, sharing what I\u0026rsquo;ve learned and my experience has helped people trying the same path.\nCreate content and there will be readers.\nYou are Building Your Personal Brand Welcome to 2022 where it is all about your personal brand.\nBuilding a personal brand can help you in many ways. Creating content on the internet and putting your name on it is a solid step towards this.\nWhat you write will be associated to your personal brand. And personal brands helps you build personal monopolies .\nYou are Setting Up Your Career When you write online you are building your professional network.\nThis network of people who share similar interests with you can help by providing opportunities that you might have not received otherwise. They already have a picture of you and your skills form the content you write.\n It is easy once you start writing.\nYou will find more reasons why you want to continue writing blog posts and you will form a habit.\nThe important step is to just start. Don\u0026rsquo;t wait till you are \u0026ldquo;good enough\u0026rdquo; or \u0026ldquo;experienced\u0026rdquo; to start writing. Things will fall in their places once you start.\n","permalink":"https://navendu.me/posts/why-developers-should-blog/","summary":"I have been blogging ever since I started out as a software engineer. Here is why you should do it too.","title":"Why Developers Should Blog"},{"content":"  Money’s greatest intrinsic value—and this can’t be overstated—is its ability to give you control over your time.\n Morgan Housel  The Psychology of Money    These are my key takeaways from Morgan Housel \u0026rsquo;s international bestseller “The Psychology of Money ”.\nI have kept my learnings to be short and skimmable and I would urge everyone reading this article to read the entire book.\n The Psychology of MoneyPhoto by Morgan Housel on Unsplash\n  The book is only about 240 pages long and it is a fast read. I read it cover to cover in two sittings—it was a real page turner.\n Housel begins by talking about how people’s financial decisions are less mathematics, graphs and spreadsheets and are more influenced by their personal history, world view, ego, pride and odd incentives.\nHe drives this point throughout the book through 19 stories and teaches how you can make better financial decisions.\nThese are my notes—my key takeaways—on what each of these stories teach us.\n Everybody’s approach to money is different and is shaped by their own beliefs and experiences from different societies, cultures and economic conditions. The role of luck is often unaccounted. You can end up in the wrong side by making good financial decisions and vice versa. People at the top may have benefitted from luck while those at the bottom might have been the casualties of risk.  Be careful who you praise and admire. Be careful who you look down upon and wish to avoid becoming. To ease this risk, replicate broader patterns instead of trying to replicate specific individuals.   There is no reason to risk what you have and need for what you don’t have and don’t need. Your goals are never complete if you start comparing yourself with others. Keep your goalpost fixed. The role of time and compounding is important in building wealth. Ask Warren Buffet. Building wealth is easier compared to staying wealthy—it is a different game. Be optimistic that things will work out in the long term even while facing adversity in the short term. Raise your odds of success at a given level of risk. Failures are okay. Long list of failures are overcome by a small list of huge successes. Investment firms have huge portfolios and how many of them actually work out and make money? What wealth gives you is the control of time. This gives you control over doing what you want, when you want and with who you want. This is the most common variable that makes people happy. No one is impressed by your possessions as much as you are. People admire your possessions and not you. Trying to buy admiration and respect is a wild goose chase. Spending money to show you have money is the fastest way to loose money. Wealth is the money you don’t spend. It is what you don’t see. It is the ability to do/get things—time, freedom, possessions— in the future. You don’t need a reason to save money. You save money because you save money. With compounding, building wealth is more about savings rate than income (over a certain level of income). Investing strategies need not always be rational. Making reasonable choices like investing in your own country, investing in stocks you like and investing in family can give advantages. The best portfolio is the one that gives you peaceful sleep at night. Investing is not a hard science to work based on historical evidence. Investors are human and they are unpredictable.  Experiencing specific events doesn’t necessarily qualify you to know what would happen next. The events that greatly affected the economy could be outliers. Surprises are surprising.   Leave room for errors in your decision making. Think about what errors your can financially endure as well what you can emotionally endure. Your financial goals, your career goals, your life goals, change. Accounting for the unknown is hard and it is okay. The hidden price with investing is the ups and downs in the market, the uncertainty and the doubt in your mind. Be prepared to pay the price. Nothing is free. People are different and have different goals. Don’t be persuaded by the actions of people who are playing a different game than you. Understand your goals and your time horizon. Pessimism sounds smarter and people are drawn to its reasoning. But, the problems pessimists forecasts are solved by clever and novel solutions leading to optimistic futures. The more you want something to be true, the more likely you are to believe a story that overestimates the odds of it being true. These “appealing fictions” have significant impact on our investments.   This book is a must read if you are serious about investing, building wealth and being financially independent.\nThe book costs $16 and ₹269 on Amazon.\n","permalink":"https://navendu.me/posts/the-psychology-of-money/","summary":"Key takeaways from Morgan Housel’s international bestseller “The Psychology of Money”—brief and to the point.","title":"Key Takeaways from Morgan Housel's \"The Psychology of Money\""},{"content":"Contributing to open-source can be overwhelming.\nYou are creating a pull request to an open-source project that would open up your code for people to give feedback and criticise.\nThis turns off a lot of new contributors from making an impact in open-source.\nYou might also be at a point where you are not confident enough in your programming skills that you hesitate to take that first step in contributing code.\nA good way to tackle this and gain confidence to contribute code is to start by contributing to documentation.\nDocumentation is necessary for every open-source project. Contributing to and maintaining documentation is not an easy task but is highly impactful.\nI will drop a bomb and say contributing to documentation is more important than contributing code.\nIn this post, I will try to share my insights on contributing to documentation from my experience as an open-source contributor and a maintainer.\n Being overwhelmed when you first start to contribute to open-source is natural. Start small. But start.\n Finding a Project The first thing to do before you can start contributing is finding a project you can contribute to. And the best project to contribute to is the one you have been using for a while.\nThat is, if you have been using a framework or a library, a tool or any other open-source project, contribute to it.\nWith this, you will have a lot of context on what the project is and you will definitely be able to find areas in the documentation you can improve.\nIf you cannot find such a project to contribute to, look for projects with an active community of contributors.\nCommunity is key in open-source and you will definitely reap the rewards for being part of a thriving community.\nAll these factors comes secondary to the fact that you should always contribute to projects that YOU are interested in.\nStart as a User As mentioned in the above section, you will be able to make impactful contributions if you are already a user of the project.\nSo, if you aren\u0026rsquo;t, you should start by exploring the project from the perspective of a user.\nAs a user, you will likely go through documentation and tutorials as you start out and you are likely to find bugs, out-dated content or things you can improve.\nWhen you find areas to improve, open up issues (or any form of tickets if you are not using GitHub) for these and discuss with a project maintainer to validate it and get it assigned to you.\nOnce you have been assigned an issue, you can start contributing.\nReview the Contributing Guidelines Most (if not all) open-source projects will have a contributing guideline in their GitHub/GitLab repository.\nThis document will be geared towards contributors with guidelines on setting up a developer environment and how you can make contributions.\nRead these guidelines carefully and make sure that you follow them.\nFor example, if a project requires you to sign every commit, your pull requests will be rejected very quickly if you do not do it.\nWhat should you Document? As mentioned in the previous sections, you are likely to find issues when you start to use the project referring to the documentation.\nThis could be as simple as an outdated screenshot to outdated or missing documentation.\nOpen up issues for these as mentioned and discuss it with a maintainer to get it assigned to you.\nIf you cannot find issues by yourself, you can try to fix already open issues.\nThere are labels in GitHub issues that can help you filer out only \u0026ldquo;documentation\u0026rdquo; or \u0026ldquo;docs\u0026rdquo; issues and similar features will be there in any ticketing system being used by the project.\nIn-Code Documentation People generally don\u0026rsquo;t think of this when they think about documentation.\nIn-code documentation refers to the error messages, help texts and other text the user interacts with that doesn\u0026rsquo;t necessarily affect the \u0026ldquo;logic\u0026rdquo; of the code.\nThese are really important as users will definitely interact with this while using the project.\nAs a new contributor you have the magic eyes to spot issues with these that seasoned contributors are too close to notice.\nMore Ways to go Beyond the Docs Page Good documentation is not just limited to the docs page.\nIt can also involve:\n Writing a blog post that takes the user through a new feature Creating a Twitter thread about the project Documenting processes that can be used for the community (a contributing guide for example) Updating the project\u0026rsquo;s website  You can always ask the project maintainers to help you find areas that need contributions.\nBefore we finish this, I want to point out that non-code open-source contributions are not just limited to writing documentation.\nYou can check out other ways you can contribute to open-source projects without contributing code .\n","permalink":"https://navendu.me/posts/contributing-to-documentation/","summary":"Contributing to documentation is really impactful for an open-source project. It can also be a stepping stone to make code contributions. Learn how.","title":"Contributing to Documentation in Open-Source"},{"content":"Contributing to open source is a great way to hone your skills while working on real world projects. But, for new developers, this process can seem daunting.\nAfter all, when you create a pull request, you are opening up your code for people to give feedback and criticize.\nThis is where open source mentorship programs like Google Summer of Code (GSoC) come in and bridge the gap between new developers and large open-source codebases.\nWhat is Google Summer of Code (GSoC)? GSoC is an annual program from Google focusing on bringing students and new developers into to open source.\nEach summer, participating mentees get to work on open source projects from the hundreds of organizations that participate in GSoC. The participating mentees are paired with mentors from the participating organizations and are also paid a stipend during their period of mentorship.\nHere is a quick summary of the program before we dive deep into the details.\n   Duration Timeline Stipend Eligibility     12 Weeks, ˜3 Months (Could be increased up to 22 weeks ) February 7th - November 28th  $1500 - $3300  Anyone about 18 years of age     Stick around till the end to learn more on how you can get selected and my tips as a GSoC mentor.\n This year, GSoC is open to everyone above the age of 18 and not just students.\n Why Should I Participate in GSoC? According to Linux Foundation\u0026#39;s 2021 Open Source Jobs Report , 92% of employers say finding people with open source skills is difficult.\nOpen source skills are in demand and there is definitely a lack of supply.\nParticipating in programs like GSoC helps you:\n Gain real world experience Build your network Boost your career Get a stable stipend  The goal of GSoC is to enable mentees to be good open source stewards. Successfully completing GSoC can open doors for you and could even lead to a career in open source .\nAm I Eligible to Participate? With the new rules for 2022 , everyone over the age of 18 is eligible to participate.\nYou should also have a strong desire to contribute to open source and help the community of course!\nWhen does GSoC Start? Organizations start applying from February 7th.\nMentees can start applying from April 4th till April 19th.\nSee the 2022 timeline for GSoC for other details.\nHow Long is the Program? This time around, mentors and mentees can extend the period of GSoC up to 22 weeks instead of the mandatory 12 weeks as in previous years.\nSee the dates of each program milestones for a better idea on the timeline.\nHow do I Apply? You can apply on the GSoC website when the application period starts.\nThe application process is mainly these 3 steps:\n Selecting an organization Selecting a project Preparing a project proposal  How do I Select an Organization? GSoC has hundreds of participating organizations each year. Mentees can apply to any of these organizations, but filtering through them and deciding which to contribute to can be difficult without a proper plan.\nSo, to be in the best position when the application process starts, you have to be prepared.\nYou can find organizations to contribute to even before they are officially announced. I would suggest following the steps below.\n Write down what skills/ tech stack you know currently Write down the skills/ tech stack you want to learn Filter organizations based on these two criteria Look through the projects in the organization to find what interests you List down multiple projects across organizations that fit this criteria  How do I Select a Project? You would have now narrowed down to a couple of projects under the same or different organizations. How do you go from here? How do you know which project to submit a proposal to?\nThe key here is homework. You need to learn more about the projects.\nHere\u0026rsquo;s what I suggest doing:\n Start by using the project and see how it works (Is it cool?) Go through the project\u0026rsquo;s documentation and contributing guidelines (Are there well documented processes?) Join the project\u0026rsquo;s community channel (Slack, Discord, mailing lists) (Are they active and welcoming?) Pick up a \u0026ldquo;good first issue\u0026rdquo; and try to fix it (Is it doable? Do you get support from the mentors/maintainers?) Ask clarifying questions about the project on the communication channels (Do you get answers?)  If the answer to all these questions is a solid \u0026ldquo;yes\u0026rdquo;, then you need to ask yourself the golden question.\n\u0026ldquo;Do I want to contribute to this project?\u0026rdquo;.\nYou know what to do next.\nHow do I Submit a Proposal? Now that you have a project(s) chosen, it is time for the biggest step yet. Submitting a proposal.\nLike other open source mentorship programs, GSoC can also get competitive. You have to make your proposal stand out to get accepted.\nGSoC\u0026rsquo;s official guides outline what is expected in a proposal and also shows some examples of good proposals .\nIt is important that you communicate with your potential mentors prior to submitting a proposal to make sure that you have the right idea. Clarify the deliverables and establish a realistic deadline before you start writing the proposal.\nIf you already have prior contributions before you submit the proposal, it would definitely put your application in front.\nHow will I Get Selected? Once you submit the proposal, the mentors will go through each submission and will let you know if you get selected or not.\nIf you get selected, it\u0026rsquo;s time to celebrate!\nWhat if I don\u0026rsquo;t get Selected? Don\u0026rsquo;t stop contributing!\nGSoC is highly competitive and it is difficult to get selected. But you don\u0026rsquo;t have to be a GSoC mentee to contribute to open source. You can do it independently!\nThe spirit of GSoC is open source and open collaboration. If you keep contributing, you will learn a lot and will have a head start for the next year\u0026rsquo;s program.\nIf your project participates in any other open source internship programs , you can apply for it.\nI\u0026rsquo;m in GSoC! The first three weeks of the program will be a community bonding period. You can use this time to get a feel of the community before you start engaging.\nYou will get to code in the next weeks. A part of the stipend will be provided once you complete one of the two evaluations.\nAnd as mentioned before, the timeline for 2022 is pretty flexible.\nEnding Notes I personally believe GSoC is a great program. It has and will continue to help attract students and new developers to work in open source.\nThe goal for this program is to not make some money (money is important) but to foster open source software.\nThe impact such programs have on the community is incredible. And as GSoC mentees, you inherit the responsibility to pay it forward.\nSo, don\u0026rsquo;t stop contributing after GSoC ends. Be a mentor next time. Help others in the community.\nBest of luck on your GSoC application!\n","permalink":"https://navendu.me/posts/everything-about-gsoc/","summary":"Google Summer of Code (GSoC) is a great way to start contributing to open source while getting paid to do so. This article dives deep into everything GSoC from the application procedure to tips from real experience on being successful","title":"Everything you Need to Know About Google Summer of Code (GSoC): Important Dates, Eligibility, Application, Getting Selected and Other Tips"},{"content":"With Hacktoberfest around the corner, here are some tips to make better Pull Requests for your next Open-Source contribution.\n As a maintainer and a contributor to Open-Source projects these are the things I look for when reviewing and take care of when submitting Pull Requests.\n This is a fast read but if you want one even faster, check out this Twitter thread .\nTip 1: Contributing Docs are your Best Friends Most Open-Source projects have a contributing doc(usually a CONTRIBUTING.md file at the root of the repo) that contains all the necessary details on how to set up your development environment, coding conventions you have to follow and much more.\nIf you are looking to contribute to a project, that should be the next place to look into after the ReadME file.\nContributing doc from the Meshery project\nTip 2: Document Your PRs This is really important. I repeat this is really important.\nYou should make the reviewing process as easy as possible and a reviewer should be able to get the context of your PR with a quick glance.\nFor this, you can:\n Write self-documenting code (What is self-documenting code and how can it help? See this great answer ) Use comments liberally in your code Write clear commit messages and Most importantly, comment what your PR does clearly  As shown below, the PR description shows what issue it fixes, the change made in the PR and the actual new User-Experience.\nThis makes the PR very easy to review.\nA Pull Request description example\nTip 3: Use Pull Request Templates Most Open-Source projects will have a Pull Request template to guide newcomers and veterans alike into documenting their PRs properly.\nAs you can see below, the PR template has sections where the contributors are expected to fill details.\nPull Request template from the Open Service Mesh project\nThis would make the job of the reviewer much easier as they would be able to easily test it out, ask a domain expert to review and much more.\nThese also acts like a checklist that the contributor can tick off. For example: Questions like \u0026ldquo;Did I write unit tests?\u0026rdquo;, \u0026ldquo;Did I sign all of my commits?\u0026rdquo;, \u0026ldquo;Is this a breaking change?\u0026quot; can be answered as you start filling the template.\nTip 4: Make your PRs Small and Focused Whenever possible, your Pull Requests should do one thing and one thing only.\nThe problem with addressing multiple concerns in a single PR is that the reviewer may not agree with all the changes and this could potentially lead to long discussions.\nSo small and focused PRs would reduce the time taken for reviews and would make your PR most likely to get merged.\nAn Example Pull Request with just a single, focused change\nTip 5: Add Tests If the project you are working on has automated tests, then make sure that you add tests for the code you are adding.\nNot sure if you should write tests? Here is a bonus tip:\n If it can be tested, it should be tested.\n Tip 6: Make sure the Automated Tests Pass Most projects have automated tests in their CI/CD pipelines which run on your Pull Requests.\nThese tests are in place to make sure that the code you are adding passes some constraints. These are mostly lint checks, build checks, unit tests and integration tests.\nIf these checks are not passing, fix it and make sure all the tests pass before asking for a review.\nYour code is not likely to get merged if these checks fail.\nAll checks passed!\nTip 7: Respond to Feedback When you open a Pull Request, you are also opening a discussion on why this change is needed and why your PR is the right way to add that change.\nSo, make sure that you respond to reviews, make changes to your PR as suggested by the reviewer and ask for clarification questions if needed.\nThis would make sure that you and the reviewer are on the same page and would make the PR reviews much easier.\nSometimes discussions can get really long! But these are necessary when you are adding a new feature with breaking changes\nTip 8: Be Patient Most of the Open-Source maintainers are volunteers. They maintain projects during their free time off from their work.\nSo, they may take some time to get to your PR and review it.\nSo be patient while they do their thing. Take a break, drink a cup of coffee and watch Naruto.\nBonus Tip: I will be writing about Contributing to, Building, Scaling and Maintaining Open-Source projects here on my blog as well as on Twitter and on the DEV Community .\nIn this last year, I have went from a noob Open-Source contributor to:\n Building my own project and making it to the #1 Trending Repository on GitHub  Maintaining 2 CNCF Projects and Working full-time in Open-Source  I would like to share everything I\u0026rsquo;ve learned to help as many people as possible who are interested in Open-Source.\nSo, follow along and may the source be with you!\n","permalink":"https://navendu.me/posts/pull-requests-like-a-pro/","summary":"Tips to make High-Quality Pull Requests.","title":"Pull Requests Like a PRO"},{"content":"Contributing to open-source is one of the best ways to gain skills and build your resume as a student or as a new developer.\nAs open-source becomes the norm, more and more big tech companies and non-profit organizations have been investing into open-source projects and into internship programs.\nThese programs often offer the intern a mentorship opportunity and a stable stipend so that they can invest their time to work on these projects.\nThis post consists of all the details of these programs. You can skip to the summary for quick access to all these resources.\nGoogle Summer of Code (GSoC)    Program Name Duration Timeline Stipend     Google Summer of Code (GSOC)  10 Weeks, ˜3 Months January 2021 - August 2021  $1500 - $3300     This is one of the most popular programs with 700+ participating organizations. The program is organized by Google and each year mentees identify projects and submit proposals to work on them. Accepted mentees are assigned a mentor by the participating organization and students spend their summer working with them.\nAs of 2022, anyone above the age of 18 can participate in GSoC .\nLinux Foundation Mentorship Program (LFX)    Program Name Duration Timeline Stipend     Linux Foundation Mentorship Program (LFX)  12 Weeks, ˜3 Months March 1st - May 31st , June 1st - August 31st , September 1st - November 30th  $3000 - $6600     This program is organized by The Linux Foundation and aims to pair open-source talent with experienced mentors. The students identify a project and create a profile to submit an application. Mentees can apply to 3 projects at a time.\nMLH Fellowship    Program Name Duration Timeline Stipend     MLH Fellowship  12 Weeks September 20th - December 13th (3 batches year round)  Up to $5000 (need based)     This program will provide you the opportunity to contribute to the type of open-source projects that every company depends on.\nThere are also non-open-source programs that MLH offers. See fellowship.mlh.io/#programs .\nGoogle Season of Docs (GSoD)    Program Name Duration Timeline Stipend     Google Season of Docs  6 Months May 17th - December 14th  Depends of budget of the organization     This program gives technical writers an opportunity to gain experience in open-source.\nIf you are interested in contributing to documentation, this is a really good opportunity to work on some interesting projects.\nOutreachy    Program Name Duration Timeline Stipend     Outreachy  3 Months May - August, December - March (Applications due September 3)  $5500      Outreachy is a diversity initiative that provides paid, remote internships to people subject to systemic bias and impacted by underrepresentation in the technical industry where they are living.\n Season of KDE    Program Name Duration Timeline Stipend     Season of KDE  3 Months January - April  No    This program offers an opportunity for people to participate in both code and non-code projects that benefits the KDE ecosystem.\nFree Software Foundation (FSF) Internship    Program Name Duration Timeline Stipend     Free Software Foundation (FSF) Internship  12 Weeks 3 terms yearly  No    This program provides an opportunity to work closely with the FSF staff members in your area of interest, such as campaign and community organizing, free software licensing, systems and network administration, GNU Project support, or Web development.\nLinux Kernel Mentorship Program    Program Name Duration Timeline Stipend     Linux Kernel Mentorship Program  12 Weeks March 1st - May 31st , June 1st - August 31st , September 1st - November 30th  $3000 - $6600     This program from The Linux Foundation connects experienced Linux Kernel developers and maintainers with mentees to help the become contributors to the Linux Kernel.\n The program serves as a vehicle to reach out to students and developers to inject new talent into the Linux Kernel community. It aims to increase diversity in the Linux Kernel community and work towards making the kernel more secure and sustainable. We strongly encourage applicants who are from traditionally underrepresented or marginalized groups in the technology and open source communities, including, but not limited to: persons identifying as LGBTQ, women, persons of color, and/or persons with disabilities.\n Linux Foundation Networking (LFN) Mentorship Program    Program Name Duration Timeline Stipend     Linux Foundation Networking (LFN) Mentorship Program  12 Weeks FT, 24 Weeks PT June 1st - August 21st , September 1st - ?  $3000 - $6600     This program - also from The Linux Foundation - aims to provide opportunity to gain exposure to LFN\u0026rsquo;s projects and technical communities.\nThe mentors in this program are active developers and technologists contributing to the industry\u0026rsquo;s leading open source networking projects such as ONAP, OPNFV, OpenDaylight, FD.io.\nGNOME Summer of Code    Program Name Duration Timeline Stipend     GNOME Summer of Code  10 Weeks, ˜3 Months January 2021 - August 2021  $1500 - $3300     This is provided through GSoC to help underrepresented groups in free and open-source software to get involved with GNOME projects.\nAlibaba Summer of Code    Program Name Duration Timeline Stipend     Alibaba Summer of Code  3 Months May 25th - August 31st (2020)  Yes    Students will receive mentorship from the Alibaba team to work on a series of open-source projects.\nFOSSASIA Codeheat    Program Name Duration Timeline Stipend     FOSSASIA Codeheat  Year round October - June   Prizes for winners     This is a coding contest for FOSSASIA projects on GitHub and a jury wil choose winners from the top 10 contributors based on the code quality and the relevance of the commits.\nFOSSASIA Internship Program    Program Name Duration Timeline Stipend     FOSSASIA Internship Program  2 - 6 Months Decided individually Yes     In the program we are looking for people who would like to work on the project they choose continuously. Different to GSoC in the internship it is not only about a specific project proposal. We rather look for participants who are interested to advance the project and solve bugs or add features that are required to bring the project forward.\n Open Summer of Code    Program Name Duration Timeline Stipend     Open Summer of Code  16 days See Timeline  Yes     This program will coach the students into working in different open innovation projects provided by partnering organizations, companies and governments.\nOpen Mainframe Project Mentorship Program    Program Name Duration Timeline Stipend     Open Mainframe Project Mentorship Program  3 Months Through GSoC or LFX  Yes     This program would help the mentee to expand their knowledge of mainframe technology and would help them contribute to open-source projects that make it easier for infrastructure applications to run on mainframe.\nCNCF Mentoring Initiatives    Program Name Duration Timeline Stipend     CNCF Mentoring Initiatives  ˜3 Months See Mentoring Programs  Yes    Cloud Native Computing Foundation (CNCF) offers a vibrant community of projects and offers internships throughout the year through different mentoring programs .\nX.Org Endless Vacation of Code (EVoC)    Program Name Duration Timeline Stipend     X.Org Endless Vacation of Code (EVoC)  3 - 4 Months Can be initiated anytime  $3000    The EVoC program was initiated to help support more projects that would otherwise go rejected through GSoC. Students are welcome to either come up with an idea on their own or work up a proposal for an idea suggested by someone else.\nHyperledger Mentorship Program    Program Name Duration Timeline Stipend     Hyperledger Mentorship Program  3 Months FT - 6 Months PT June 1st  $3000 - $6600     This program provides a structured and hands-on opportunity for students and new developers gain exposure to Hyperledger open source development and entry to the technical community.\nJulia Seasons of Contributions (JSoC)    Program Name Duration Timeline Stipend     Julia Seasons of Contributions (JSoC)  - Through GSoC or LFX  Yes    A set of seasonal programs for funding or mentoring students and other developers to contribute to the Julia open-source ecosystem.\nSummer of Haskell    Program Name Duration Timeline Stipend     Summer of Haskell  10 Weeks, ˜3 Months January 2021 - August 2021  $1500 - $3300     This program is an effort by Haskell.Org to reach out to students and encourage them to contribute to the Haskell community with the aid of experienced mentors.\n24 Pull Requests    Program Name Duration Timeline Stipend     24 Pull Requests  1 Month December 1st - December 24th -    As the name suggests, this program encourages new contributors to make 24 pull requests in the month of December. This is a very beginner friendly program.\nSummary Here is the entire article summarized into a table:\n   Program Name Duration Timeline Stipend     Google Summer of Code (GSOC)  10 Weeks, ˜3 Months January 2021 - August 2021  $1500 - $3300    Linux Foundation Mentorship Program (LFX)  12 Weeks, ˜3 Months March 1st - May 31st , June 1st - August 31st , September 1st - November 30th  $3000 - $6600    MLH Fellowship  12 Weeks September 20th - December 13th (3 batches year round)  Up to $5000 (need based)    Google Season of Docs  6 Months May 17th - December 14th  Depends of budget of the organization    Outreachy  3 Months May - August, December - March (Applications due September 3)  $5500    Season of KDE  3 Months January - April  No   Free Software Foundation (FSF) Internship  12 Weeks 3 terms yearly  No   Linux Kernel Mentorship Program  12 Weeks March 1st - May 31st , June 1st - August 31st , September 1st - November 30th  $3000 - $6600    Linux Foundation Networking (LFN) Mentorship Program  12 Weeks FT, 24 Weeks PT June 1st - August 21st , September 1st - ?  $3000 - $6600    GNOME Summer of Code  10 Weeks, ˜3 Months January 2021 - August 2021  $1500 - $3300    Alibaba Summer of Code  3 Months May 25th - August 31st (2020)  Yes   FOSSASIA Codeheat  Year round October - June   Prizes for winners    FOSSASIA Internship Program  2 - 6 Months Decided individually Yes   Open Summer of Code  16 days See Timeline  Yes    Open Mainframe Project Mentorship Program  3 Months Through GSoC or LFX  Yes    CNCF Mentoring Initiatives  ˜3 Months See Mentoring Programs  Yes   X.Org Endless Vacation of Code (EVoC)  3 - 4 Months Can be initiated anytime  $3000   Hyperledger Mentorship Program  3 Months FT - 6 Months PT June 1st  $3000 - $6600    Julia Seasons of Contributions (JSoC)  - Through GSoC or LFX  Yes   Summer of Haskell  10 Weeks, ˜3 Months January 2021 - August 2021  $1500 - $3300    24 Pull Requests  1 Month December 1st - December 24th -    ","permalink":"https://navendu.me/posts/open-source-internship-programs/","summary":"A curated list of Open Source Internship Programs with all the necessary details.","title":"20+ Open Source Internship Programs that you can Apply to"},{"content":"Takeaways from Navendu Pottekkat\u0026rsquo;s session on \u0026ldquo;Building Your Career in Open-Source\u0026rdquo; at Open Source India .\nHave questions? Comment below or reach out to me at @sudo_navendu .\nYou can also checkout this Twitter thread for a quick summary of my talk.\n♥️ Love Open-Source? ⚡️ Here is how you can build your Career in Open-Source.\n🧵 A thread.\n❗️ Everything mentioned here is from my experience as a noob coder to working in Open-Source full time.\n\u0026mdash; Navendu Pottekkat (@sudo_navendu) October 9, 2021  Timeline From new programmer in January 2020 to Full-time Open-Source in March 2021.\n January 2020: Started learning to code full-time March - April 2020: Started open-sourcing personal projects August 2020: Built and scaled my first open-source project (Went on to get featured on Product Hunt and JS Weekly and got to #1 Trending on GitHub) September 2020: Featured project at TFUG India  November 2020: Started contributing to Meshery  March 2021: LFX Mentee at Meshery (Layer5) April 2021: Maintainer of Meshery May 2021: GSoC Mentor at CNCF for Meshery June 2021: Maintainer of Service Mesh Performance July 2021: Full-time employee at Layer5 September 2021: LFX Mentor for CNCF  Slides  Key Takeaways Getting Started in Open-Source  Learn a skill or explore different area until you find what you want to skill up on Make your personal projects open-source even if it may not get wide adoption Learn Git and GitHub (see resources ) Learn things in public  Building and Scaling Your Own Open-Source Projects  Solve your problem; It is likely that others face the same problem too Use social media liberally to scale up your projects Leverage successful open-source business models (see resources )  Making Your First Contribution  Find the projects you are interested in (see resources ) Learn the skills needed to contribute Join the community of the project Read the contributing guidelines Grab a \u0026ldquo;good first issue\u0026rdquo; and fix it  Non-Code Contributions  Writing: Articles, social media content, documentation Designing: Artworks for social media, creating style guides Testing/Using: Reporting bugs, advocacy, improving UX, alpha/beta testing Mentoring: Reviewing code, mentoring a contributor Community Managing/Organizing: Being a project manager, being a release manager, organizing events and meetups, helping onboard new Contributors  (See resources )\nLevelling Up  Find more ways to contribute Do what a maintainer is expected to do even if you aren\u0026rsquo;t a maintainer Pay it forward  Making Money  Get an internship/paid mentorship (See resources ) Get a full-time role  Getting Sponsored  GitHub Sponsors, Patreon, Open Collective Apply for open-source grants Donating the project to a foundation  Resources Referenced Surveys  Tidelift 2021 Open Source Maintainer Survey  Linux Foundation Jobs Report 2021  MLH Bi-Annual Census (Spring 2021)  Tutorials/ Additional Reading  Git for Professionals Tutorial - Tools \u0026amp;amp; Concepts for Mastering Version Control with Git  Git and GitHub  Complete Guide to Open Source - How to Contribute  Why Open-Source - Twitter thread  Starting an Open-Source Project  How to Make Non-Code Contributions to Open-Source Projects  20\u0026#43; Open Source Internship Programs that you can Apply to  Tips to Make High-Quality Pull Requests - Twitter thread  Tips to Make High-Quality Pull Requests - Blog post  Business Models for Open-Source Software  Getting Paid for Open-Source Work  Finding Users for Your Project  Open Source Metrics  open-funding (Open-Source Grants)   If you liked the content of the session, follow me @sudo_navendu where I will be sharing more similar content.\nMore of my talks are in this playlist .\n","permalink":"https://navendu.me/posts/building-your-career-in-open-source/","summary":"My experience from a new programmer to working in open-source full time.","title":"Building Your Career in Open-Source"},{"content":"Takeaways and additional resources from Navendu\u0026#39;s session on \u0026#39;Non-Code Contributions to Open-Source\u0026#39; .\nHave questions? Comment below or reach out to me at @sudo_navendu .\nYou can also checkout this Twitter thread for a quick summary of my talk.\n🤩 11 days to go until @hacktoberfest !\nHere are some ways you can contribute to open-source projects WITHOUT contributing code.\nA thread 👇.\n\u0026mdash; Navendu Pottekkat (@sudo_navendu) September 18, 2021  Recording   Slides  Key Takeaways Why Contribute to Open-Source?  Open-source is gaining traction and is the new norm Companies are hiring people with open-source skills and experience Build your skills, resume and your network Be part of a community, get mentorship or contribute to projects you use  Why Non-Code Contributions?  Has huge impact You are skilled in non-code ways Stepping stone to contributing code  How to Make Non-Code Contributions? You can engage in:\n Writing Designing Testing/Using Mentoring Community Managing/Organising  Learn more about non-code contributions here: How to Make Non-Code Contributions to Open-Source Projects .\n","permalink":"https://navendu.me/posts/webinar-non-code-contributions-to-open-source/","summary":"Key takeaways and resources from Navendu\u0026rsquo;s talk on Non-Code Contributions to Open-Source.","title":"Webinar: Non-Code Contributions to Open-Source"},{"content":"This CNCF Webinar introduces Meshery, the open source, service mesh management plane that enables the adoption, operation, and management of any service mesh and their workloads.\nWatch the webinar below.\n  Check out the links below for getting started with Meshery.\nWebsite: meshery.io GitHub: meshery/meshery Twitter: mesheryio Join the Slack channel ","permalink":"https://navendu.me/posts/cncf-webinar-meshery/","summary":"This CNCF Webinar introduces Meshery, the open source, service mesh management plane that enables the adoption, operation, and management of any service mesh and their workloads.","title":"CNCF Webinar: Meshery, the Service Mesh Manager"},{"content":"If you are in a hurry and just want the template skip to the bottom (not cool).\nThe rest of the cool people get ready to take your first step to being a README MASTER! (absolutely not clickbait).\nYou have just created this awesome project and you shared it on GitHub. You think now you’ll just sit back and wait for the world to tell you how cool your project is. After all, you have worked tirelessly for the past month on this very challenging project, right?\nWell, let’s just take a step back and look from the perspective of a developer or a user checking your project. Although you know how cool your project is, and how it is going to solve that one pressing problem that hasn’t been fixed(until you came along), the person would be looking at your project and wondering what in the world you have built.\nIf nobody can’t figure out how to use your software, there’s something very bad going on.\n  Photo from imgflip\n  If people don’t know what your software does, then they won’t use it or contribute to it and they will most likely find something more clear and concise in the sea of open-source software.\nThat’s where the README comes in!\nA good README is like the face of your project. It is the first thing a person looks at in your project and it gives them a very brief introduction to your software.\n  Beginning of the README from my NSFW Filter project\n  A good-looking and helpful README file can make your project stand out and grab attention from the developer community.\nIt will help them understand your project, how they can get it working and why they should contribute.\n “Wow, man! Nice rant there! If you know so much about stuff why don’t you tell us how to write then…”\n Hey, I can’t tell you that there is a concrete set of rules and you should follow those rules without swaying for a good README.\nIt doesn’t work like that.\nI will share how I write a README for my Open-Source projects and things that you should consider while you write one for your projects and you will(hopefully) be able to get some insights.\nAlso, remember that you won’t be the README master in a day. Like all things, it takes practice.\nI have been contributing to open-source for a while now and a thing I noticed is that all great projects have an Awesome README.\nYou are on the project page and within minutes you are up and running with your version of the project.\nThere are a lot of contributors, a lot of pull requests, updated versions being released frequently and they all have the common factor of an awesome README.\nA new developer will be able to find all the details to get started like install instructions and contributing guides.\nA new user would be able to find how the project is being used with informative screenshots and demos.\n “I don’t have time for this, show me the README already!”\n Alright, alright, alright (sorry I went a little McConaughey ).\nHere is the README file for my project \u0026lt;strong\u0026gt;NSFW\u0026lt;/strong\u0026gt; \u0026lt;strong\u0026gt;Filter\u0026lt;/strong\u0026gt; . I think this is the best README that I have written.\nI will go through the different parts of the README that I think are essential to every README.\n Here is the link for the README file used in this example. You can also find a \u0026lt;strong\u0026gt;template\u0026lt;/strong\u0026gt; README file that you can directly copy and paste in your projects.\nWell, that winds things up.\nI bid you farewell my grasshopper with these parting words(of wisdom) from the README Sensei(new twitter handle alert!).\n  Photo from imgflip\n  ","permalink":"https://navendu.me/posts/how-to-write-an-awesome-readme/","summary":"A comprehensive guide to writing README that stands out and makes people go WOW!","title":"How to write an Awesome README"}]